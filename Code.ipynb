{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvIYUtvdzSHS"
      },
      "outputs": [],
      "source": [
        "#@title Importing libraries\n",
        "# Run if it gives you an error for any of these; FEEL FREE TO IMPORT MORE FOR EVERYONE TO USE!\n",
        "!pip install numpy pandas scipy matplotlib seaborn plotly scikit-learn tensorflow torch nltk spacy geopandas folium pillow opencv-python pyspark dask\n",
        "\n",
        "# Data manipulation and analysis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "\n",
        "# Data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Machine learning\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "# Statistical analysis\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Natural Language Processing\n",
        "import nltk\n",
        "import spacy\n",
        "\n",
        "# Geospatial analysis\n",
        "import geopandas as gpd\n",
        "import folium\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPplmCzbGB-L"
      },
      "outputs": [],
      "source": [
        "#@title importing datasets\n",
        "!pip install -U -q PyDrive\n",
        "\n",
        "import pandas as pd\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "def import_dataset(file_id, dataframe_name):\n",
        "    # Authenticate and create the PyDrive client\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "\n",
        "    # Download the file\n",
        "    downloaded = drive.CreateFile({'id': file_id})\n",
        "    downloaded.GetContentFile(f'{dataframe_name}.csv')\n",
        "\n",
        "    # Read the CSV file into a DataFrame\n",
        "    df = pd.read_csv(f'{dataframe_name}.csv')\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU8Qeq_8Nie3"
      },
      "source": [
        "To import a dataset, make sure it's a csv, upload to drive, copy the drive link, extract the id:\n",
        "\n",
        "https://drive.google.com/open?id=1AzU_F3Usfx_iYRLH1KAMMaNZ3-9y0AwK&usp=drive_copy means that the id is 1AzU_F3Usfx_iYRLH1KAMMaNZ3-9y0AwK\n",
        "\n",
        "\n",
        "plug into\n",
        "name = import_dataset(\"id\", \"name\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_Vg6IzEFBmI"
      },
      "outputs": [],
      "source": [
        "#@title PROVIDED DATASETS\n",
        "stock_descriptions = import_dataset(\"1AzU_F3Usfx_iYRLH1KAMMaNZ3-9y0AwK\", \"stock_descriptions\")\n",
        "nutrition = import_dataset(\"1nxW91Jp65jQOdR_2_FJl1XaXDzhiHzgC\", \"nutrition\")\n",
        "meat_stats_slaughter_weights = import_dataset(\"1IHd-9p3aDoxbER3oBCZqBY23VVD5LOnu\", \"meat_stats_slaughter_weights\")\n",
        "meat_stats_slaughter_counts = import_dataset(\"1Eo58CgEeIk7Hnw_7I2yY46Ng6qiBLyeX\", \"meat_stats_slaughter_counts\")\n",
        "meat_stats_meat_production = import_dataset(\"1xZqVE4caTO4H60FFIP7Z5Lx1xCz0UuSd\", \"meat_stats_meat_production\")\n",
        "meat_stats_cold_storage = import_dataset(\"1YKBAaJKN_-RRp789RJ4wNYnpO40GyQqD\", \"meat_stats_cold_storage\")\n",
        "all_stocks = import_dataset(\"1hY7xiB-84DbqWhsZAazfwGbgtVxnBzDJ\", \"all_stocks\")\n",
        "all_commodities = import_dataset(\"1E8ELVRv2OFMbXwWYp2XVE1wH6Or6kPkB\", \"all_commodities\")\n",
        "acs5yr = import_dataset(\"1JFpNj9SlUiWwZTFc-pnlggKu6T7h_urP\", \"acs5yr\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRkxnLvqOmFa"
      },
      "outputs": [],
      "source": [
        "#@title EXTERNAL DATASETS\n",
        "agri_variable_list = import_dataset(\"1Y6Dv0yA8UXN5V5NhSdwUpT7JtzKj3oWm\", \"agri_variable_list\")\n",
        "agri_supplemental_county = import_dataset(\"1eM9e49xO841V-iR7wvFl1DPE-I8IRAKl\", \"agri_supplemental_county\")\n",
        "agri_supplemental_state = import_dataset(\"1BJsIiC3yPwtps1dA_-Ex-rUKzurce4c6\", \"agri_supplemental_state\")\n",
        "agri_state_and_county = import_dataset(\"1OFtRcynXKbMHFezP4ZLE5DEp6QUIGnHF\", \"agri_state_and_county\")\n",
        "\n",
        "\n",
        "employment = import_dataset(\"1x2uh0JILYFsarSP1h6Dtars79CtVYIP2\", \"employment\")\n",
        "unemployment = import_dataset(\"16HmWmtgK8MM3RQ3Ed_R0-gRGqP67iDT7\", \"unemployment\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lheln3vLlQ-F"
      },
      "outputs": [],
      "source": [
        "health_region = import_dataset(\"18Y813FCdS2dXJSxOxUwcyVaAvwIxbOx-\", \"health_region\")\n",
        "social_determinants_of_health = import_dataset(\"17enYtkP62akioKX4_W9aup3pIiOyuwM8\", \"social_determinants_of_health\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utf-8 encoding\n",
        "def import_dataset(file_id, name, encoding='utf-8'):\n",
        "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "    return pd.read_csv(url, encoding=encoding)\n",
        "\n",
        "try:\n",
        "    us_census_2020_2023 = import_dataset(\"1JHgbYOHYHR-Bdd1-NAv6laH8xMFZQJTq\", \"us_census_2020_2023\", encoding='latin1')\n",
        "except UnicodeDecodeError:\n",
        "    print(\"Failed to decode us_census_2020_2023 with 'latin1' encoding\")\n",
        "\n",
        "try:\n",
        "    us_census_2010_2020 = import_dataset(\"1aFonlFnV2dQKS4VgO4kgmsUaxYctWp8e\", \"us_census_2010_2020\", encoding='latin1')\n",
        "except UnicodeDecodeError:\n",
        "    print(\"Failed to decode us_census_2010_2020 with 'latin1' encoding\")\n",
        "\n",
        "try:\n",
        "    us_census_2000_2010 = import_dataset(\"1pWz7cBJmTBitrY6GJ_mwyISFijdbDIbK\", \"us_census_2000_2010\", encoding='latin1')\n",
        "except UnicodeDecodeError:\n",
        "    print(\"Failed to decode us_census_2000_2010 with 'latin1' encoding\")"
      ],
      "metadata": {
        "id": "mJ8hrJW6MrFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elj9Jl59Q04n"
      },
      "outputs": [],
      "source": [
        "employment.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FZI88Lg2JI6p"
      },
      "outputs": [],
      "source": [
        "#@title WRANGLING\n",
        "#FOR all_commodites.csv !!!!\n",
        "\n",
        "#fill missing 'Commodity' values since theyre fucking with us on corn\n",
        "all_commodities['Commodity'] = all_commodities['Commodity'].fillna(method='ffill')\n",
        "\n",
        "# Fill missing 'Unit' values based on 'Commodity'\n",
        "all_commodities.loc[all_commodities['Commodity'] == 'Corn', 'Unit'] = 'Dollar Per Metric Tonne'\n",
        "all_commodities.loc[all_commodities['Commodity'].isin(['Coffee', 'Sugar']), 'Unit'] = 'Cents Per Pound'\n",
        "\n",
        "def convert_to_cents_per_pound(row):\n",
        "    if row['Commodity'] == 'Corn' and row['Unit'] == 'Dollar Per Metric Tonne':\n",
        "        #conversion: 1 tonne = 2204.62 pounds, dollar = 100 cents\n",
        "        return (row['Value'] / 2204.62) * 100\n",
        "    else:\n",
        "        return row['Value']\n",
        "\n",
        "all_commodities['Standardized_Value'] = all_commodities.apply(convert_to_cents_per_pound, axis=1)\n",
        "\n",
        "# Update the Unit for Corn to 'Cents Per Pound'\n",
        "all_commodities.loc[all_commodities['Commodity'] == 'Corn', 'Unit'] = 'Cents Per Pound'\n",
        "\n",
        "# Save the corrected CSV file (optional)\n",
        "all_commodities.to_csv('all_commodities_corrected.csv', index=False)\n",
        "\n",
        "\n",
        "#FOR ALL meat_stats DATASETS\n",
        "meat_stats_meat_production = pd.read_csv(\"meat_stats_meat_production.csv\")\n",
        "meat_stats_meat_production['Date-Time'] = pd.to_datetime(meat_stats_meat_production['Year'].astype(str) + '-' + meat_stats_meat_production['Month'].astype(str).str.zfill(2) + '-01')\n",
        "meat_stats_meat_production.set_index('Date-Time', inplace=True)\n",
        "meat_stats_meat_production.sort_index(inplace=True)\n",
        "meat_stats_meat_production = meat_stats_meat_production.drop(['Year', 'Month'], axis=1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSrix5SEc4PP"
      },
      "outputs": [],
      "source": [
        "#@title Import Proxy Loan Dataset\n",
        "loans0 = import_dataset(\"1LUw8MtYYFNapLUmt-ISqk_YzglegoYut\", \"1991-1999\")\n",
        "loans1 = import_dataset(\"1xpgOe0W8HMQOtl3f7qnwmx-Xi9r1zcKd\", \"2000-2009\")\n",
        "loans2 = import_dataset(\"1uhW_2RkHtHXQxq4TpYq5UlcTg-4hrOvY\", \"2010-2019\")\n",
        "loans3 = import_dataset(\"11RhgGiwaiXnROI4ZBxIHfetVK_03urzz\", \"2010-present\")\n",
        "loans0 = loans0[['BorrName', 'BorrZip', 'ProjectCounty', \"NaicsCode\", 'NaicsDescription', 'FranchiseCode', 'FranchiseName', 'ApprovalFiscalYear', 'ApprovalDate']]\n",
        "loans1 = loans1[['BorrName', 'BorrZip', 'ProjectCounty', \"NaicsCode\", 'NaicsDescription', 'FranchiseCode', 'FranchiseName', 'ApprovalFiscalYear', 'ApprovalDate']]\n",
        "loans2 = loans2[['BorrName', 'BorrZip', 'ProjectCounty', \"NaicsCode\", 'NaicsDescription', 'FranchiseCode', 'FranchiseName', 'ApprovalFiscalYear', 'ApprovalDate']]\n",
        "loans3 = loans3[['BorrName', 'BorrZip', 'ProjectCounty', \"NaicsCode\", 'NaicsDescription', 'FranchiseCode', 'FranchiseName', 'ApprovalFiscalYear', 'ApprovalDate']]\n",
        "\n",
        "# Can use these regex's to sort using str.contains if you have questions lmk, case-insensitive\n",
        "chain_names = ['POPEYE', 'starbucks', 'checkers', 'MCDONALD', 'del taco', ' qdoba', 'domino\\'s', 'dutch bro', 'TACO BELL', 'DAIRY QUEEN', 'WAFFLE HOUSE', 'FRIED', 'KENTUCKY', 'BURGER', 'pizza', 'Arctic Circle', 'Chili', 'sonic drive', 'jack in the box', 'panda express', 'pei wei','papa john', 'little caesar', 'wendy\\'s', 'wingstop', 'zaxby', 'jimmy john', 'five guys', 'hardee', 'bojangle', 'carl\\'s jr', 'dunkin', 'krispy kreme', 'el pollo loco', 'shake shack', 'baskin robbins', 'church\\'s chicken', 'papa murphy\\'s', 'moe\\s', 'freddy\\'s frozen']\n",
        "chain_regex = '|'.join(chain_names)\n",
        "grocery_indicators = ['Grocery', 'Food store', 'bodega', 'food mart']\n",
        "grocery_regex = '|'.join(grocery_indicators)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brAZplfHc-Q7"
      },
      "outputs": [],
      "source": [
        "#@title Pipeline for Getting Data (Example here based on Fiscal Year)\n",
        "total_loans = pd.concat([loans0, loans1], ignore_index=False)\n",
        "total_loans = pd.concat([total_loans, loans2], ignore_index=False)\n",
        "total_loans = pd.concat([total_loans, loans3], ignore_index=False)\n",
        "\n",
        "chains = total_loans[total_loans['BorrName'].str.contains(chain_regex, case=False, na=False)]\n",
        "chain_year = chains['ApprovalFiscalYear'].value_counts(dropna=False)\n",
        "\n",
        "# Chain_df is the full dataframe of loans given to top-50 chains\n",
        "chain_df = chain_year.reset_index()\n",
        "chain_df.sort_values('ApprovalFiscalYear', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JME4PRWMzLoS"
      },
      "outputs": [],
      "source": [
        "#@title Code in report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "bkLQBrmYfyVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "loans1 = pd.read_csv('2000-2009.csv')\n",
        "loans2 = pd.read_csv('2010-2019.csv')\n",
        "\n",
        "print(loans1.head())\n",
        "print(loans2.head())"
      ],
      "metadata": {
        "id": "doAgxPGYf3SM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTYnGE2JzFjf"
      },
      "outputs": [],
      "source": [
        "# @title Emily's code - loan stuff\n",
        "\n",
        "# Combine loan datasets into one DataFrame\n",
        "columns = ['BorrName', 'BorrZip', 'ProjectCounty', \"NaicsCode\", 'NaicsDescription',\n",
        "           'FranchiseCode', 'FranchiseName', 'ApprovalFiscalYear', 'ApprovalDate']\n",
        "loans0 = loans0[columns]\n",
        "loans1 = loans1[columns]\n",
        "loans2 = loans2[columns]\n",
        "loans3 = loans3[columns]\n",
        "total_loans = pd.concat([loans0, loans1, loans2, loans3], ignore_index=False)\n",
        "\n",
        "# Ensure ApprovalFiscalYear is an integer\n",
        "total_loans['ApprovalFiscalYear'] = total_loans['ApprovalFiscalYear'].astype(int)\n",
        "\n",
        "# Load the FIPS to county mapping dataset from the GitHub URL\n",
        "fips_url = 'https://github.com/ChuckConnell/articles/raw/master/fips2county.tsv'\n",
        "fips_mapping = pd.read_csv(fips_url, sep='\\t', dtype={'FIPS': str})\n",
        "\n",
        "# Standardize the case and strip whitespace for consistent merging\n",
        "total_loans['ProjectCounty'] = total_loans['ProjectCounty'].str.upper().str.strip()\n",
        "fips_mapping['CountyName'] = fips_mapping['CountyName'].str.upper().str.strip()\n",
        "\n",
        "# Print samples to verify the cleaning\n",
        "print(total_loans[['ProjectCounty']].head())\n",
        "print(fips_mapping[['CountyName']].head())\n",
        "\n",
        "# Merge loan data with the FIPS mapping data\n",
        "total_loans = total_loans.merge(fips_mapping, left_on='ProjectCounty', right_on='CountyName', how='left')\n",
        "\n",
        "# Print the first few rows to verify the merge\n",
        "print(total_loans[['ProjectCounty', 'StateFIPS', 'CountyFIPS', 'StateName', 'CountyName']].head())\n",
        "\n",
        "# Rename the columns to STATE and COUNTY for consistency\n",
        "total_loans.rename(columns={'StateFIPS': 'STATE', 'CountyFIPS': 'COUNTY'}, inplace=True)\n",
        "\n",
        "# Convert STATE and COUNTY in total_loans to integers\n",
        "total_loans['STATE'] = pd.to_numeric(total_loans['STATE'], errors='coerce').astype(pd.Int64Dtype())\n",
        "total_loans['COUNTY'] = pd.to_numeric(total_loans['COUNTY'], errors='coerce').astype(pd.Int64Dtype())\n",
        "\n",
        "# Prepare the census data for each period\n",
        "census_2020_2023 = us_census_2020_2023[['STATE', 'COUNTY', 'POPESTIMATE2020', 'POPESTIMATE2021', 'POPESTIMATE2022', 'POPESTIMATE2023']]\n",
        "census_2010_2020 = us_census_2010_2020[['STATE', 'COUNTY', 'POPESTIMATE2010', 'POPESTIMATE2011', 'POPESTIMATE2012', 'POPESTIMATE2013', 'POPESTIMATE2014', 'POPESTIMATE2015', 'POPESTIMATE2016', 'POPESTIMATE2017', 'POPESTIMATE2018', 'POPESTIMATE2019', 'POPESTIMATE2020']]\n",
        "census_2000_2010 = us_census_2000_2010[['STATE', 'COUNTY', 'POPESTIMATE2000', 'POPESTIMATE2001', 'POPESTIMATE2002', 'POPESTIMATE2003', 'POPESTIMATE2004', 'POPESTIMATE2005', 'POPESTIMATE2006', 'POPESTIMATE2007', 'POPESTIMATE2008', 'POPESTIMATE2009', 'POPESTIMATE2010']]\n",
        "\n",
        "# Melt the census data to have one population estimate per row\n",
        "census_2020_2023 = census_2020_2023.melt(id_vars=['STATE', 'COUNTY'], var_name='Year', value_name='Population')\n",
        "census_2020_2023['Year'] = census_2020_2023['Year'].str.extract('(\\d+)').astype(int)\n",
        "\n",
        "census_2010_2020 = census_2010_2020.melt(id_vars=['STATE', 'COUNTY'], var_name='Year', value_name='Population')\n",
        "census_2010_2020['Year'] = census_2010_2020['Year'].str.extract('(\\d+)').astype(int)\n",
        "\n",
        "census_2000_2010 = census_2000_2010.melt(id_vars=['STATE', 'COUNTY'], var_name='Year', value_name='Population')\n",
        "census_2000_2010['Year'] = census_2000_2010['Year'].str.extract('(\\d+)').astype(int)\n",
        "\n",
        "# Combine all census data\n",
        "census_data = pd.concat([census_2020_2023, census_2010_2020, census_2000_2010])\n",
        "\n",
        "# Verify the state and county columns are in the correct format\n",
        "print(total_loans[['STATE', 'COUNTY']].drop_duplicates().head())\n",
        "print(census_data[['STATE', 'COUNTY']].drop_duplicates().head())\n",
        "\n",
        "# Merge loan data with census data\n",
        "merged_loans = total_loans.merge(census_data, left_on=['STATE', 'COUNTY', 'ApprovalFiscalYear'], right_on=['STATE', 'COUNTY', 'Year'], how='left', indicator=True)\n",
        "\n",
        "# Check the merge result\n",
        "print(merged_loans['_merge'].value_counts())\n",
        "\n",
        "# Filter out only the successfully merged rows for further analysis\n",
        "total_loans = merged_loans[merged_loans['_merge'] == 'both']\n",
        "total_loans.drop(columns=['_merge'], inplace=True)\n",
        "\n",
        "# Recalculate LoansPerCapita\n",
        "total_loans['LoansPerCapita'] = total_loans.groupby(['STATE', 'COUNTY', 'ApprovalFiscalYear'])['BorrName'].transform('count') / total_loans['Population']\n",
        "\n",
        "# Filter for fast food chains and grocery stores\n",
        "chain_names = ['POPEYE', 'starbucks', 'checkers', 'MCDONALD', 'del taco', 'qdoba', 'domino\\'s', 'dutch bro', 'TACO BELL', 'DAIRY QUEEN', 'WAFFLE HOUSE', 'FRIED', 'KENTUCKY', 'BURGER', 'pizza', 'Arctic Circle', 'Chili', 'sonic drive', 'jack in the box', 'panda express', 'pei wei','papa john', 'little caesar', 'wendy\\'s', 'wingstop', 'zaxby', 'jimmy john', 'five guys', 'hardee', 'bojangle', 'carl\\'s jr', 'dunkin', 'krispy kreme', 'el pollo loco', 'shake shack', 'baskin robbins', 'church\\'s chicken', 'papa murphy\\'s', 'moe\\s', 'freddy\\'s frozen']\n",
        "chain_regex = '|'.join(chain_names)\n",
        "grocery_indicators = ['Grocery', 'Food store', 'bodega', 'food mart']\n",
        "grocery_regex = '|'.join(grocery_indicators)\n",
        "\n",
        "fast_food_loans = total_loans[total_loans['FranchiseName'].str.contains(chain_regex, case=False, na=False)]\n",
        "grocery_loans = total_loans[total_loans['NaicsDescription'].str.contains(grocery_regex, case=False, na=False)]\n",
        "\n",
        "# Recalculate trends\n",
        "fast_food_trends = fast_food_loans.groupby('ApprovalFiscalYear')['LoansPerCapita'].mean()\n",
        "grocery_trends = grocery_loans.groupby('ApprovalFiscalYear')['LoansPerCapita'].mean()\n",
        "\n",
        "# Plot the trends\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(fast_food_trends, label='Fast Food Chains')\n",
        "plt.plot(grocery_trends, label='Grocery Stores')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Loans Per Capita')\n",
        "plt.title('Trends in Loan Approvals Per Capita for Fast Food Chains vs Grocery Stores')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Calculate top 10 counties by loans per capita\n",
        "top_10_counties = total_loans.groupby('COUNTY')['LoansPerCapita'].mean().sort_values(ascending=False).head(10)\n",
        "top_10_counties.plot(kind='bar')\n",
        "plt.xlabel('County')\n",
        "plt.ylabel('Loans Per Capita')\n",
        "plt.title('Top 10 Counties by Loans Per Capita')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure ApprovalFiscalYear is an integer\n",
        "total_loans['ApprovalFiscalYear'] = pd.to_numeric(total_loans['ApprovalFiscalYear'], errors='coerce').astype(int)\n",
        "\n",
        "# Standardize the case and strip whitespace for consistent merging\n",
        "total_loans['ProjectCounty'] = total_loans['ProjectCounty'].str.upper().str.strip()\n",
        "fips_mapping['CountyName'] = fips_mapping['CountyName'].str.upper().str.strip()\n",
        "\n",
        "# Merge loan data with the FIPS mapping data\n",
        "total_loans = total_loans.merge(fips_mapping, left_on='ProjectCounty', right_on='CountyName', how='left')\n",
        "\n",
        "# Rename the columns to STATE and COUNTY for consistency\n",
        "total_loans.rename(columns={'StateFIPS': 'STATE', 'CountyFIPS': 'COUNTY'}, inplace=True)\n",
        "\n",
        "# Convert STATE and COUNTY in total_loans to integers\n",
        "total_loans['STATE'] = pd.to_numeric(total_loans['STATE'], errors='coerce').astype(pd.Int64Dtype())\n",
        "total_loans['COUNTY'] = pd.to_numeric(total_loans['COUNTY'], errors='coerce').astype(pd.Int64Dtype())\n",
        "\n",
        "# Filter Mississippi loan data\n",
        "mississippi_loans = total_loans[total_loans['StateName'] == 'MISSISSIPPI']\n",
        "print(\"Mississippi loans data before merge:\")\n",
        "print(mississippi_loans[['STATE', 'COUNTY', 'ApprovalFiscalYear']].drop_duplicates().head())\n",
        "\n",
        "# Verify unique values in Mississippi loan data\n",
        "print(\"Unique STATE and COUNTY values in mississippi_loans:\")\n",
        "print(mississippi_loans[['STATE', 'COUNTY']].drop_duplicates())\n",
        "\n",
        "# Filter Mississippi data from census_data\n",
        "mississippi_census_data = census_data[census_data['STATE'] == 28]  # Mississippi's FIPS code is 28\n",
        "print(\"Unique COUNTY values in Mississippi census_data:\")\n",
        "print(mississippi_census_data[['COUNTY']].drop_duplicates().head())\n",
        "\n",
        "# Merge loan data with census data\n",
        "mississippi_loans = mississippi_loans.merge(mississippi_census_data, left_on=['STATE', 'COUNTY', 'ApprovalFiscalYear'], right_on=['STATE', 'COUNTY', 'Year'], how='left', indicator=True)\n",
        "\n",
        "# Check the merge result\n",
        "print(\"Merge result for Mississippi loans after specific filtering:\")\n",
        "print(mississippi_loans['_merge'].value_counts())\n",
        "\n",
        "# Filter out only the successfully merged rows for further analysis\n",
        "mississippi_loans = mississippi_loans[mississippi_loans['_merge'] == 'both']\n",
        "mississippi_loans.drop(columns=['_merge'], inplace=True)\n",
        "\n",
        "# Recalculate LoansPerCapita\n",
        "mississippi_loans['LoansPerCapita'] = mississippi_loans.groupby(['STATE', 'COUNTY', 'ApprovalFiscalYear'])['BorrName'].transform('count') / mississippi_loans['Population']\n",
        "\n",
        "# Filter for fast food chains and grocery stores in Mississippi\n",
        "fast_food_loans = mississippi_loans[mississippi_loans['FranchiseName'].str.contains(chain_regex, case=False, na=False)]\n",
        "grocery_loans = mississippi_loans[mississippi_loans['NaicsDescription'].str.contains(grocery_regex, case=False, na=False)]\n",
        "\n",
        "# Recalculate trends\n",
        "fast_food_trends = fast_food_loans.groupby('ApprovalFiscalYear')['LoansPerCapita'].mean()\n",
        "grocery_trends = grocery_loans.groupby('ApprovalFiscalYear')['LoansPerCapita'].mean()\n",
        "\n",
        "# Plot the trends for Mississippi\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(fast_food_trends, label='Fast Food Chains')\n",
        "plt.plot(grocery_trends, label='Grocery Stores')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Loans Per Capita')\n",
        "plt.title('Trends in Loan Approvals Per Capita for Fast Food Chains vs Grocery Stores in Mississippi')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Calculate top 10 counties by loans per capita in Mississippi\n",
        "top_10_counties = mississippi_loans.groupby('COUNTY')['LoansPerCapita'].mean().sort_values(ascending=False).head(10)\n",
        "top_10_counties.plot(kind='bar')\n",
        "plt.xlabel('County')\n",
        "plt.ylabel('Loans Per Capita')\n",
        "plt.title('Top 10 Counties by Loans Per Capita in Mississippi')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "r0iGTVuIsda_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mississippi stuff\n",
        "\n",
        "# Combine loan datasets into one DataFrame\n",
        "columns = ['BorrName', 'BorrZip', 'ProjectCounty', \"NaicsCode\", 'NaicsDescription',\n",
        "           'FranchiseCode', 'FranchiseName', 'ApprovalFiscalYear', 'ApprovalDate']\n",
        "loans0 = loans0[columns]\n",
        "loans1 = loans1[columns]\n",
        "loans2 = loans2[columns]\n",
        "loans3 = loans3[columns]\n",
        "total_loans = pd.concat([loans0, loans1, loans2, loans3], ignore_index=False)\n",
        "\n",
        "# Ensure ApprovalFiscalYear is an integer\n",
        "total_loans['ApprovalFiscalYear'] = total_loans['ApprovalFiscalYear'].astype(int)\n",
        "\n",
        "# Load the FIPS to county mapping dataset from the GitHub URL\n",
        "fips_url = 'https://github.com/ChuckConnell/articles/raw/master/fips2county.tsv'\n",
        "fips_mapping = pd.read_csv(fips_url, sep='\\t', dtype={'FIPS': str})\n",
        "\n",
        "# Standardize the case and strip whitespace for consistent merging\n",
        "total_loans['ProjectCounty'] = total_loans['ProjectCounty'].str.upper().str.strip()\n",
        "fips_mapping['CountyName'] = fips_mapping['CountyName'].str.upper().str.strip()\n",
        "\n",
        "# Merge loan data with the FIPS mapping data\n",
        "total_loans = total_loans.merge(fips_mapping, left_on='ProjectCounty', right_on='CountyName', how='left')\n",
        "\n",
        "# Rename the columns to STATE and COUNTY for consistency\n",
        "total_loans.rename(columns={'StateFIPS': 'STATE', 'CountyFIPS': 'COUNTY'}, inplace=True)\n",
        "\n",
        "# Convert STATE and COUNTY in total_loans to integers\n",
        "total_loans['STATE'] = pd.to_numeric(total_loans['STATE'], errors='coerce').astype(pd.Int64Dtype())\n",
        "total_loans['COUNTY'] = pd.to_numeric(total_loans['COUNTY'], errors='coerce').astype(pd.Int64Dtype())\n",
        "\n",
        "# Prepare the census data for each period\n",
        "census_2020_2023 = us_census_2020_2023[['STATE', 'COUNTY', 'POPESTIMATE2020', 'POPESTIMATE2021', 'POPESTIMATE2022', 'POPESTIMATE2023']]\n",
        "census_2010_2020 = us_census_2010_2020[['STATE', 'COUNTY', 'POPESTIMATE2010', 'POPESTIMATE2011', 'POPESTIMATE2012', 'POPESTIMATE2013', 'POPESTIMATE2014', 'POPESTIMATE2015', 'POPESTIMATE2016', 'POPESTIMATE2017', 'POPESTIMATE2018', 'POPESTIMATE2019', 'POPESTIMATE2020']]\n",
        "census_2000_2010 = us_census_2000_2010[['STATE', 'COUNTY', 'POPESTIMATE2000', 'POPESTIMATE2001', 'POPESTIMATE2002', 'POPESTIMATE2003', 'POPESTIMATE2004', 'POPESTIMATE2005', 'POPESTIMATE2006', 'POPESTIMATE2007', 'POPESTIMATE2008', 'POPESTIMATE2009', 'POPESTIMATE2010']]\n",
        "\n",
        "# Melt the census data to have one population estimate per row\n",
        "census_2020_2023 = census_2020_2023.melt(id_vars=['STATE', 'COUNTY'], var_name='Year', value_name='Population')\n",
        "census_2020_2023['Year'] = census_2020_2023['Year'].str.extract('(\\d+)').astype(int)\n",
        "\n",
        "census_2010_2020 = census_2010_2020.melt(id_vars=['STATE', 'COUNTY'], var_name='Year', value_name='Population')\n",
        "census_2010_2020['Year'] = census_2010_2020['Year'].str.extract('(\\d+)').astype(int)\n",
        "\n",
        "census_2000_2010 = census_2000_2010.melt(id_vars=['STATE', 'COUNTY'], var_name='Year', value_name='Population')\n",
        "census_2000_2010['Year'] = census_2000_2010['Year'].str.extract('(\\d+)').astype(int)\n",
        "\n",
        "# Combine all census data\n",
        "census_data = pd.concat([census_2020_2023, census_2010_2020, census_2000_2010])\n",
        "\n",
        "# Filter the data for Mississippi (State FIPS code 28)\n",
        "mississippi_loans = total_loans[total_loans['STATE'] == 28]\n",
        "mississippi_census_data = census_data[census_data['STATE'] == 28]\n",
        "\n",
        "# Merge loan data with census data for Mississippi\n",
        "merged_loans = mississippi_loans.merge(mississippi_census_data, left_on=['STATE', 'COUNTY', 'ApprovalFiscalYear'], right_on=['STATE', 'COUNTY', 'Year'], how='left', indicator=True)\n",
        "\n",
        "# Check the merge result\n",
        "print(merged_loans['_merge'].value_counts())\n",
        "\n",
        "# Filter out only the successfully merged rows for further analysis\n",
        "total_loans = merged_loans[merged_loans['_merge'] == 'both']\n",
        "total_loans.drop(columns=['_merge'], inplace=True)\n",
        "\n",
        "# Recalculate LoansPerCapita\n",
        "total_loans['LoansPerCapita'] = total_loans.groupby(['STATE', 'COUNTY', 'ApprovalFiscalYear'])['BorrName'].transform('count') / total_loans['Population']\n",
        "\n",
        "# Filter for fast food chains and grocery stores\n",
        "chain_names = ['POPEYE', 'starbucks', 'checkers', 'MCDONALD', 'del taco', 'qdoba', 'domino\\'s', 'dutch bro', 'TACO BELL', 'DAIRY QUEEN', 'WAFFLE HOUSE', 'FRIED', 'KENTUCKY', 'BURGER', 'pizza', 'Arctic Circle', 'Chili', 'sonic drive', 'jack in the box', 'panda express', 'pei wei','papa john', 'little caesar', 'wendy\\'s', 'wingstop', 'zaxby', 'jimmy john', 'five guys', 'hardee', 'bojangle', 'carl\\'s jr', 'dunkin', 'krispy kreme', 'el pollo loco', 'shake shack', 'baskin robbins', 'church\\'s chicken', 'papa murphy\\'s', 'moe\\s', 'freddy\\'s frozen']\n",
        "chain_regex = '|'.join(chain_names)\n",
        "grocery_indicators = ['Grocery', 'Food store', 'bodega', 'food mart']\n",
        "grocery_regex = '|'.join(grocery_indicators)\n",
        "\n",
        "fast_food_loans = total_loans[total_loans['FranchiseName'].str.contains(chain_regex, case=False, na=False)]\n",
        "grocery_loans = total_loans[total_loans['NaicsDescription'].str.contains(grocery_regex, case=False, na=False)]\n",
        "\n",
        "# Recalculate trends\n",
        "fast_food_trends = fast_food_loans.groupby('ApprovalFiscalYear')['LoansPerCapita'].mean()\n",
        "grocery_trends = grocery_loans.groupby('ApprovalFiscalYear')['LoansPerCapita'].mean()\n",
        "\n",
        "# Plot the trends\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(fast_food_trends, label='Fast Food Chains')\n",
        "plt.plot(grocery_trends, label='Grocery Stores')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Loans Per Capita')\n",
        "plt.title('Trends in Loan Approvals Per Capita for Fast Food Chains vs Grocery Stores in Mississippi')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Calculate top 10 counties by loans per capita\n",
        "top_10_counties = total_loans.groupby('COUNTY')['LoansPerCapita'].mean().sort_values(ascending=False).head(10)\n",
        "top_10_counties.plot(kind='bar')\n",
        "plt.xlabel('County')\n",
        "plt.ylabel('Loans Per Capita')\n",
        "plt.title('Top 10 Counties by Loans Per Capita in Mississippi')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GtJNyK7ErZ2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emp3V_TEt2I4"
      },
      "outputs": [],
      "source": [
        "# @title Emily's code - county stuff\n",
        "\n",
        "print(\"agri_variable_list:\")\n",
        "print(agri_variable_list.info())\n",
        "print(agri_variable_list.head(), \"\\n\")\n",
        "\n",
        "print(\"agri_supplemental_county:\")\n",
        "print(agri_supplemental_county.info())\n",
        "print(agri_supplemental_county.head(), \"\\n\")\n",
        "\n",
        "print(\"agri_supplemental_state:\")\n",
        "print(agri_supplemental_state.info())\n",
        "print(agri_supplemental_state.head(), \"\\n\")\n",
        "\n",
        "print(\"agri_state_and_county:\")\n",
        "print(agri_state_and_county.info())\n",
        "print(agri_state_and_county.head(), \"\\n\")\n",
        "\n",
        "print(\"Unique Variable Codes in agri_variable_list:\")\n",
        "print(agri_variable_list['Variable_Code'].unique(), \"\\n\")\n",
        "\n",
        "print(\"Unique Variable Codes in agri_supplemental_county:\")\n",
        "print(agri_supplemental_county['Variable_Code'].unique(), \"\\n\")\n",
        "\n",
        "print(\"Unique Variable Codes in agri_supplemental_state:\")\n",
        "print(agri_supplemental_state['Variable_Code'].unique(), \"\\n\")\n",
        "\n",
        "print(\"Unique Variable Codes in agri_state_and_county:\")\n",
        "print(agri_state_and_county['Variable_Code'].unique(), \"\\n\")\n",
        "\n",
        "unique_variable_codes = agri_state_and_county['Variable_Code'].unique()\n",
        "\n",
        "# for code in unique_variable_codes:\n",
        "#     print(code)\n",
        "\n",
        "# looking into these 3 things:\n",
        "# 1. Obesity rates\n",
        "# 2. Food access indicators\n",
        "# 3. Socioeconomic factors\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Filter relevant data\n",
        "relevant_variable_codes = [\n",
        "    'PCT_OBESE_ADULTS12', 'PCT_OBESE_ADULTS17',  # Obesity rates\n",
        "    'LACCESS_POP10', 'LACCESS_POP15',            # Food access\n",
        "    'PCT_LACCESS_POP10', 'PCT_LACCESS_POP15',\n",
        "    'MEDHHINC15', 'POVRATE15', 'CHILDPOVRATE15'  # Socioeconomic factors\n",
        "]\n",
        "\n",
        "relevant_data = agri_state_and_county[agri_state_and_county['Variable_Code'].isin(relevant_variable_codes)]\n",
        "pivot_data = relevant_data.pivot_table(index=['FIPS', 'State', 'County'], columns='Variable_Code', values='Value')\n",
        "pivot_data.reset_index(inplace=True)\n",
        "\n",
        "# plot\n",
        "def plot_obesity_rates(data):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    for column in ['PCT_OBESE_ADULTS12', 'PCT_OBESE_ADULTS17']:\n",
        "        if column in data.columns:\n",
        "            plt.plot(data['FIPS'], data[column], linestyle='-', label=column)\n",
        "    plt.xlabel('County FIPS Code')\n",
        "    plt.ylabel('Percentage')\n",
        "    plt.title('Obesity Rates Over Time')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_food_access(data):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    for column in ['LACCESS_POP10', 'LACCESS_POP15', 'PCT_LACCESS_POP10', 'PCT_LACCESS_POP15']:\n",
        "        if column in data.columns:\n",
        "            plt.plot(data['FIPS'], data[column], linestyle='-', label=column)\n",
        "    plt.xlabel('County FIPS Code')\n",
        "    plt.ylabel('Value')\n",
        "    plt.title('Food Access Indicators Over Time')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_socioeconomic_factors(data):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    for column in ['MEDHHINC15', 'POVRATE15', 'CHILDPOVRATE15']:\n",
        "        if column in data.columns:\n",
        "            plt.plot(data['FIPS'], data[column], linestyle='-', label=column)\n",
        "    plt.xlabel('County FIPS Code')\n",
        "    plt.ylabel('Value')\n",
        "    plt.title('Socioeconomic Factors')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_obesity_rates(pivot_data)\n",
        "plot_food_access(pivot_data)\n",
        "plot_socioeconomic_factors(pivot_data)\n",
        "\n",
        "# summary stats\n",
        "summary_stats = pivot_data.describe()\n",
        "print(summary_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gwCj_R7ExK5"
      },
      "outputs": [],
      "source": [
        "#@title Emily's code - merge loan data and agri data - I need to fix this\n",
        "\n",
        "total_loans['ProjectCounty'] = total_loans['ProjectCounty'].str.lower()\n",
        "pivot_data['ProjectCounty'] = pivot_data['ProjectCounty'].str.lower()\n",
        "\n",
        "if 'State' in total_loans.columns and 'State' in pivot_data.columns:\n",
        "    merge_columns = ['ProjectCounty', 'State']\n",
        "else:\n",
        "    merge_columns = ['ProjectCounty']\n",
        "\n",
        "merged_data = pd.merge(total_loans, pivot_data, on=merge_columns, how='inner')\n",
        "\n",
        "print(merged_data.head())\n",
        "print(merged_data.describe())\n",
        "print(merged_data.isnull().sum())\n",
        "\n",
        "# plots\n",
        "# def plot_obesity_vs_loans(data):\n",
        "#     plt.figure(figsize=(10, 5))\n",
        "#     if 'PCT_OBESE_ADULTS12' in data.columns:\n",
        "#         plt.scatter(data['PCT_OBESE_ADULTS12'], data['ApprovalFiscalYear'], alpha=0.5, label='Obesity Rate 2012')\n",
        "#     if 'PCT_OBESE_ADULTS17' in data.columns:\n",
        "#         plt.scatter(data['PCT_OBESE_ADULTS17'], data['ApprovalFiscalYear'], alpha=0.5, label='Obesity Rate 2017')\n",
        "#     plt.xlabel('Obesity Rate (%)')\n",
        "#     plt.ylabel('Approval Fiscal Year')\n",
        "#     plt.title('Obesity Rates vs. Loan Approval Years')\n",
        "#     plt.legend()\n",
        "#     plt.show()\n",
        "\n",
        "# def plot_food_access_vs_loans(data):\n",
        "#     plt.figure(figsize=(10, 5))\n",
        "#     if 'LACCESS_POP10' in data.columns:\n",
        "#         plt.scatter(data['LACCESS_POP10'], data['ApprovalFiscalYear'], alpha=0.5, label='Low Access Pop 2010')\n",
        "#     if 'LACCESS_POP15' in data.columns:\n",
        "#         plt.scatter(data['LACCESS_POP15'], data['ApprovalFiscalYear'], alpha=0.5, label='Low Access Pop 2015')\n",
        "#     plt.xlabel('Low Access Population')\n",
        "#     plt.ylabel('Approval Fiscal Year')\n",
        "#     plt.title('Food Access vs. Loan Approval Years')\n",
        "#     plt.legend()\n",
        "#     plt.show()\n",
        "\n",
        "# def plot_socioeconomic_vs_loans(data):\n",
        "#     plt.figure(figsize=(10, 5))\n",
        "#     if 'MEDHHINC15' in data.columns:\n",
        "#         plt.scatter(data['MEDHHINC15'], data['ApprovalFiscalYear'], alpha=0.5, label='Median Household Income 2015')\n",
        "#     if 'POVRATE15' in data.columns:\n",
        "#         plt.scatter(data['POVRATE15'], data['ApprovalFiscalYear'], alpha=0.5, label='Poverty Rate 2015')\n",
        "#     if 'CHILDPOVRATE15' in data.columns:\n",
        "#         plt.scatter(data['CHILDPOVRATE15'], data['ApprovalFiscalYear'], alpha=0.5, label='Child Poverty Rate 2015')\n",
        "#     plt.xlabel('Socioeconomic Factors')\n",
        "#     plt.ylabel('Approval Fiscal Year')\n",
        "#     plt.title('Socioeconomic Factors vs. Loan Approval Years')\n",
        "#     plt.legend()\n",
        "#     plt.show()\n",
        "\n",
        "# if not merged_data.empty:\n",
        "#     plot_obesity_vs_loans(merged_data)\n",
        "#     plot_food_access_vs_loans(merged_data)\n",
        "#     plot_socioeconomic_vs_loans(merged_data)\n",
        "# else:\n",
        "#     print(\"The merged data is empty. Check the county and state names for mismatches.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFCIyeDfwIne"
      },
      "outputs": [],
      "source": [
        "#@title aggregate data by Fiscal Year and calculate mean values\n",
        "aggregated_data = merged_data.groupby('ApprovalFiscalYear').agg({\n",
        "    'PCT_OBESE_ADULTS12': 'mean',\n",
        "    'PCT_OBESE_ADULTS17': 'mean',\n",
        "    'LACCESS_POP10': 'mean',\n",
        "    'LACCESS_POP15': 'mean',\n",
        "    'PCT_LACCESS_POP10': 'mean',\n",
        "    'PCT_LACCESS_POP15': 'mean',\n",
        "    'MEDHHINC15': 'mean',\n",
        "    'POVRATE15': 'mean',\n",
        "    'CHILDPOVRATE15': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "# plots\n",
        "def plot_aggregated_obesity_vs_loans(data):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(data['ApprovalFiscalYear'], data['PCT_OBESE_ADULTS12'], marker='o', linestyle='-', label='Obesity Rate 2012')\n",
        "    plt.plot(data['ApprovalFiscalYear'], data['PCT_OBESE_ADULTS17'], marker='o', linestyle='-', label='Obesity Rate 2017')\n",
        "    plt.xlabel('Approval Fiscal Year')\n",
        "    plt.ylabel('Average Obesity Rate (%)')\n",
        "    plt.title('Average Obesity Rates vs. Loan Approval Years')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_aggregated_food_access_vs_loans(data):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(data['ApprovalFiscalYear'], data['LACCESS_POP10'], marker='o', linestyle='-', label='Low Access Pop 2010')\n",
        "    plt.plot(data['ApprovalFiscalYear'], data['LACCESS_POP15'], marker='o', linestyle='-', label='Low Access Pop 2015')\n",
        "    plt.plot(data['ApprovalFiscalYear'], data['PCT_LACCESS_POP10'], marker='o', linestyle='-', label='Low Access Pop % 2010')\n",
        "    plt.plot(data['ApprovalFiscalYear'], data['PCT_LACCESS_POP15'], marker='o', linestyle='-', label='Low Access Pop % 2015')\n",
        "    plt.xlabel('Approval Fiscal Year')\n",
        "    plt.ylabel('Average Low Access Population')\n",
        "    plt.title('Average Food Access vs. Loan Approval Years')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_aggregated_socioeconomic_vs_loans(data):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(data['ApprovalFiscalYear'], data['MEDHHINC15'], marker='o', linestyle='-', label='Median Household Income 2015')\n",
        "    plt.plot(data['ApprovalFiscalYear'], data['POVRATE15'], marker='o', linestyle='-', label='Poverty Rate 2015')\n",
        "    plt.plot(data['ApprovalFiscalYear'], data['CHILDPOVRATE15'], marker='o', linestyle='-', label='Child Poverty Rate 2015')\n",
        "    plt.xlabel('Approval Fiscal Year')\n",
        "    plt.ylabel('Average Socioeconomic Factors')\n",
        "    plt.title('Average Socioeconomic Factors vs. Loan Approval Years')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "plot_aggregated_obesity_vs_loans(aggregated_data)\n",
        "plot_aggregated_food_access_vs_loans(aggregated_data)\n",
        "plot_aggregated_socioeconomic_vs_loans(aggregated_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hIwQJqVCkAV3"
      },
      "outputs": [],
      "source": [
        "#@title need to fix\n",
        "total_loans['ProjectCounty'] = total_loans['ProjectCounty'].str.lower()\n",
        "\n",
        "pivot_data.reset_index(inplace=True)\n",
        "if 'County' in pivot_data.columns:\n",
        "    pivot_data['County'] = pivot_data['County'].str.lower()\n",
        "    pivot_data.rename(columns={'County': 'ProjectCounty'}, inplace=True)\n",
        "else:\n",
        "    print(\"Error: 'County' column is missing in pivot_data\")\n",
        "\n",
        "print(\"Total Loans Columns:\", total_loans.columns)\n",
        "print(\"Pivot Data Columns:\", pivot_data.columns)\n",
        "\n",
        "if 'State' not in total_loans.columns:\n",
        "    total_loans['State'] = 'unknown'\n",
        "\n",
        "if 'State' not in pivot_data.columns:\n",
        "    pivot_data['State'] = 'unknown'\n",
        "\n",
        "merged_data = pd.merge(total_loans, pivot_data, on=['ProjectCounty'], how='inner')\n",
        "\n",
        "print(\"Merged Data Head:\")\n",
        "print(merged_data.head())\n",
        "print(\"Merged Data Describe:\")\n",
        "print(merged_data.describe())\n",
        "print(\"Merged Data Null Values:\")\n",
        "print(merged_data.isnull().sum())\n",
        "\n",
        "# aggregate\n",
        "aggregated_data = merged_data.groupby('ProjectCounty').agg({\n",
        "    'PCT_OBESE_ADULTS12': 'mean',\n",
        "    'PCT_OBESE_ADULTS17': 'mean',\n",
        "    'LACCESS_POP10': 'mean',\n",
        "    'LACCESS_POP15': 'mean',\n",
        "    'PCT_LACCESS_POP10': 'mean',\n",
        "    'PCT_LACCESS_POP15': 'mean',\n",
        "    'MEDHHINC15': 'mean',\n",
        "    'POVRATE15': 'mean',\n",
        "    'CHILDPOVRATE15': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "# plot\n",
        "def plot_aggregated_obesity_vs_loans(data):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.scatter(data['PCT_OBESE_ADULTS12'], data['PCT_OBESE_ADULTS17'], alpha=0.5, label='Obesity Rate 2012 vs 2017')\n",
        "    plt.xlabel('Obesity Rate 2012 (%)')\n",
        "    plt.ylabel('Obesity Rate 2017 (%)')\n",
        "    plt.title('Obesity Rates 2012 vs 2017 by County')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_aggregated_food_access_vs_loans(data):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.scatter(data['LACCESS_POP10'], data['LACCESS_POP15'], alpha=0.5, label='Low Access Pop 2010 vs 2015')\n",
        "    plt.scatter(data['PCT_LACCESS_POP10'], data['PCT_LACCESS_POP15'], alpha=0.5, label='Low Access Pop % 2010 vs 2015')\n",
        "    plt.xlabel('Low Access Population 2010')\n",
        "    plt.ylabel('Low Access Population 2015')\n",
        "    plt.title('Food Access 2010 vs 2015 by County')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_aggregated_socioeconomic_vs_loans(data):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.scatter(data['MEDHHINC15'], data['POVRATE15'], alpha=0.5, label='Median Household Income vs Poverty Rate 2015')\n",
        "    plt.scatter(data['MEDHHINC15'], data['CHILDPOVRATE15'], alpha=0.5, label='Median Household Income vs Child Poverty Rate 2015')\n",
        "    plt.xlabel('Median Household Income 2015')\n",
        "    plt.ylabel('Poverty Rate / Child Poverty Rate 2015')\n",
        "    plt.title('Socioeconomic Factors 2015 by County')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# plot\n",
        "plot_aggregated_obesity_vs_loans(aggregated_data)\n",
        "plot_aggregated_food_access_vs_loans(aggregated_data)\n",
        "plot_aggregated_socioeconomic_vs_loans(aggregated_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6u4lVHsStBWN"
      },
      "outputs": [],
      "source": [
        "#@title Hasan\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.stattools import adfuller, grangercausalitytests\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "all_stocks = pd.read_csv(all_stocks.csv)\n",
        "stock_descriptions = pd.read_csv(stock_descriptions.csv)\n",
        "nutrition = pd.read_csv(nutrition.csv)\n",
        "#useful functions\n",
        "def check_stationarity(timeseries):\n",
        "    result = adfuller(timeseries, autolag='AIC')\n",
        "    return result[1] <= 0.05  # Returns True if p-value <= 0.05 (stationary)\n",
        "\n",
        "def cross_correlation_lags(data, max_lag):\n",
        "    columns = data.columns\n",
        "    cross_corr_lags = {lag: pd.DataFrame(index=columns, columns=columns) for lag in range(-max_lag, max_lag + 1)}\n",
        "    for i in columns:\n",
        "        for j in columns:\n",
        "            cross_corr = sm.tsa.stattools.ccf(data[i], data[j], adjusted=False)\n",
        "            for lag in range(-max_lag, max_lag + 1):\n",
        "                if lag < 0:\n",
        "                    cross_corr_lags[lag].loc[i, j] = cross_corr[-lag]\n",
        "                else:\n",
        "                    cross_corr_lags[lag].loc[i, j] = cross_corr[lag]\n",
        "    return cross_corr_lags\n",
        "\n",
        "#data prep\n",
        "fast_food_stocks = stock_descriptions[stock_descriptions['Industry'].isin(['RETAIL-EATING PLACES', 'RETAIL-EATING & DRINKING PLACES'])]\n",
        "fast_food_stock_prices = all_stocks[all_stocks['Ticker_Symbol'].isin(fast_food_stocks['Symbol'])]\n",
        "\n",
        "fast_food_stock_prices['Date-Time'] = pd.to_datetime(fast_food_stock_prices['Date-Time'])\n",
        "fast_food_stock_prices['Year'] = fast_food_stock_prices['Date-Time'].dt.year\n",
        "avg_fast_food_stock_prices_close = fast_food_stock_prices.groupby('Year')['Close'].mean().reset_index()\n",
        "\n",
        "obesity_data = nutrition[(nutrition['Topic'] == 'Obesity / Weight Status') &\n",
        "                         (nutrition['Question'].str.contains('obesity', case=False))]\n",
        "obesity_by_year = obesity_data.groupby('YearStart')['Data_Value'].mean().reset_index()\n",
        "obesity_by_year.columns = ['Year', 'ObesityRate']\n",
        "\n",
        "#merging\n",
        "merged_data = pd.merge(obesity_by_year, avg_fast_food_stock_prices_close, on='Year', how='inner')\n",
        "\n",
        "print(\"\\nStationarity Check:\")\n",
        "for column in ['ObesityRate', 'Close']:\n",
        "    is_stationary = check_stationarity(merged_data[column])\n",
        "    print(f\"{column}: {'Stationary' if is_stationary else 'Non-stationary'}\")\n",
        "\n",
        "#standardizing\n",
        "scaler = StandardScaler()\n",
        "merged_data_standardized = pd.DataFrame(scaler.fit_transform(merged_data[['ObesityRate', 'Close']]),\n",
        "                                        columns=['ObesityRate_std', 'Close_std'],\n",
        "                                        index=merged_data.index)\n",
        "merged_data_standardized['Year'] = merged_data['Year']\n",
        "\n",
        "#plotting\n",
        "fig, ax1 = plt.subplots()\n",
        "ax1.set_xlabel('Year')\n",
        "ax1.set_ylabel('Obesity Rate (%)', color='tab:blue')\n",
        "ax1.plot(merged_data['Year'], merged_data['ObesityRate'], color='tab:blue')\n",
        "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.set_ylabel('Average Stock Price for Fast Food Companies (USD)', color='tab:red')\n",
        "ax2.plot(merged_data['Year'], merged_data['Close'], color='tab:red')\n",
        "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
        "\n",
        "plt.title('Obesity Rates and Fast Food Stock Prices Over Time')\n",
        "plt.show()\n",
        "\n",
        "#cross correlation\n",
        "numeric_data = merged_data[['ObesityRate', 'Close']]\n",
        "max_lag = 2  # Set max lag to 2 cause others arent relevant by inspection\n",
        "cross_corr_lags = cross_correlation_lags(numeric_data, max_lag)\n",
        "\n",
        "for lag in range(-max_lag, max_lag + 1):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cross_corr_lags[lag].astype(float), annot=True, cmap='crest', vmin=-1, vmax=1, center=0)\n",
        "    plt.title(f'Cross-Correlation Matrix Heatmap (Lag {lag} months)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "#growth rate\n",
        "merged_data['ObesityRateGrowth'] = merged_data['ObesityRate'].pct_change()\n",
        "merged_data['StockPriceGrowth'] = merged_data['Close'].pct_change()\n",
        "\n",
        "avg_growth_rates = merged_data[['ObesityRateGrowth', 'StockPriceGrowth']].mean()\n",
        "print(\"\\nAverage Annual Growth Rates:\")\n",
        "print(avg_growth_rates)\n",
        "\n",
        "#causality\n",
        "print(\"\\nGranger Causality Tests:\")\n",
        "max_lag = 4  #it bugs out for higher than this idk why but it shouldnt matter anyway since corerlation beyond 3 is low\n",
        "variables = ['ObesityRate', 'Close']\n",
        "for i in range(len(variables)):\n",
        "    for j in range(len(variables)):\n",
        "        if i != j:\n",
        "            print(f\"{variables[i]} -> {variables[j]}:\")\n",
        "            test_result = grangercausalitytests(merged_data[[variables[i], variables[j]]], maxlag=max_lag, verbose=False)\n",
        "            for lag in range(1, max_lag + 1):  # Start from lag 1 to avoid lag 0\n",
        "                p_value = test_result[lag][0]['ssr_ftest'][1]\n",
        "                print(f\"  Lag {lag}: p-value = {p_value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbRYUanAAIu8"
      },
      "outputs": [],
      "source": [
        "#@title Hasan code 2: volume and volatility for fast food stocks; feel free to superpose whatever else you think should be compared in top of this\n",
        "#additional analysis on stock volume trends\n",
        "volume_trends = fast_food_stock_prices.groupby('Year')['Volume'].mean().reset_index()\n",
        "plt.figure(figsize=(14, 7))\n",
        "sns.lineplot(data=volume_trends, x='Year', y='Volume', color='purple')\n",
        "plt.title('Average Trading Volume of Fast Food Stocks Over Time')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Average Volume')\n",
        "plt.show()\n",
        "\n",
        "#volatility analysis\n",
        "fast_food_stock_prices['Daily_Return'] = fast_food_stock_prices.groupby('Ticker_Symbol')['Close'].pct_change()\n",
        "volatility = fast_food_stock_prices.groupby(['Year', 'Ticker_Symbol'])['Daily_Return'].std().reset_index()\n",
        "avg_volatility = volatility.groupby('Year')['Daily_Return'].mean().reset_index()\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "sns.lineplot(data=avg_volatility, x='Year', y='Daily_Return', color='orange')\n",
        "plt.title('Average Volatility of Fast Food Stocks Over Time')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Average Daily Return Volatility')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hasan 2.5 volatility and volume for entire market vs just fast food\n",
        "#data prep\n",
        "fast_food_stocks = stock_descriptions[stock_descriptions['Industry'].isin(['RETAIL-EATING PLACES', 'RETAIL-EATING & DRINKING PLACES'])]\n",
        "fast_food_stock_prices = all_stocks[all_stocks['Ticker_Symbol'].isin(fast_food_stocks['Symbol'])]\n",
        "\n",
        "fast_food_stock_prices['Date-Time'] = pd.to_datetime(fast_food_stock_prices['Date-Time'])\n",
        "fast_food_stock_prices['Year'] = fast_food_stock_prices['Date-Time'].dt.year\n",
        "\n",
        "# volume\n",
        "fast_food_volume_trends = fast_food_stock_prices.groupby('Year')['Volume'].mean().reset_index()\n",
        "fast_food_volume_trends.columns = ['Year', 'FastFoodVolume']\n",
        "\n",
        "# entire market\n",
        "all_stocks['Date-Time'] = pd.to_datetime(all_stocks['Date-Time'])\n",
        "all_stocks['Year'] = all_stocks['Date-Time'].dt.year\n",
        "market_volume_trends = all_stocks.groupby('Year')['Volume'].mean().reset_index()\n",
        "market_volume_trends.columns = ['Year', 'MarketVolume']\n",
        "\n",
        "#fast food stocks\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(fast_food_volume_trends['Year'], fast_food_volume_trends['FastFoodVolume'], color='purple', label='Fast Food Stocks')\n",
        "plt.plot(market_volume_trends['Year'], market_volume_trends['MarketVolume'], color='blue', label='Entire Stock Market')\n",
        "plt.title('Average Trading Volume Over Time')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Average Volume')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#volatility\n",
        "#fast food stocks\n",
        "fast_food_stock_prices['Daily_Return'] = fast_food_stock_prices.groupby('Ticker_Symbol')['Close'].pct_change()\n",
        "fast_food_volatility = fast_food_stock_prices.groupby(['Year', 'Ticker_Symbol'])['Daily_Return'].std().reset_index()\n",
        "avg_fast_food_volatility = fast_food_volatility.groupby('Year')['Daily_Return'].mean().reset_index()\n",
        "avg_fast_food_volatility.columns = ['Year', 'FastFoodVolatility']\n",
        "\n",
        "#entire market\n",
        "all_stocks['Daily_Return'] = all_stocks.groupby('Ticker_Symbol')['Close'].pct_change()\n",
        "market_volatility = all_stocks.groupby(['Year', 'Ticker_Symbol'])['Daily_Return'].std().reset_index()\n",
        "avg_market_volatility = market_volatility.groupby('Year')['Daily_Return'].mean().reset_index()\n",
        "avg_market_volatility.columns = ['Year', 'MarketVolatility']\n",
        "\n",
        "#plot\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(avg_fast_food_volatility['Year'], avg_fast_food_volatility['FastFoodVolatility'], color='orange', label='Fast Food Stocks')\n",
        "plt.plot(avg_market_volatility['Year'], avg_market_volatility['MarketVolatility'], color='green', label='Entire Stock Market')\n",
        "plt.title('Average Volatility Over Time')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Average Daily Return Volatility')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hD8e03oXrco4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-L9mVbg0d_2"
      },
      "outputs": [],
      "source": [
        "#@title Hasan 3: commodities, meat production, and stocks\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#preparing data\n",
        "fast_food_stock_prices\n",
        "if 'Date-Time' in fast_food_stock_prices.columns:\n",
        "    fast_food_stock_prices.set_index('Date-Time', inplace=True)\n",
        "fast_food_monthly = fast_food_stock_prices['Close'].resample('M').mean().to_frame('Fast_Food_Stock_Price')\n",
        "\n",
        "commodities = ['Corn', 'Coffee', 'Sugar']\n",
        "commodity_prices = all_commodities[all_commodities['Commodity'].isin(commodities)]\n",
        "commodity_prices['Date-Time'] = pd.to_datetime(commodity_prices['Date-Time'])\n",
        "commodity_prices.set_index('Date-Time', inplace=True)\n",
        "\n",
        "meat_production = meat_stats_meat_production.copy()\n",
        "\n",
        "# meat_production['DateTime'] = pd.to_datetime(meat_production['Year'].astype(str) + '-' + meat_production['Month'].astype(str).str.zfill(2) + '-01')\n",
        "# meat_production.set_index('Date-Time', inplace=True)\n",
        "# meat_production.sort_index(inplace=True)\n",
        "# meat_production = meat_production.drop(['Year', 'Month'], axis=1)\n",
        "\n",
        "\n",
        "#pivot meat_production data to make merging easier later\n",
        "meat_production_pivoted = meat_production.pivot_table(\n",
        "    values='Production',\n",
        "    index='Date-Time',\n",
        "    columns=['Animal', 'Commercial or Federally Inspected', 'Type of Meat'],\n",
        "    aggfunc='sum'\n",
        ")\n",
        "\n",
        "#flatten column names to make merging easier later\n",
        "meat_production_pivoted.columns = [f\"{animal}_{inspection}_{meat_type}\" for animal, inspection, meat_type in meat_production_pivoted.columns]\n",
        "\n",
        "#resample all data to monthly frequency\n",
        "fast_food_monthly = fast_food_stock_prices['Close'].resample('M').mean().to_frame('Fast_Food_Stock_Price')\n",
        "commodity_monthly = commodity_prices.pivot(columns='Commodity', values='Value').resample('M').mean()\n",
        "meat_production_monthly = meat_production_pivoted.resample('M').sum()\n",
        "\n",
        "#getting common date ranges since theyre pretty different\n",
        "start_date = max(fast_food_monthly.index.min(), commodity_monthly.index.min(), meat_production_monthly.index.min())\n",
        "end_date = min(fast_food_monthly.index.max(), commodity_monthly.index.max(), meat_production_monthly.index.max())\n",
        "\n",
        "fast_food_monthly = fast_food_monthly.loc[start_date:end_date]\n",
        "commodity_monthly = commodity_monthly.loc[start_date:end_date]\n",
        "meat_production_monthly = meat_production_monthly.loc[start_date:end_date]\n",
        "\n",
        "#merge data\n",
        "merged_data = pd.concat([fast_food_monthly, commodity_monthly, meat_production_monthly], axis=1)\n",
        "merged_data = merged_data.dropna()\n",
        "\n",
        "merged_data = merged_data.replace(',', '', regex=True).astype(float)\n",
        "\n",
        "#standardize data\n",
        "scaler = StandardScaler()\n",
        "normalized_data = pd.DataFrame(scaler.fit_transform(merged_data),\n",
        "                               columns=merged_data.columns,\n",
        "                               index=merged_data.index)\n",
        "\n",
        "#correlation\n",
        "plt.figure(figsize=(20, 16))\n",
        "sns.heatmap(normalized_data.corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
        "plt.title('Correlation Heatmap: Fast Food Stocks, Commodities, and Meat Production')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#would probably want to figure out how to group the meats but imt oo lazy rn\n",
        "#really good stuff for commodities ig, want more qualitative research. should also really fix these graphs they suck\n",
        "\n",
        "merged_data.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iA0nsvJohhGM"
      },
      "outputs": [],
      "source": [
        "#@title Hasan 3.better: cross correlation\n",
        "\n",
        "merged_data['Total_Red_Meat'] = merged_data[['Beef_Commercial_Red Meat', 'Beef_Federally Inspected_Red Meat', 'Lamb and Mutton_Commercial_Red Meat', 'Lamb and Mutton_Federally Inspected_Red Meat', 'Pork_Commercial_Red Meat', 'Pork_Federally Inspected_Red Meat', 'Veal_Commercial_Red Meat', 'Veal_Federally Inspected_Red Meat']].sum(axis=1)\n",
        "merged_data['Total_Poultry'] = merged_data[['Broilers_Federally Inspected_Poultry', 'Other Chicken_Federally Inspected_Poultry', 'Turkey_Federally Inspected_Poultry']].sum(axis=1)\n",
        "merged_data.drop(['Beef_Commercial_Red Meat', 'Beef_Federally Inspected_Red Meat', 'Lamb and Mutton_Commercial_Red Meat', 'Lamb and Mutton_Federally Inspected_Red Meat', 'Pork_Commercial_Red Meat', 'Pork_Federally Inspected_Red Meat', 'Veal_Commercial_Red Meat', 'Veal_Federally Inspected_Red Meat', 'Broilers_Federally Inspected_Poultry', 'Other Chicken_Federally Inspected_Poultry', 'Turkey_Federally Inspected_Poultry'], axis=1, inplace=True)\n",
        "merged_data.columns = [\"Fast_Food_Stock_Price\", \"Coffee\", \"Corn\", \"Sugar\", \"Red_Meat\", \"Poultry\"]\n",
        "scaler = StandardScaler()   #making sure it's standardized ig\n",
        "normalized_data = pd.DataFrame(scaler.fit_transform(merged_data),\n",
        "                               columns=merged_data.columns,\n",
        "                               index=merged_data.index)\n",
        "\n",
        "def cross_correlation_lags(data, max_lag):\n",
        "    columns = ['Fast_Food_Stock_Price', 'Coffee', 'Corn', 'Sugar', 'Red_Meat', 'Poultry']\n",
        "    cross_corr_lags = {lag: pd.DataFrame(index=columns, columns=columns) for lag in range(-max_lag, max_lag + 1)}\n",
        "\n",
        "    for i in columns:\n",
        "        for j in columns:\n",
        "            cross_corr = sm.tsa.stattools.ccf(data[i], data[j], adjusted=False)\n",
        "            for lag in range(-max_lag, max_lag + 1):\n",
        "                if lag < 0:\n",
        "                    cross_corr_lags[lag].loc[i, j] = cross_corr[-lag]\n",
        "                else:\n",
        "                    cross_corr_lags[lag].loc[i, j] = cross_corr[lag]\n",
        "\n",
        "    return cross_corr_lags\n",
        "\n",
        "#merge data\n",
        "numeric_data = merged_data[['Fast_Food_Stock_Price', 'Coffee', 'Corn', 'Sugar', 'Red_Meat', 'Poultry']]\n",
        "\n",
        "\n",
        "max_lag = 24  #its surprisingly good across 2 years\n",
        "cross_corr_lags = cross_correlation_lags(numeric_data, max_lag)\n",
        "\n",
        "#plot\n",
        "for lag in range(-max_lag, max_lag + 1):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cross_corr_lags[lag].astype(float), annot=True, cmap='crest', vmin=-1, vmax=1, center=0, fmt='.2f')\n",
        "    plt.title(f'Cross-Correlation Matrix Heatmap (Lag {lag} months)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1Kb3zlXmmyr"
      },
      "outputs": [],
      "source": [
        "#@title Hasan 3.2: causality\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "\n",
        "variables = ['Fast_Food_Stock_Price', 'Coffee', 'Corn', 'Sugar', 'Red_Meat', 'Poultry']\n",
        "max_lag = 12\n",
        "\n",
        "\n",
        "granger_results = {}\n",
        "for cause in variables:\n",
        "    for effect in variables:\n",
        "        if cause != effect:\n",
        "            test_result = grangercausalitytests(merged_data[[cause, effect]], maxlag=max_lag, verbose=False)\n",
        "            granger_results[(cause, effect)] = [test_result[i+1][0]['ssr_ftest'][1] for i in range(max_lag)]\n",
        "\n",
        "\n",
        "results_df = pd.DataFrame(index=range(1, max_lag+1), columns=[f\"{cause} -> {effect}\" for cause in variables for effect in variables if cause != effect])\n",
        "\n",
        "\n",
        "for (cause, effect), p_values in granger_results.items():\n",
        "    results_df[f\"{cause} -> {effect}\"] = p_values\n",
        "#to make everything numeric and avoid errors\n",
        "results_df = results_df.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "#color p vals based on significance\n",
        "def color_p_values(val):\n",
        "    if pd.isna(val):\n",
        "        return ''\n",
        "    elif val <= 0.01:\n",
        "        return 'background-color: red'\n",
        "    elif val <= 0.05:\n",
        "        return 'background-color: green'\n",
        "    elif val <= 0.1:\n",
        "        return 'background-color: orange'\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "styled_results_df = results_df.style.applymap(color_p_values)\n",
        "\n",
        "\n",
        "print(\"Granger Causality Test Results (p-values):\")\n",
        "display(styled_results_df)\n",
        "\n",
        "#results_df.to_csv('granger_causality_results.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nkYSp_vlphX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "BxNrzZgpHXEl"
      },
      "outputs": [],
      "source": [
        "#@title Hasan 3.5 ignore\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "corr_matrix = normalized_data.corr()\n",
        "\n",
        "mask = np.abs(corr_matrix) < 0.5\n",
        "\n",
        "# Plot the heatmap with the mask\n",
        "plt.figure(figsize=(20, 16))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0, mask=mask)\n",
        "plt.title('Correlation Heatmap: Fast Food Stocks, Commodities, and Meat Production (Moderate to High Correlations)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1t-lt8o7uu3E"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhtrmeB96jzb"
      },
      "outputs": [],
      "source": [
        "#@title Hasan 4.5: commodities vs types of meat regression\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.api import VAR\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "commodities = ['Coffee', 'Corn', 'Sugar']\n",
        "meat_columns = [col for col in normalized_data.columns if 'Meat' in col]\n",
        "\n",
        "for commodity in commodities:\n",
        "    for meat_type in meat_columns[:5]:  #analyzing the meat columns\n",
        "        plt.figure(figsize=(5, 3))\n",
        "        plt.scatter(normalized_data[commodity], normalized_data[meat_type], alpha=0.5)\n",
        "        plt.title(f'{commodity} Price vs {meat_type} Production')\n",
        "        plt.xlabel(f'Normalized {commodity} Price')\n",
        "        plt.ylabel(f'Normalized {meat_type} Production')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        correlation = normalized_data[[commodity, meat_type]].corr().iloc[0, 1]\n",
        "        print(f\"Correlation between {commodity} price and {meat_type}: {correlation:.4f}\")\n",
        "\n",
        "from statsmodels.tsa.api import VAR\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "#checks stationarity and differences if necessary\n",
        "def make_stationary(data):\n",
        "    for column in data.columns:\n",
        "        adf_result = adfuller(data[column])\n",
        "        if adf_result[1] > 0.05:  # p-value > 0.05 indicates non-stationarity\n",
        "            data[column] = data[column].diff().dropna()\n",
        "    return data\n",
        "\n",
        "# VAR models for each commodity-meat pair\n",
        "for commodity in commodities:\n",
        "    for meat_type in meat_columns[:5]:\n",
        "        #prep data here\n",
        "        data = normalized_data[[commodity, meat_type]].dropna()\n",
        "        data = make_stationary(data)\n",
        "\n",
        "        #seeing if there's enough to analyze after making stuff stationary\n",
        "        if len(data) < 12: # Adjust this threshold as needed\n",
        "            print(f\"Insufficient data for VAR model for {commodity} and {meat_type} after differencing.\")\n",
        "            continue\n",
        "\n",
        "        # Fit VAR model\n",
        "        try:\n",
        "            model = VAR(data)\n",
        "            results = model.fit(maxlags=12)\n",
        "        except np.linalg.LinAlgError:\n",
        "            print(f\"LinAlgError encountered for {commodity} and {meat_type}. Possible high multicollinearity or outliers.\")\n",
        "            continue #usually it says that svd did not converge, thinking more about that\n",
        "\n",
        "        #summaries of var models\n",
        "        print(f'VAR Model Summary for {commodity} and {meat_type}')\n",
        "        print(results.summary())\n",
        "\n",
        "        #impulse-response functions\n",
        "        irf = results.irf(10)\n",
        "        irf.plot(orth=False)\n",
        "        plt.title(f'Impulse Response Function: {commodity} and {meat_type}')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3V_Uq5hc98IZ"
      },
      "source": [
        "lot to go through in 4.5. maybe want to find other datasets to avoid multicollinearity\n",
        "\n",
        "may also want to fit actual lines on the visualization for var regression!\n",
        "\n",
        "emily kinda want to team up with you on this to see what more work we could do in this area\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NuvpZJAbhL9e"
      },
      "outputs": [],
      "source": [
        "#@title Hasan 4.7: commodities and stocks\n",
        "\n",
        "#time series plot\n",
        "plt.figure(figsize=(14, 8))\n",
        "for column in ['Fast_Food_Stock_Price', 'Coffee', 'Corn', 'Sugar']:\n",
        "    plt.plot(normalized_data.index, normalized_data[column], label=column)\n",
        "plt.title('Normalized Time Series: Fast Food Stock Price and Commodity Prices')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8SqI_KuAeR2E"
      },
      "outputs": [],
      "source": [
        "#@title Hasan 5: meat production, stocks, and unemployment OLD\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "\n",
        "#ACS5YR DATA\n",
        "acs5yr = import_dataset(\"1JFpNj9SlUiWwZTFc-pnlggKu6T7h_urP\", \"acs5yr\")\n",
        "acs5yr = acs5yr.dropna(subset=['Percent'])   #they sabotaged it bro istg\n",
        "acs5yr = acs5yr[acs5yr['Percent'].str.contains('%')]  #filtering for percentages\n",
        "acs5yr['Percent'] = acs5yr['Percent'].str.rstrip('%').astype('float') / 100.0   #converting percentages to decimal\n",
        "acs5yr = acs5yr[acs5yr['Year'].between(2010, 2014)]   #data only available for this years, so everything else is going to have be synced according to this\n",
        "unemployment = acs5yr.groupby('Year')['Percent'].mean().reset_index()\n",
        "unemployment['Date'] = pd.to_datetime(unemployment['Year'], format='%Y')\n",
        "unemployment\n",
        "\n",
        "\n",
        "#MEAT PRODUCTION DATA\n",
        "meat_production['Date'] = pd.to_datetime(meat_production['Date'])\n",
        "meat_production['Year'] = meat_production['Date'].dt.year\n",
        "meat_production['Production'] = pd.to_numeric(meat_production['Production'], errors='coerce')\n",
        "meat_production = meat_production.dropna(subset=['Production'])\n",
        "meat_production = meat_production[meat_production['Year'].between(2010, 2014)]\n",
        "#aggregate by year and type of meat instead of specific animal\n",
        "meat_production_agg = meat_production.groupby(['Year', 'Type of Meat'])['Production'].mean().unstack(fill_value=0).reset_index()\n",
        "\n",
        "\n",
        "\n",
        "# STOCK DATA FOR FAST FOOD RESTAURANTS\n",
        "fast_food_stocks = stock_descriptions[stock_descriptions['Industry'].isin(['RETAIL-EATING PLACES', 'RETAIL-EATING & DRINKING PLACES'])]\n",
        "fast_food_stock_prices = all_stocks[all_stocks['Ticker_Symbol'].isin(fast_food_stocks['Symbol'])]\n",
        "\n",
        "# Calculate average stock prices by year\n",
        "fast_food_stock_prices['Date-Time'] = pd.to_datetime(fast_food_stock_prices['Date-Time'])\n",
        "fast_food_stock_prices['Year'] = fast_food_stock_prices['Date-Time'].dt.year\n",
        "avg_fast_food_stock_prices_close = fast_food_stock_prices.groupby('Year')['Close'].mean().reset_index()\n",
        "avg_fast_food_stock_prices_close.rename(columns={'Close': 'Fast_Food_Stock_Price'}, inplace=True)\n",
        "\n",
        "\n",
        "#merge dataframes\n",
        "data = pd.merge(meat_production_agg, unemployment, on='Year', how='inner')\n",
        "data = pd.merge(data, avg_fast_food_stock_prices_close, on='Year', how='inner')\n",
        "\n",
        "#ilter for years 2010-2014 since thats whats available for unemployment\n",
        "data = data[data['Year'].between(2010, 2014)]\n",
        "data = data.sort_values('Year')\n",
        "data.columns = [\"Year\", \"Poultry\", \"Red Meat\", \"Unemployment_Rate\", \"Date\", \"Fast_Food_Stock_Price\"]\n",
        "print(\"Combined Data:\")\n",
        "print(data)\n",
        "\n",
        "\n",
        "#CORRELATIONS: good correlations at lag = 0 indicating instantaneous relationships which wouldnt really make sense FUCK\n",
        "\n",
        "correlation_matrix = data.drop(['Year', 'Date'], axis=1, errors='ignore').corr()\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
        "plt.title('Correlation Matrix Heatmap (Fast Food Stocks)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "\n",
        "def cross_correlation_lags(data, max_lag):\n",
        "    columns = data.columns\n",
        "    cross_corr_lags = {lag: pd.DataFrame(index=columns, columns=columns) for lag in range(-max_lag, max_lag + 1)}\n",
        "\n",
        "    for i in columns:\n",
        "        for j in columns:\n",
        "            cross_corr = sm.tsa.stattools.ccf(data[i], data[j], adjusted=False)\n",
        "            for lag in range(-max_lag, max_lag + 1):\n",
        "                if lag < 0:\n",
        "                    cross_corr_lags[lag].loc[i, j] = cross_corr[-lag]\n",
        "                else:\n",
        "                    cross_corr_lags[lag].loc[i, j] = cross_corr[lag]\n",
        "\n",
        "    return cross_corr_lags\n",
        "\n",
        "# Assuming 'data' is your DataFrame\n",
        "numeric_data = data.drop(['Year', 'Date'], axis=1, errors='ignore') #not sure if we should do that dropping time thing or not\n",
        "\n",
        "# Define the maximum lag you want to consider\n",
        "max_lag = 4\n",
        "# Calculate the cross-correlation matrix for multiple lags\n",
        "cross_corr_lags = cross_correlation_lags(numeric_data, max_lag)\n",
        "\n",
        "# Plot the cross-correlation matrices for each lag\n",
        "for lag in range(-max_lag, max_lag + 1):\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cross_corr_lags[lag].astype(float), annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
        "    plt.title(f'Cross-Correlation Matrix Heatmap (Lag {lag})')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#CAUSALITY: only somewhat for poultry -> unemployment\n",
        "\n",
        "results = {}\n",
        "\n",
        "#granger causality test for meat production and unemployment rate and vice versa\n",
        "for meat_type in meat_production_agg.columns[1:]:\n",
        "    test_result = grangercausalitytests(data[[meat_type, 'Unemployment_Rate']], maxlag = 1, verbose=False)\n",
        "    results[f'{meat_type} -> Unemployment Rate'] = [test_result[i+1][0]['ssr_ftest'][1] for i in range(1)]\n",
        "    test_result = grangercausalitytests(data[['Unemployment_Rate', meat_type]], maxlag = 1, verbose=False)\n",
        "    results[f'Unemployment Rate -> {meat_type}'] = [test_result[i+1][0]['ssr_ftest'][1] for i in range(1)]\n",
        "\n",
        "#granger causality test for meat production and fast food stock prices and vice versa\n",
        "for meat_type in meat_production_agg.columns[1:]:\n",
        "    test_result = grangercausalitytests(data[[meat_type, 'Fast_Food_Stock_Price']], maxlag = 1, verbose=False)\n",
        "    results[f'{meat_type} -> Fast Food Stock Price'] = [test_result[i+1][0]['ssr_ftest'][1] for i in range(1)]\n",
        "    test_result = grangercausalitytests(data[['Fast_Food_Stock_Price', meat_type]], maxlag = 1, verbose=False)\n",
        "    results[f'Fast Food Stock Price -> {meat_type}'] = [test_result[i+1][0]['ssr_ftest'][1] for i in range(1)]\n",
        "\n",
        "#granger causality test for unemployment rate and fast food stock prices and vice versa\n",
        "test_result = grangercausalitytests(data[['Unemployment_Rate', 'Fast_Food_Stock_Price']], maxlag = 1, verbose=False)\n",
        "results['Unemployment Rate -> Fast Food Stock Price'] = [test_result[i+1][0]['ssr_ftest'][1] for i in range(1)]\n",
        "test_result = grangercausalitytests(data[['Fast_Food_Stock_Price', 'Unemployment_Rate']], maxlag = 1, verbose=False)\n",
        "results['Fast Food Stock Price -> Unemployment Rate'] = [test_result[i+1][0]['ssr_ftest'][1] for i in range(1)]\n",
        "\n",
        "results_df = pd.DataFrame(results, index=[f'Lag {i+1}' for i in range(1)])\n",
        "print(\"Granger Causality Test Results (p-values):\")\n",
        "print(results_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1DsUUuCqE-o",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Hasan 5.2 better employment data for 5, correlation on meat, commodities, fast food and all stocks, and obesity\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "import statsmodels.api as sm\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load and process employment data\n",
        "employment_data = pd.read_csv('employment.csv')\n",
        "employment_data['Date'] = pd.to_datetime(employment_data[['Year', 'Month']].assign(DAY=1))\n",
        "employment = employment_data[['Date', 'Employment Rate']]\n",
        "unemployment = employment_data[['Date', 'Unemployment Rate']]\n",
        "\n",
        "# Process meat production data\n",
        "meat_production['Date'] = pd.to_datetime(meat_production['Date'])\n",
        "meat_production['Production'] = pd.to_numeric(meat_production['Production'], errors='coerce')\n",
        "meat_production = meat_production.dropna(subset=['Production'])\n",
        "meat_production_agg = meat_production.groupby(['Date', 'Type of Meat'])['Production'].mean().unstack(fill_value=0).reset_index()\n",
        "\n",
        "#fast food stock data\n",
        "all_stocks = pd.read_csv('all_stocks.csv')\n",
        "fast_food_stocks = stock_descriptions[stock_descriptions['Industry'].isin(['RETAIL-EATING PLACES', 'RETAIL-EATING & DRINKING PLACES'])]\n",
        "fast_food_stock_prices = all_stocks[all_stocks['Ticker_Symbol'].isin(fast_food_stocks['Symbol'])]\n",
        "fast_food_stock_prices['Date'] = pd.to_datetime(fast_food_stock_prices['Date-Time'])\n",
        "avg_fast_food_stock_prices_close = fast_food_stock_prices.groupby(fast_food_stock_prices['Date'].dt.to_period('M'))['Close'].mean().reset_index()\n",
        "avg_fast_food_stock_prices_close['Date'] = avg_fast_food_stock_prices_close['Date'].dt.to_timestamp()\n",
        "avg_fast_food_stock_prices_close.rename(columns={'Close': 'Fast_Food_Stock_Price'}, inplace=True)\n",
        "\n",
        "#all stock data\n",
        "all_stocks['Date'] = pd.to_datetime(all_stocks['Date-Time'])\n",
        "avg_stock_prices_close = all_stocks.groupby(all_stocks['Date'].dt.to_period('M'))['Close'].mean().reset_index()\n",
        "avg_stock_prices_close['Date'] = avg_stock_prices_close['Date'].dt.to_timestamp()\n",
        "\n",
        "\n",
        "#merge data\n",
        "data = pd.merge(meat_production_agg, unemployment, on='Date', how='inner')\n",
        "data = pd.merge(data, employment, on='Date', how='inner')\n",
        "data = pd.merge(data, avg_fast_food_stock_prices_close, on='Date', how='inner')\n",
        "data = pd.merge(data, avg_stock_prices_close, on='Date', how='inner')\n",
        "\n",
        "\n",
        "data['Year'] = data['Date'].dt.year\n",
        "data = data.sort_values('Date')\n",
        "\n",
        "\n",
        "new_column_names = [\"Date\", \"Poultry\", \"Red Meat\", \"Unemployment Rate\", \"Employment Rate\", \"Fast_Food_Stock_Price\", \"Stock_Market\", \"Year\"]\n",
        "if len(data.columns) == len(new_column_names):\n",
        "    data.columns = new_column_names\n",
        "else:\n",
        "    print(\"Warning: The number of columns in the DataFrame does not match the expected number.\")\n",
        "    print(\"Current columns:\", data.columns)\n",
        "\n",
        "print(\"Combined Data:\")\n",
        "print(data.head())\n",
        "\n",
        "# CORRELATIONS:\n",
        "correlation_data = data.drop(['Date'], axis=1)\n",
        "correlation_matrix = correlation_data.corr()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
        "plt.title('Correlation Matrix Heatmap (Including Year)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "def cross_correlation_lags(data, max_lag):\n",
        "    columns = data.columns\n",
        "    cross_corr_lags = {lag: pd.DataFrame(index=columns, columns=columns) for lag in range(-max_lag, max_lag + 1)}\n",
        "\n",
        "    for i in columns:\n",
        "        for j in columns:\n",
        "            cross_corr = sm.tsa.stattools.ccf(data[i], data[j], adjusted=False)\n",
        "            for lag in range(-max_lag, max_lag + 1):\n",
        "                if lag < 0:\n",
        "                    cross_corr_lags[lag].loc[i, j] = cross_corr[-lag]\n",
        "                else:\n",
        "                    cross_corr_lags[lag].loc[i, j] = cross_corr[lag]\n",
        "\n",
        "    return cross_corr_lags\n",
        "\n",
        "numeric_data = data.drop(['Date'], axis=1)\n",
        "\n",
        "max_lag = 12  # Increased to 12 months for a full year of lags\n",
        "\n",
        "cross_corr_lags = cross_correlation_lags(numeric_data, max_lag)\n",
        "\n",
        "for lag in range(-max_lag, max_lag + 1):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cross_corr_lags[lag].astype(float), annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
        "    plt.title(f'Cross-Correlation Matrix Heatmap (Lag {lag})')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-n44awLsQex"
      },
      "outputs": [],
      "source": [
        "#@title Hasan 5.3 causality\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "\n",
        "def run_granger_causality(data, max_lag=12):\n",
        "    variables = ['Poultry', 'Red Meat', 'Employment Rate', 'Unemployment Rate', 'Fast_Food_Stock_Price', 'Stock_Market']\n",
        "    results = {}\n",
        "\n",
        "    for i in range(len(variables)):\n",
        "        for j in range(len(variables)):\n",
        "            if i != j:\n",
        "                x = variables[i]\n",
        "                y = variables[j]\n",
        "                key = f\"{x} -> {y}\"\n",
        "                test_result = grangercausalitytests(data[[x, y]], maxlag=max_lag, verbose=False)\n",
        "                results[key] = [test_result[i+1][0]['ssr_ftest'][1] for i in range(max_lag)]\n",
        "\n",
        "    return results\n",
        "\n",
        "def color_p_values(val):\n",
        "    if pd.isna(val):\n",
        "        return ''\n",
        "    elif val <= 0.01:\n",
        "        return 'background-color: red'\n",
        "    elif val <= 0.05:\n",
        "        return 'background-color: green'\n",
        "    elif val <= 0.1:\n",
        "        return 'background-color: orange'\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "def print_granger_results(results, max_lag):\n",
        "    results_df = pd.DataFrame(results, index=[f'Lag {i+1}' for i in range(max_lag)]).T\n",
        "    styled_results_df = results_df.style.applymap(color_p_values)\n",
        "    display(styled_results_df)\n",
        "\n",
        "max_lag = 12\n",
        "granger_results = run_granger_causality(data, max_lag)\n",
        "print_granger_results(granger_results, max_lag)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "U6458kYIQSCG"
      },
      "outputs": [],
      "source": [
        "#@title Hasan 5.5 IGNORE\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming the data preparation steps have been completed as previously discussed\n",
        "\n",
        "# Standardize the data for visualization purposes\n",
        "scaler = StandardScaler()\n",
        "data_standardized = pd.DataFrame(scaler.fit_transform(data[['Poultry', 'Red Meat', 'Fast_Food_Stock_Price']]),\n",
        "                                 columns=['Poultry', 'Red Meat', 'Fast_Food_Stock_Price'],\n",
        "                                 index=data.index)\n",
        "data_standardized['Year'] = data['Year']\n",
        "data_standardized['Unemployment Rate'] = data['Unemployment Rate']  # Keep unemployment rate as percentage\n",
        "\n",
        "# Create the plot\n",
        "fig, ax1 = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "# Plot standardized variables\n",
        "ax1.plot(data_standardized['Year'], data_standardized['Poultry'], label='Poultry Production', color='blue')\n",
        "ax1.plot(data_standardized['Year'], data_standardized['Red Meat'], label='Red Meat Production', color='red')\n",
        "ax1.plot(data_standardized['Year'], data_standardized['Fast_Food_Stock_Price'], label='Fast Food Stock Price', color='green')\n",
        "\n",
        "# Set labels for standardized axis\n",
        "ax1.set_xlabel('Year')\n",
        "ax1.set_ylabel('Standardized Values')\n",
        "\n",
        "# Create a secondary y-axis for unemployment rate\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(data['Year'], data['Unemployment Rate'], label='Unemployment Rate', color='purple', linestyle='--')\n",
        "ax2.set_ylabel('Unemployment Rate (%)')\n",
        "\n",
        "# Combine legends\n",
        "lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
        "\n",
        "# Set title and display the plot\n",
        "plt.title('Meat Production, Unemployment Rate, and Fast Food Stock Prices (2010-2014)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VAdVoMIuxDAK"
      },
      "outputs": [],
      "source": [
        "#@title Hasan 5.7 graphing food stocks, unemployment, meat production, and stock market\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming 'data' is your DataFrame with columns 'Date', 'Poultry', 'Red Meat', 'Unemployment Rate', 'Employment Rate', 'Fast_Food_Stock_Price', 'Stock_Market'\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "columns_to_standardize = ['Poultry', 'Red Meat', 'Unemployment Rate', 'Employment Rate', 'Fast_Food_Stock_Price', 'Stock_Market']\n",
        "data_standardized = pd.DataFrame(scaler.fit_transform(data[columns_to_standardize]), columns=columns_to_standardize, index=data.index)\n",
        "data_standardized['Date'] = data['Date']\n",
        "\n",
        "# Create the plot\n",
        "fig, ax1 = plt.subplots(figsize=(18, 9))\n",
        "\n",
        "# Plot Poultry and Red Meat on the first y-axis\n",
        "ax1.plot(data_standardized['Date'], data_standardized['Poultry'], label='Poultry', color='blue')\n",
        "ax1.plot(data_standardized['Date'], data_standardized['Red Meat'], label='Red Meat', color='red')\n",
        "ax1.set_xlabel('Date')\n",
        "ax1.set_ylabel('Standardized Meat Production', color='black')\n",
        "ax1.tick_params(axis='y', labelcolor='black')\n",
        "\n",
        "# Create a second y-axis for Unemployment Rate and Employment Rate\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(data_standardized['Date'], data_standardized['Unemployment Rate'], label='Unemployment Rate', color='green')\n",
        "ax2.plot(data_standardized['Date'], data_standardized['Employment Rate'], label='Employment Rate', color='orange')\n",
        "ax2.set_ylabel('Standardized Rates (%)', color='green')\n",
        "ax2.tick_params(axis='y', labelcolor='green')\n",
        "\n",
        "# Create a third y-axis for Fast Food Stock Price and Stock Market\n",
        "ax3 = ax1.twinx()\n",
        "ax3.spines['right'].set_position(('axes', 1.1))  # Offset the right spine of the third axis\n",
        "ax3.plot(data_standardized['Date'], data_standardized['Fast_Food_Stock_Price'], label='Fast Food Stock Price', color='purple')\n",
        "ax3.plot(data_standardized['Date'], data_standardized['Stock_Market'], label='Stock Market', color='brown')\n",
        "ax3.set_ylabel('Standardized Stock Prices ($)', color='purple')\n",
        "ax3.tick_params(axis='y', labelcolor='purple')\n",
        "\n",
        "# Combine legends\n",
        "lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "lines3, labels3 = ax3.get_legend_handles_labels()\n",
        "ax1.legend(lines1 + lines2 + lines3, labels1 + labels2 + labels3, loc='upper left')\n",
        "\n",
        "plt.title('Standardized Time Series of Meat Production, Employment, Unemployment, Fast Food Stocks, and Stock Market')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLcItpz6BkGJ"
      },
      "outputs": [],
      "source": [
        "#@title Hasan 6 finding more correlations and causation for obestiy\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load and process employment data\n",
        "employment_data = pd.read_csv('employment.csv')\n",
        "employment_data['Year'] = pd.to_datetime(employment_data['Date']).dt.year\n",
        "employment = employment_data.groupby('Year')['Employment Rate'].mean().reset_index()\n",
        "unemployment = employment_data.groupby('Year')['Unemployment Rate'].mean().reset_index()\n",
        "\n",
        "# Process meat production data\n",
        "meat_production = pd.read_csv('meat_stats_meat_production.csv')\n",
        "meat_production['Year'] = pd.to_datetime(meat_production['Date']).dt.year\n",
        "meat_production['Production'] = pd.to_numeric(meat_production['Production'], errors='coerce')\n",
        "meat_production = meat_production.dropna(subset=['Production'])\n",
        "meat_production_agg = meat_production.groupby(['Year', 'Type of Meat'])['Production'].mean().unstack(fill_value=0).reset_index()\n",
        "\n",
        "# Fast food stock data\n",
        "all_stocks = pd.read_csv('all_stocks.csv')\n",
        "stock_descriptions = pd.read_csv('stock_descriptions.csv')\n",
        "fast_food_stocks = stock_descriptions[stock_descriptions['Industry'].isin(['RETAIL-EATING PLACES', 'RETAIL-EATING & DRINKING PLACES'])]\n",
        "fast_food_stock_prices = all_stocks[all_stocks['Ticker_Symbol'].isin(fast_food_stocks['Symbol'])]\n",
        "fast_food_stock_prices['Year'] = pd.to_datetime(fast_food_stock_prices['Date-Time']).dt.year\n",
        "avg_fast_food_stock_prices_close = fast_food_stock_prices.groupby('Year')['Close'].mean().reset_index()\n",
        "avg_fast_food_stock_prices_close.rename(columns={'Close': 'Fast_Food_Stock_Price'}, inplace=True)\n",
        "\n",
        "# All stock data\n",
        "all_stocks['Year'] = pd.to_datetime(all_stocks['Date-Time']).dt.year\n",
        "avg_stock_prices_close = all_stocks.groupby('Year')['Close'].mean().reset_index()\n",
        "avg_stock_prices_close.rename(columns={'Close': 'Stock_Market'}, inplace=True)\n",
        "\n",
        "# Load and process obesity data\n",
        "nutrition = pd.read_csv('nutrition.csv')\n",
        "obesity_data = nutrition[(nutrition['Topic'] == 'Obesity / Weight Status') &\n",
        "                         (nutrition['Question'].str.contains('obesity', case=False))]\n",
        "obesity_by_year = obesity_data.groupby('YearStart')['Data_Value'].mean().reset_index()\n",
        "obesity_by_year.columns = ['Year', 'ObesityRate']\n",
        "\n",
        "# Merge data\n",
        "data = pd.merge(meat_production_agg, unemployment, on='Year', how='inner')\n",
        "data = pd.merge(data, employment, on='Year', how='inner')\n",
        "data = pd.merge(data, avg_fast_food_stock_prices_close, on='Year', how='inner')\n",
        "data = pd.merge(data, avg_stock_prices_close, on='Year', how='inner')\n",
        "data = pd.merge(data, obesity_by_year, on='Year', how='inner')\n",
        "\n",
        "data = data.sort_values('Year')\n",
        "\n",
        "print(\"Combined Data:\")\n",
        "print(data.head())\n",
        "\n",
        "# CORRELATIONS:\n",
        "correlation_data = data.drop(['Year'], axis=1)\n",
        "correlation_matrix = correlation_data.corr()\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
        "plt.title('Correlation Matrix Heatmap')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "def cross_correlation_lags(data, max_lag):\n",
        "    columns = data.columns\n",
        "    cross_corr_lags = {lag: pd.DataFrame(index=columns, columns=columns) for lag in range(-max_lag, max_lag + 1)}\n",
        "\n",
        "    for i in columns:\n",
        "        for j in columns:\n",
        "            cross_corr = sm.tsa.stattools.ccf(data[i], data[j], adjusted=False)\n",
        "            for lag in range(-max_lag, max_lag + 1):\n",
        "                if lag < 0:\n",
        "                    cross_corr_lags[lag].loc[i, j] = cross_corr[-lag]\n",
        "                else:\n",
        "                    cross_corr_lags[lag].loc[i, j] = cross_corr[lag]\n",
        "\n",
        "    return cross_corr_lags\n",
        "\n",
        "numeric_data = data.drop(['Year'], axis=1)\n",
        "\n",
        "max_lag = 5\n",
        "\n",
        "cross_corr_lags = cross_correlation_lags(numeric_data, max_lag)\n",
        "\n",
        "for lag in range(-max_lag, max_lag + 1):\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cross_corr_lags[lag].astype(float), annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
        "    plt.title(f'Cross-Correlation Matrix Heatmap (Lag {lag} years)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hasan 6.2 causations for obesity!! help!!\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "from IPython.display import display\n",
        "\n",
        "# Assuming 'data' is your DataFrame with yearly data\n",
        "\n",
        "variables = ['ObesityRate', 'Poultry', 'Red Meat', 'Unemployment Rate', 'Employment Rate', 'Fast_Food_Stock_Price', 'Stock_Market']\n",
        "max_lag = 2  # Adjusted for yearly data\n",
        "\n",
        "granger_results = {}\n",
        "for cause in variables:\n",
        "    for effect in variables:\n",
        "        if cause != effect:\n",
        "            test_result = grangercausalitytests(data[[cause, effect]], maxlag=max_lag, verbose=False)\n",
        "            granger_results[(cause, effect)] = [test_result[i+1][0]['ssr_ftest'][1] for i in range(max_lag)]\n",
        "\n",
        "results_df = pd.DataFrame(index=range(1, max_lag+1), columns=[f\"{cause} -> {effect}\" for cause in variables for effect in variables if cause != effect])\n",
        "\n",
        "for (cause, effect), p_values in granger_results.items():\n",
        "    results_df[f\"{cause} -> {effect}\"] = p_values\n",
        "\n",
        "# Convert to numeric values\n",
        "results_df = results_df.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Color p-values based on significance\n",
        "def color_p_values(val):\n",
        "    if pd.isna(val):\n",
        "        return ''\n",
        "    elif val <= 0.01:\n",
        "        return 'background-color: red'\n",
        "    elif val <= 0.05:\n",
        "        return 'background-color: green'\n",
        "    elif val <= 0.1:\n",
        "        return 'background-color: orange'\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "styled_results_df = results_df.style.applymap(color_p_values)\n",
        "\n",
        "print(\"Granger Causality Test Results (p-values):\")\n",
        "display(styled_results_df)\n",
        "\n",
        "# Create a new DataFrame with only obesity-related results\n",
        "obesity_results = results_df[[col for col in results_df.columns if 'ObesityRate' in col]]\n",
        "\n",
        "# Style the obesity results\n",
        "styled_obesity_results = obesity_results.style.applymap(color_p_values)\n",
        "\n",
        "print(\"\\nGranger Causality Test Results for Obesity Rate (p-values):\")\n",
        "display(styled_obesity_results)\n",
        "\n",
        "# Optionally, save the results to CSV files\n",
        "# results_df.to_csv('granger_causality_results.csv')\n",
        "# obesity_results.to_csv('obesity_granger_causality_results.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "1Wkv9nrHzUtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hQG3vxS6zUO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 7 cold storage?!?!\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load and process employment data\n",
        "employment_data = pd.read_csv('employment.csv')\n",
        "employment_data['Year'] = pd.to_datetime(employment_data['Date']).dt.year\n",
        "employment = employment_data.groupby('Year')['Employment Rate'].mean().reset_index()\n",
        "unemployment = employment_data.groupby('Year')['Unemployment Rate'].mean().reset_index()\n",
        "\n",
        "# Process meat production data\n",
        "meat_production = pd.read_csv('meat_stats_meat_production.csv')\n",
        "meat_production['Year'] = pd.to_datetime(meat_production['Date']).dt.year\n",
        "meat_production['Production'] = pd.to_numeric(meat_production['Production'], errors='coerce')\n",
        "meat_production = meat_production.dropna(subset=['Production'])\n",
        "meat_production_agg = meat_production.groupby(['Year', 'Type of Meat'])['Production'].mean().unstack(fill_value=0).reset_index()\n",
        "\n",
        "# Fast food stock data\n",
        "all_stocks = pd.read_csv('all_stocks.csv')\n",
        "stock_descriptions = pd.read_csv('stock_descriptions.csv')\n",
        "fast_food_stocks = stock_descriptions[stock_descriptions['Industry'].isin(['RETAIL-EATING PLACES', 'RETAIL-EATING & DRINKING PLACES'])]\n",
        "fast_food_stock_prices = all_stocks[all_stocks['Ticker_Symbol'].isin(fast_food_stocks['Symbol'])]\n",
        "fast_food_stock_prices['Year'] = pd.to_datetime(fast_food_stock_prices['Date-Time']).dt.year\n",
        "avg_fast_food_stock_prices_close = fast_food_stock_prices.groupby('Year')['Close'].mean().reset_index()\n",
        "avg_fast_food_stock_prices_close.rename(columns={'Close': 'Fast_Food_Stock_Price'}, inplace=True)\n",
        "\n",
        "# All stock data\n",
        "all_stocks['Year'] = pd.to_datetime(all_stocks['Date-Time']).dt.year\n",
        "avg_stock_prices_close = all_stocks.groupby('Year')['Close'].mean().reset_index()\n",
        "avg_stock_prices_close.rename(columns={'Close': 'Stock_Market'}, inplace=True)\n",
        "\n",
        "# Load and process obesity data\n",
        "nutrition = pd.read_csv('nutrition.csv')\n",
        "obesity_data = nutrition[(nutrition['Topic'] == 'Obesity / Weight Status') &\n",
        "                         (nutrition['Question'].str.contains('obesity', case=False))]\n",
        "obesity_by_year = obesity_data.groupby('YearStart')['Data_Value'].mean().reset_index()\n",
        "obesity_by_year.columns = ['Year', 'ObesityRate']\n",
        "\n",
        "# Merge data\n",
        "data = pd.merge(meat_production_agg, unemployment, on='Year', how='inner')\n",
        "data = pd.merge(data, employment, on='Year', how='inner')\n",
        "data = pd.merge(data, avg_fast_food_stock_prices_close, on='Year', how='inner')\n",
        "data = pd.merge(data, avg_stock_prices_close, on='Year', how='inner')\n",
        "data = pd.merge(data, obesity_by_year, on='Year', how='inner')\n",
        "\n",
        "# Process and aggregate cold storage data\n",
        "meat_stats_cold_storage['Date'] = pd.to_datetime(meat_stats_cold_storage['Date'])\n",
        "meat_stats_cold_storage['Year'] = meat_stats_cold_storage['Date'].dt.year\n",
        "cold_storage_agg = meat_stats_cold_storage.groupby(['Year', 'Type_Of_Meat'])['Weight'].mean().unstack(fill_value=0).reset_index()\n",
        "cold_storage_agg.columns = ['Year', 'ColdStorageRedMeat', 'ColdStoragePoultry']\n",
        "\n",
        "# Merge cold storage data\n",
        "data = pd.merge(data, cold_storage_agg, on='Year', how='inner')\n",
        "\n",
        "print(\"Combined Data with Cold Storage:\")\n",
        "print(data.head())\n",
        "\n",
        "# CORRELATIONS:\n",
        "correlation_data = data.drop(['Year'], axis=1)\n",
        "correlation_matrix = correlation_data.corr()\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
        "plt.title('Correlation Matrix Heatmap')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "def cross_correlation_lags(data, max_lag):\n",
        "    columns = data.columns\n",
        "    cross_corr_lags = {lag: pd.DataFrame(index=columns, columns=columns) for lag in range(-max_lag, max_lag + 1)}\n",
        "\n",
        "    for i in columns:\n",
        "        for j in columns:\n",
        "            cross_corr = sm.tsa.stattools.ccf(data[i], data[j], adjusted=False)\n",
        "            for lag in range(-max_lag, max_lag + 1):\n",
        "                if lag < 0:\n",
        "                    cross_corr_lags[lag].loc[i, j] = cross_corr[-lag]\n",
        "                else:\n",
        "                    cross_corr_lags[lag].loc[i, j] = cross_corr[lag]\n",
        "\n",
        "    return cross_corr_lags\n",
        "\n",
        "numeric_data = data.drop(['Year'], axis=1)\n",
        "\n",
        "max_lag = 5\n",
        "\n",
        "cross_corr_lags = cross_correlation_lags(numeric_data, max_lag)\n",
        "\n",
        "for lag in range(-max_lag, max_lag + 1):\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cross_corr_lags[lag].astype(float), annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
        "    plt.title(f'Cross-Correlation Matrix Heatmap (Lag {lag} years)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "max_lag = 2\n",
        "granger_results = {}\n",
        "for cause in numeric_data.columns:\n",
        "    for effect in numeric_data.columns:\n",
        "        if cause != effect:\n",
        "            try:\n",
        "                test_result = grangercausalitytests(data[[cause, effect]], maxlag=max_lag, verbose=False)\n",
        "                granger_results[(cause, effect)] = [test_result[i+1][0]['ssr_ftest'][1] for i in range(max_lag)]\n",
        "            except Exception as e:\n",
        "                print(f\"Error testing {cause} -> {effect}: {e}\")\n",
        "                granger_results[(cause, effect)] = [np.nan] * max_lag\n",
        "\n",
        "results_df = pd.DataFrame(index=range(1, max_lag+1), columns=[f\"{cause} -> {effect}\" for cause in numeric_data.columns for effect in numeric_data.columns if cause != effect])\n",
        "\n",
        "for (cause, effect), p_values in granger_results.items():\n",
        "    results_df[f\"{cause} -> {effect}\"] = p_values\n",
        "\n",
        "# Convert to numeric values\n",
        "results_df = results_df.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Color p-values based on significance\n",
        "def color_p_values(val):\n",
        "    if pd.isna(val):\n",
        "        return ''\n",
        "    elif val <= 0.01:\n",
        "        return 'background-color: red'\n",
        "    elif val <= 0.05:\n",
        "        return 'background-color: green'\n",
        "    elif val <= 0.1:\n",
        "        return 'background-color: orange'\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "styled_results_df = results_df.style.applymap(color_p_values)\n",
        "\n",
        "print(\"Granger Causality Test Results (p-values):\")\n",
        "display(styled_results_df)"
      ],
      "metadata": {
        "id": "et5ylTk5ysDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#reload data to exclude obesity\n",
        "employment_data = pd.read_csv('employment.csv')\n",
        "employment_data['Date'] = pd.to_datetime(employment_data['Date'])\n",
        "employment_data['YearMonth'] = employment_data['Date'].dt.to_period('M')\n",
        "employment = employment_data.groupby('YearMonth')['Employment Rate'].mean().reset_index()\n",
        "unemployment = employment_data.groupby('YearMonth')['Unemployment Rate'].mean().reset_index()\n",
        "\n",
        "#meat\n",
        "meat_production = pd.read_csv('meat_stats_meat_production.csv')\n",
        "meat_production['Date'] = pd.to_datetime(meat_production['Date'])\n",
        "meat_production['YearMonth'] = meat_production['Date'].dt.to_period('M')\n",
        "meat_production['Production'] = pd.to_numeric(meat_production['Production'], errors='coerce')\n",
        "meat_production = meat_production.dropna(subset=['Production'])\n",
        "meat_production_agg = meat_production.groupby(['YearMonth', 'Type of Meat'])['Production'].mean().unstack(fill_value=0).reset_index()\n",
        "\n",
        "#fast food stocks\n",
        "all_stocks = pd.read_csv('all_stocks.csv')\n",
        "stock_descriptions = pd.read_csv('stock_descriptions.csv')\n",
        "fast_food_stocks = stock_descriptions[stock_descriptions['Industry'].isin(['RETAIL-EATING PLACES', 'RETAIL-EATING & DRINKING PLACES'])]\n",
        "fast_food_stock_prices = all_stocks[all_stocks['Ticker_Symbol'].isin(fast_food_stocks['Symbol'])]\n",
        "fast_food_stock_prices['Date'] = pd.to_datetime(fast_food_stock_prices['Date-Time'])\n",
        "fast_food_stock_prices['YearMonth'] = fast_food_stock_prices['Date'].dt.to_period('M')\n",
        "avg_fast_food_stock_prices_close = fast_food_stock_prices.groupby('YearMonth')['Close'].mean().reset_index()\n",
        "avg_fast_food_stock_prices_close.rename(columns={'Close': 'Fast_Food_Stock_Price'}, inplace=True)\n",
        "\n",
        "#all stocks\n",
        "all_stocks['Date'] = pd.to_datetime(all_stocks['Date-Time'])\n",
        "all_stocks['YearMonth'] = all_stocks['Date'].dt.to_period('M')\n",
        "avg_stock_prices_close = all_stocks.groupby('YearMonth')['Close'].mean().reset_index()\n",
        "avg_stock_prices_close.rename(columns={'Close': 'Stock_Market'}, inplace=True)\n",
        "\n",
        "#merge data\n",
        "data = pd.merge(meat_production_agg, unemployment, on='YearMonth', how='inner')\n",
        "data = pd.merge(data, employment, on='YearMonth', how='inner')\n",
        "data = pd.merge(data, avg_fast_food_stock_prices_close, on='YearMonth', how='inner')\n",
        "data = pd.merge(data, avg_stock_prices_close, on='YearMonth', how='inner')\n",
        "\n",
        "# aggregate by month now since obesity's excluded\n",
        "meat_stats_cold_storage['Date'] = pd.to_datetime(meat_stats_cold_storage['Date'])\n",
        "meat_stats_cold_storage['YearMonth'] = meat_stats_cold_storage['Date'].dt.to_period('M')\n",
        "cold_storage_agg = meat_stats_cold_storage.groupby(['YearMonth', 'Type_Of_Meat'])['Weight'].mean().unstack(fill_value=0).reset_index()\n",
        "cold_storage_agg.columns = ['YearMonth', 'ColdStorageRedMeat', 'ColdStoragePoultry']\n",
        "\n",
        "#merge\n",
        "data = pd.merge(data, cold_storage_agg, on='YearMonth', how='inner')\n",
        "\n",
        "print(\"Combined Data with Cold Storage (Monthly):\")\n",
        "print(data.head())\n",
        "\n",
        "# CORRELATIONS:\n",
        "correlation_data = data.drop(['YearMonth'], axis=1)\n",
        "correlation_matrix = correlation_data.corr()\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
        "plt.title('Correlation Matrix Heatmap')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "def cross_correlation_lags(data, max_lag):\n",
        "    columns = data.columns\n",
        "    cross_corr_lags = {lag: pd.DataFrame(index=columns, columns=columns) for lag in range(-max_lag, max_lag + 1)}\n",
        "\n",
        "    for i in columns:\n",
        "        for j in columns:\n",
        "            cross_corr = sm.tsa.stattools.ccf(data[i], data[j], adjusted=False)\n",
        "            for lag in range(-max_lag, max_lag + 1):\n",
        "                if lag < 0:\n",
        "                    cross_corr_lags[lag].loc[i, j] = cross_corr[-lag]\n",
        "                else:\n",
        "                    cross_corr_lags[lag].loc[i, j] = cross_corr[lag]\n",
        "\n",
        "    return cross_corr_lags\n",
        "\n",
        "numeric_data = data.drop(['YearMonth'], axis=1)\n",
        "\n",
        "max_lag = 5\n",
        "\n",
        "cross_corr_lags = cross_correlation_lags(numeric_data, max_lag)\n",
        "\n",
        "for lag in range(-max_lag, max_lag + 1):\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cross_corr_lags[lag].astype(float), annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
        "    plt.title(f'Cross-Correlation Matrix Heatmap (Lag {lag} months)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "granger_results = {}\n",
        "for cause in numeric_data.columns:\n",
        "    for effect in numeric_data.columns:\n",
        "        if cause != effect:\n",
        "            try:\n",
        "                test_result = grangercausalitytests(data[[cause, effect]], maxlag=max_lag, verbose=False)\n",
        "                granger_results[(cause, effect)] = [test_result[i+1][0]['ssr_ftest'][1] for i in range(max_lag)]\n",
        "            except Exception as e:\n",
        "                print(f\"Error testing {cause} -> {effect}: {e}\")\n",
        "                granger_results[(cause, effect)] = [np.nan] * max_lag\n",
        "\n",
        "results_df = pd.DataFrame(index=range(1, max_lag+1), columns=[f\"{cause} -> {effect}\" for cause in numeric_data.columns for effect in numeric_data.columns if cause != effect])\n",
        "\n",
        "for (cause, effect), p_values in granger_results.items():\n",
        "    results_df[f\"{cause} -> {effect}\"] = p_values\n",
        "\n",
        "# Convert to numeric values\n",
        "results_df = results_df.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Color p-values based on significance\n",
        "def color_p_values(val):\n",
        "    if pd.isna(val):\n",
        "        return ''\n",
        "    elif val <= 0.01:\n",
        "        return 'background-color: red'\n",
        "    elif val <= 0.05:\n",
        "        return 'background-color: green'\n",
        "    elif val <= 0.1:\n",
        "        return 'background-color: orange'\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "styled_results_df = results_df.style.applymap(color_p_values)\n",
        "\n",
        "print(\"Granger Causality Test Results (p-values):\")\n",
        "display(styled_results_df)\n",
        "\n",
        "\n",
        "# Filter results to include only columns that involve ColdStorageRedMeat or ColdStoragePoultry\n",
        "cold_storage_columns = [col for col in results_df.columns if 'ColdStorageRedMeat' in col or 'ColdStoragePoultry' in col]\n",
        "filtered_results_df = results_df[cold_storage_columns]\n",
        "\n",
        "# Style the filtered results\n",
        "styled_filtered_results_df = filtered_results_df.style.applymap(color_p_values)\n",
        "\n",
        "print(\"Granger Causality Test Results Involving Cold Storage Data (p-values):\")\n",
        "display(styled_filtered_results_df)"
      ],
      "metadata": {
        "id": "f46fs1Fq5P8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hasan table\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Create a dictionary for the data\n",
        "data = {\n",
        "    #'No.': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
        "    'Variable X': [\n",
        "        'Fast Food Stocks', 'Fast Food Consumption', 'Unemployment Rate',\n",
        "        'Unemployment Rate', 'Obesity Rates', 'Obesity','Obesity','aaCold Storage Red Meat Levels (CSRM)',\n",
        "        'Red Meat Production', 'Red Meat Production', 'Cold Storage Poultry (CSP)',\n",
        "        'Cold Storage Poultry (CSP)', 'Cold Storage Poultry (CSP)', 'Cold Storage Poultry (CSP)',\n",
        "        'Cold Storage Poultry (CSP)', 'Poultry Production', 'Red Meat Production',\n",
        "        'Red Meat Production', 'Unemployment Rate', 'Employment Rate', 'Fast Food Stock Prices (FFSP)'\n",
        "    ],\n",
        "    'Variable Y': [\n",
        "        'Obesity Rate', 'Poultry Production', 'Stock Market Performance',\n",
        "        'Fast Food Stock Prices', 'Red Meat Consumption','Fast Food Stock Prices','Market Stock Prices', 'aaRed Meat Production',\n",
        "        'Employment Levels', 'Economic Performance', 'Red Meat Production',\n",
        "        'Employment Levels', 'Fast Food Stock Prices', 'Stock Market Performance',\n",
        "        'Obesity Rates', 'Cold Storage Meat (CSM)', 'Cold Storage Red Meat Levels (CSRM)',\n",
        "        'Cold Storage Poultry (CSP)', 'Cold Storage Red Meat Levels (CSRM)',\n",
        "        'Cold Storage Poultry', 'Cold Storage Poultry (CSP)'\n",
        "    ],\n",
        "    'Correlation': [\n",
        "        0.84, 0.81, -0.98, -0.98, -0.85, 0.80 ,0.75, 'aa-0.63 to -0.72', 'N/A', 'N/A', 'N/A',\n",
        "        'N/A', 0.54, 0.59, 0.57, 'N/A', 'N/A', 'N/A', 'N/A', '',''\n",
        "    ],\n",
        "    'X to Y Causation': [\n",
        "        'No', 'Yes (p=0.01)', 'No', 'Yes (p=0.05)', 'No', 'aaYes (p=0.01)', 'Yes (p=0.01)',\n",
        "        'Yes (p=0.01)', 'Yes (p=0.05)', 'Yes (p=0.1)', 'Yes (p=0.01)', 'Yes (p=0.05)',\n",
        "        'Yes (p=0.05)', 'Yes (p=0.01)', 'Yes (p=0.01)', 'Yes (p=0.1)', 'Yes (p=0.05)',\n",
        "        'Yes (p=0.1)', 'Yes (p=0.1)','',''\n",
        "    ],\n",
        "    'Y to X Causation': [\n",
        "        'No', 'Yes (p=0.01)', 'No', 'Yes (p=0.05)', 'No', 'aaYes (p=0.01)', 'Yes (p=0.05)',\n",
        "        'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No','',''\n",
        "    ],\n",
        "    'Lags in discussion': [\n",
        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,'',''\n",
        "    ],\n",
        "    'Explanation': [\n",
        "        'Parallel but not predictive increase', 'Chicken is the number 1 item used in fast food', 'Lower unemployment rates are associated with higher stock market performance (Philips Curve)',\n",
        "        'Lower unemployment rates are associated with higher fast food stock performance (Philips Curve)', 'Red meat consumption might be associated with lower obesity rates, though this relationship is complex and may involve other factors',\n",
        "        'aa', '', '', '', '', '', '', '', '', '', '', '', '', '','',''\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the table\n",
        "print(tabulate(df, headers='keys', tablefmt='fancy'))"
      ],
      "metadata": {
        "id": "LlzbbjgPOA7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JbCAvuIAzHQa"
      },
      "outputs": [],
      "source": [
        "#@title Ikenna's code\n",
        "# this analysis is time-independent -> Illinois has highest and Virgin Islands has lowest\n",
        "# Filter the dataset for physical activity data\n",
        "pa_data = nutrition[nutrition['Class'] == 'Physical Activity'].head(300)\n",
        "\n",
        "# Bar Plot of Physical Activity Levels\n",
        "plt.figure(figsize=(14, 7))\n",
        "pa_state_means = pa_data.groupby('LocationDesc')['Data_Value'].mean().sort_values()\n",
        "pa_state_means.plot(kind='bar', color='skyblue')\n",
        "plt.title('Average Percentage of Students Achieving Physical Activity Recommendations by State')\n",
        "plt.xlabel('State')\n",
        "plt.ylabel('Percentage')\n",
        "plt.xticks(rotation=90)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36XbXmm0kOWD",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Ikenna 2\n",
        "# Looking at the visualization for children's nutrition, consumption, and physical activity over time\n",
        "# group by starting year\n",
        "n_df = nutrition.dropna()\n",
        "sugar_drinks_df = n_df[n_df[\"Class\"]== \"Sugar Drinks\"]\n",
        "fruits_veg_df = n_df[n_df[\"Class\"]== \"Fruits and Vegetables\"]\n",
        "physical_activity_df = n_df[n_df[\"Class\"]== \"Physical Activity\"]\n",
        "obesity_df = n_df[n_df[\"Class\"]== \"Obesity / Weight Status\"]\n",
        "tv_df = n_df[n_df[\"Class\"]== \"Television Viewing\"]\n",
        "\n",
        "new_sugar_drinks_df = sugar_drinks_df.groupby(\"YearStart\")[\"Data_Value\"].mean()\n",
        "new_fruits_veg_df = fruits_veg_df.groupby(\"YearStart\")[\"Data_Value\"].mean()\n",
        "new_physical_activity_df = physical_activity_df.groupby(\"YearStart\")[\"Data_Value\"].mean()\n",
        "new_obesity_df = obesity_df.groupby(\"YearStart\")[\"Data_Value\"].mean()\n",
        "new_tv_df = tv_df.groupby(\"YearStart\")[\"Data_Value\"].mean()\n",
        "\n",
        "\n",
        "# Plot the trends\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(new_fruits_veg_df.index, new_fruits_veg_df.values, marker='o', linestyle='-', color='green', label='fruits and vegetables')\n",
        "plt.plot(new_sugar_drinks_df.index, new_sugar_drinks_df.values, marker='o', linestyle='-', color='orange', label='sugar drinks')\n",
        "plt.plot(new_physical_activity_df.index, new_physical_activity_df.values, marker='o', linestyle='-', color='blue', label='physical activity')\n",
        "plt.plot(new_obesity_df.index, new_obesity_df.values, marker='o', linestyle='-', color='red', label='obesity')\n",
        "plt.plot(new_tv_df.index, new_tv_df.values, marker='o', linestyle='-', color='purple', label='tv')\n",
        "\n",
        "plt.title(\"Average Percent of students in grades 9-12 who drank regular soda/pop at least one time per day\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Percent Consumed\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ikenna's Code: Data Wrangling to produce health-region csv"
      ],
      "metadata": {
        "id": "6gPa2pu3o0It"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1: preparing the CDI data\n",
        "\n",
        "cdi_file = \"/content/drive/MyDrive/Datathon Practice/US-CDI-Report.csv\" # can replace\n",
        "cdi_data = pd.read_csv(cdi_file)\n",
        "dropped_cdi_columns = [\"DataSource\", \"Response\", \"DataValueFootnoteSymbol\", \"DatavalueFootnote\", \"StratificationCategory1\", \"Stratification1\",\"StratificationCategory2\", \"Stratification2\", \"StratificationCategory3\", \"Stratification3\", \"ResponseID\", \"DataValueTypeID\",\"StratificationCategoryID2\", \"StratificationID2\", \"StratificationCategoryID3\", \"StratificationID3\"]\n",
        "reduced_cdi_data = cdi_data.drop(columns=dropped_cdi_columns)\n",
        "included_topics = ['Diabetes', 'Cancer','Nutrition, Physical Activity, and Weight Status']\n",
        "# drop entries with no value for the DataValue\n",
        "reduced2_cdi_data = reduced_cdi_data[reduced_cdi_data[\"Topic\"].isin(included_topics)].dropna(subset=[\"DataValue\"])\n",
        "\n",
        "# Currently ignoring the non-hispanic discrepancy along ethnicity to focus on race\n",
        "\n",
        "# Checking the composition of different topics in the dataset\n",
        "condition = reduced2_cdi_data[\"Topic\"] == \"Diabetes\"\n",
        "d_count = condition.sum()\n",
        "condition2 = reduced2_cdi_data[\"Topic\"] == \"Nutrition, Physical Activity, and Weight Status\"\n",
        "n_count = condition2.sum()\n",
        "condition3 = reduced2_cdi_data[\"Topic\"] == \"Cancer\"\n",
        "c_count = condition3.sum()\n",
        "print(f\"Diabetes Entry Count: {d_count}. Diabetes Percentage: {d_count/total_count*100}%\")\n",
        "print(f\"Nutrition Entry Count: {n_count}. Nutrition Percentage: {n_count/total_count*100}%\")\n",
        "print(f\"Cancer Entry Count: {c_count}. Cancer Percentage: {c_count/total_count*100}%\")\n",
        "\n",
        "# rename categorical symbols to match nutrition dataset\n",
        "race_name_change = {\n",
        "    'GENM': 'MALE',\n",
        "    'GENF': 'FEMALE',\n",
        "    'AIAN': 'RACENAA',\n",
        "    'ASN': 'RACEASN',\n",
        "    'WHT': 'RACEWHT',\n",
        "    'BLK': 'RACEBLK',\n",
        "    'OVR': 'OVERALL',\n",
        "    'API': 'RACEHPI',\n",
        "    'APIO': 'RACEHPI',\n",
        "    'AIAO': 'RACENAA',\n",
        "    'MRC': 'RACE2PLUS',\n",
        "    'OTH': 'RACEOTH'\n",
        "}\n",
        "\n",
        "reduced2_cdi_data['StratificationID1'] = reduced2_cdi_data['StratificationID1'].replace(race_name_change)\n",
        "\n",
        "# us changes: renaming + change Number for DataValueType to Value,\n",
        "reduced2_cdi_data['DataValueType'] = reduced2_cdi_data['DataValueType'].replace({'Number': 'Value'})\n",
        "# make sample size value -1 to denote no sample size provided\n",
        "cdi_column_renaming = {\n",
        "    \"DataValue\":\"Data_Value\",\n",
        "    \"DataValueAlt\":\"Data_Value_Alt\",\n",
        "    \"LowConfidenceLimit\":\"Low_Confidence_Limit\",\n",
        "    \"HighConfidenceLimit\":\"High_Confidence_Limit\",\n",
        "    \"StratificationCategoryID1\":\"StratificationCategoryId1\",\n",
        "}\n",
        "\n",
        "reduced2_cdi_data.rename(columns=cdi_column_renaming, inplace=True)\n",
        "\n",
        "# since sample sizes are provided in the nutrition dataset, make sample size value -1 to denote no sample size provided.\n",
        "reduced2_cdi_data[\"Sample_Size\"] = \"-1\"\n",
        "\n",
        "import re\n",
        "# switch coordinate from Point to tuple for potential location analysis with longitude-latitude coordinates\n",
        "def location_parsing_cdi(s):\n",
        "    if pd.isna(s):\n",
        "        return None\n",
        "    pattern = re.compile(r'[()]| ')\n",
        "    splits = re.split(pattern, s)\n",
        "    t = (float(splits[2]), float(splits[3]))\n",
        "    return t\n",
        "reduced2_cdi_data[\"GeoLocation\"] = reduced2_cdi_data[\"GeoLocation\"].apply(location_parsing_cdi)\n",
        "\n",
        "# get the types for the GeoLocation to ensure that there are only tuple types\n",
        "def get_column_types(column):\n",
        "    return set(column.apply(lambda entry: type(entry)))\n",
        "vals = get_column_types(reduced2_cdi_data[\"GeoLocation\"])\n",
        "\n",
        "# 2: Prepare the nutrition dataset\n",
        "nutrition_data = \"/content/drive/MyDrive/Datathon Practice/Nutrition_Physical_Activity_and_Obesity_Data.csv\" # can replace\n",
        "nutrition_df = pd.read_csv(nutrition_data)\n",
        "\n",
        "# drop uneeded columns\n",
        "dropped_nutrition_columns = [\"Datasource\", \"Data_Value_Unit\", \"Data_Value_Type\", \"Topic\", \"Data_Value_Footnote_Symbol\",\"Data_Value_Footnote\",\"Total\",\"Age(years)\",\"Education\", \"Gender\", \"Grade\", \"Income\", \"Race/Ethnicity\", \"DataValueTypeID\", \"StratificationCategory1\", \"Stratification1\", \"ClassID\"]\n",
        "reduced_nutrition_df = nutrition_df.drop(columns=dropped_nutrition_columns)\n",
        "# Rename class to topic\n",
        "reduced_nutrition_df.rename(columns={'Class': 'Topic'}, inplace=True)\n",
        "\n",
        "# parse the nutrition datset for the longitude and latitude values\n",
        "def location_parsing_nutrition(s):\n",
        "    if pd.isna(s):\n",
        "        return None\n",
        "    pattern = re.compile(r'[()]|(?:, )')\n",
        "    splits = re.split(pattern, s)\n",
        "    t = (float(splits[1]), float(splits[2]))\n",
        "    return t\n",
        "reduced_nutrition_df[\"GeoLocation\"] = reduced_nutrition_df[\"GeoLocation\"].apply(location_parsing_nutrition)\n",
        "\n",
        "n_vals = get_column_types(reduced_nutrition_df[\"GeoLocation\"])\n",
        "print(n_vals)\n",
        "\n",
        "# combine the dataframes\n",
        "merged_df = pd.concat([reduced_nutrition_df, reduced2_cdi_data])\n",
        "# reset the indices\n",
        "merged_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# 3: save the merged dataset as a csv\n",
        "merged_df.to_csv('/content/drive/MyDrive/Datathon Practice/health-region.csv', index=False)\n",
        "\n",
        "# ensure that the merged dataset has the desired features\n",
        "merged_df[\"Topic\"].unique()\n"
      ],
      "metadata": {
        "id": "tfC7gaTfo5w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ikenna's Code: Producing Regional Graphs"
      ],
      "metadata": {
        "id": "Yiqz2olUpByA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f54_-DNih0kf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# rename health_region_df to whatever the health-region.csv DF is called\n",
        "diabetes_df = health_region_df[health_region_df[\"Topic\"]==\"Diabetes\"]\n",
        "isNotTargetYear = diabetes_df[\"YearStart\"] >= 2011\n",
        "diabetes_target_years = diabetes_df[isNotTargetYear].reset_index()\n",
        "plt.hist(diabetes_target_years[\"YearStart\"])\n",
        "\n",
        "# get the topic questions for the pipeline\n",
        "print(diabetes_target_years[\"Question\"].unique())\n",
        "\n",
        "# make this a function\n",
        "def getCombo(df):\n",
        "  units = df[\"DataValueUnit\"].unique()\n",
        "  types = df[\"DataValueType\"].unique()\n",
        "  max_count = 0\n",
        "  final_combo = [\"\",\"\"]\n",
        "  dUnit = \"\"\n",
        "  dType = \"\"\n",
        "  for i in units:\n",
        "    dUnit = i\n",
        "    # check for none type: (should probably deal with this early when cleaning the data)\n",
        "    if i:\n",
        "      # get a df with all entries of that unit\n",
        "      unit_temp = df[df[\"DataValueUnit\"]==i]\n",
        "      # get the unit-type combo with the max entries\n",
        "      for j in types:\n",
        "        type_temp = unit_temp[unit_temp[\"DataValueType\"]==j]\n",
        "        # we'll prioritize age-adjusted over anything else\n",
        "        if j==\"Age-adjusted Rate\" and len(type_temp) > 0:\n",
        "          final_combo = [i,j]\n",
        "          return final_combo\n",
        "        if len(type_temp) > max_count:\n",
        "          max_count = len(type_temp)\n",
        "          # make this unit-type combo the one to track, only changes when max_count changes\n",
        "          final_combo = [i, j]\n",
        "  return final_combo\n",
        "\n",
        "def qSubset(df, questions, case_sensitive=False):\n",
        "    \"\"\"\n",
        "    Filter the DataFrame based on a list of questions.\n",
        "\n",
        "    Parameters:\n",
        "    df (pd.DataFrame): The DataFrame to filter.\n",
        "    questions (list): A list of question strings to filter by.\n",
        "    case_sensitive (bool): If True, perform case-sensitive filtering. Default is False.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: A DataFrame containing only the rows with questions in the provided list.\n",
        "    \"\"\"\n",
        "    if not case_sensitive:\n",
        "        questions = [q.lower() for q in questions]\n",
        "        output = df[df[\"Question\"].str.lower().isin(questions)]\n",
        "    else:\n",
        "        output = df[df[\"Question\"].isin(questions)]\n",
        "    return output\n",
        "\n",
        "def getGraph(df, question, combo, topic):\n",
        "  # Filter the DataFrame for questions related to diabetes mortality (only 5219 data points for mortality :/)\n",
        "  subset = df[(df[\"DataValueUnit\"]==combo[0]) & (df[\"DataValueType\"]==combo[1])].reset_index()\n",
        "\n",
        "  # Convert 'Data_Value' column to numeric, handling non-numeric values\n",
        "  subset['Data_Value'] = pd.to_numeric(subset['Data_Value'], errors='coerce')\n",
        "\n",
        "  # Group by state and calculate the average Data_Value\n",
        "  state_averages = subset.groupby('LocationAbbr')['Data_Value'].mean().reset_index()\n",
        "\n",
        "  # Create a choropleth map\n",
        "  fig = px.choropleth(\n",
        "      state_averages,\n",
        "      locations='LocationAbbr',\n",
        "      locationmode=\"USA-states\",\n",
        "      color='Data_Value',\n",
        "      color_continuous_scale=px.colors.sequential.Tealgrn,  # Optional: choose a color scale\n",
        "      scope=\"usa\",\n",
        "      title=topic\n",
        "  )\n",
        "\n",
        "  # Display the map\n",
        "  fig.show()\n",
        "\n",
        "### Look at diabetes mortality\n",
        "def regionDiabetesPipeline(topic_qs, topic):\n",
        "  subset = qSubset(diabetes_target_years, topic_qs)\n",
        "  combo = getCombo(subset)\n",
        "  getGraph(subset, topic_qs, combo, topic)\n",
        "  # f\"{topic} from 2011-2022. {combo[0]}, {combo[1]}\" initially printed out unit metrics to determine the best metrics\n",
        "mortality_qs = ['Mortality due to diabetes reported as any listed cause of death','Mortality with diabetic ketoacidosis reported as any listed cause of death', ]\n",
        "regionDiabetesPipeline(mortality_qs, \"Age-Adjusted Rate of Cases per 100,000 of Mortality due to Diabetes as any Listed Cause of Death from 2011 to 2022\")\n",
        "\n",
        "morbidity_qs = ['Amputation of a lower extremity attributable to diabetes']\n",
        "regionDiabetesPipeline(morbidity_qs, \"Age-Adjusted Rate of Cases per 10,000 of Amputations of a Lower Extremity Attributable to Diabetes from 2011 to 2022\")\n",
        "\n",
        "hospitalization_qs = ['Hospitalization with diabetes as a listed diagnosis']\n",
        "regionDiabetesPipeline(hospitalization_qs, \"Age-Adjusted Rate of Cases per 10,000 of Hospitalizations with Diabetes as a Listed Diagnosis from 2011 to 2022\")\n",
        "\n",
        "bloodPressure_qs = ['Prevalence of high blood pressure among adults aged >= 18 years with diagnosed diabetes']\n",
        "regionDiabetesPipeline(bloodPressure_qs, \"Age-Adjusted Rate of the Percentage of High Blood Pressure Prevalence Amongst Adults 18 years or Older with Diagnosed Diabetes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMEQdzplzIUO",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Oliver's code\n",
        "\n",
        "#@title Importing libraries\n",
        "# Run if it gives you an error for any of these; FEEL FREE TO IMPORT MORE FOR EVERYONE TO USE!\n",
        "\n",
        "\n",
        "# Data manipulation and analysis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "import seaborn as sns\n",
        "\n",
        "# Data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Machine learning\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# Statistical analysis\n",
        "# import statsmodels.api as sm\n",
        "\n",
        "# Natural Language Processing\n",
        "# import nltk\n",
        "# import spacy\n",
        "\n",
        "# Geospatial analysis\n",
        "# import geopandas as gpd\n",
        "# import folium\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "CWUrQiEmZbo9"
      },
      "outputs": [],
      "source": [
        "#@title Import Proxy Loan Dataset\n",
        "loans0 = import_dataset(\"1LUw8MtYYFNapLUmt-ISqk_YzglegoYut\", \"1991-1999\")\n",
        "loans1 = import_dataset(\"1Z1KjplSitwZaHU4bJrJMdFLTJBdXgxI7\", \"2000-2009\")\n",
        "loans2 = import_dataset(\"1uhW_2RkHtHXQxq4TpYq5UlcTg-4hrOvY\", \"2010-2019\")\n",
        "loans3 = import_dataset(\"11RhgGiwaiXnROI4ZBxIHfetVK_03urzz\", \"2010-present\")\n",
        "loans0 = loans0[['BorrName', 'BorrZip', 'ProjectCounty', \"NaicsCode\", 'NaicsDescription', 'FranchiseCode', 'FranchiseName', 'ApprovalFiscalYear', 'ApprovalDate']]\n",
        "loans1 = loans1[['BorrName', 'BorrZip', 'ProjectCounty', \"NaicsCode\", 'NaicsDescription', 'FranchiseCode', 'FranchiseName', 'ApprovalFiscalYear', 'ApprovalDate']]\n",
        "loans2 = loans2[['BorrName', 'BorrZip', 'ProjectCounty', \"NaicsCode\", 'NaicsDescription', 'FranchiseCode', 'FranchiseName', 'ApprovalFiscalYear', 'ApprovalDate']]\n",
        "loans3 = loans3[['BorrName', 'BorrZip', 'ProjectCounty', \"NaicsCode\", 'NaicsDescription', 'FranchiseCode', 'FranchiseName', 'ApprovalFiscalYear', 'ApprovalDate']]\n",
        "\n",
        "chain_names = ['POPEYE', 'MCDONALD', 'chick-fil-a', 'TACO BELL', 'DAIRY QUEEN', 'WAFFLE HOUSE', 'FRIED', 'KENTUCKY', 'BURGER', 'pizza', 'Arctic Circle', 'Chili', 'sonic drive', 'jack in the box', 'panda express', 'pei wei','papa john', 'little caesar', 'wendy\\'s', 'wingstop', 'zaxby', 'jimmy john', 'five guys', 'hardee', 'bojangle', 'carl\\'s jr', 'dunkin', 'krispy kreme', 'el pollo loco', 'shake shack', 'baskin robbins', 'church\\'s chicken', 'papa murphy\\'s', 'moe\\s', 'freddy\\'s frozen']\n",
        "chain_regex = '|'.join(chain_names)\n",
        "grocery_indicators = ['Grocery', 'Food store', 'bodega', 'food mart']\n",
        "grocery_regex = '|'.join(grocery_indicators)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "frIOvYpLZind"
      },
      "outputs": [],
      "source": [
        "#@title Pipeline for Getting Data (Example here based on Fiscal Year)\n",
        "for i in range(4):\n",
        "    var_name = f'loans{i}'\n",
        "    result_name = f'chains{i}'\n",
        "    globals()[result_name] = globals()[var_name][globals()[var_name]['BorrName'].str.contains(chain_regex, case=False, na=False)]\n",
        "    print(globals()[result_name]['ApprovalFiscalYear'].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ah9ZWNjbZ-c"
      },
      "outputs": [],
      "source": [
        "#@title Multivariate Regressions\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K67KOj3O82YW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPAVkNhG88Y3"
      },
      "outputs": [],
      "source": [
        "#@title Testing correlations\n",
        "\n",
        "clean = import_dataset('1Pi2g3ep0xoGuxJbqt6iHqYWoc3PRgOOq', 'clean')\n",
        "plt.scatter(np.array(clean['PCT_FREE_LUNCH15']), np.array(clean['Data_Value']))\n",
        "correlation = clean['PCT_FREE_LUNCH15'].corr(clean['Data_Value'])\n",
        "print(\"Correlation between SNAP and Diabetes:\", correlation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orBB2uVZ9Hwz"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "or-_4lLu9O6i"
      },
      "outputs": [],
      "source": [
        "X = clean[['PCT_FREE_LUNCH15', 'PCT_LACCESS_SNAP15', 'FFRPTH16', 'CONVSPTH16', 'PC_FFRSALES12']]\n",
        "y = clean['Data_Value']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Linear Regression\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_lr = lr_model.predict(X_test)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "print(\"Linear Regression MSE:\", mse_lr)\n",
        "print(\"Random Forest MSE:\", mse_rf)\n",
        "\n",
        "# Goal is to stay as close to the diagonal as possible\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(y_test, y_pred_lr, color='red', label='LR Predictions', alpha=0.5)\n",
        "plt.scatter(y_test, y_pred_rf, color='blue', label='RF Predictions', alpha=0.5)\n",
        "plt.title('Actual vs. Predicted Values')\n",
        "plt.xlabel('Actual Indicator')\n",
        "plt.ylabel('Predicted Indicator')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGTNsw8d9YS5"
      },
      "outputs": [],
      "source": [
        "# Creating a GBM model\n",
        "gbm = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Fitting the model\n",
        "gbm.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions\n",
        "y_pred_gbm = gbm.predict(X_test)\n",
        "\n",
        "# Evaluating the model\n",
        "mse = mean_squared_error(y_test, y_pred_gbm)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(y_test, y_pred_gbm, label='GBM Net Predictions', alpha=0.5)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4, label='True Rate')\n",
        "plt.title('Gradient Boosted Machine')\n",
        "plt.xlabel('Actual Diabetes Rate')\n",
        "plt.ylabel('Predicted Diabetes Rate')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Make a feature importance plot\n",
        "feature_importances = gbm.feature_importances_\n",
        "features = X.columns\n",
        "indices = np.argsort(feature_importances)\n",
        "\n",
        "# Make a fancier plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Feature Importances')\n",
        "plt.barh(range(len(indices)), feature_importances[indices], align='center', edgecolor='black')\n",
        "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "os4njxOv0DFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FORECASTING USING ARIMA MODEL"
      ],
      "metadata": {
        "id": "UxPNRxkvpMIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# necessary imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.stattools import adfuller, grangercausalitytests\n",
        "from statsmodels.tsa.api import VAR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.arima.model import ARIMA"
      ],
      "metadata": {
        "id": "0tjuUIk8zVw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plan:**\n",
        "\n",
        "Data Preparation: Use the provided data sections to gather relevant data.\n",
        "\n",
        "Stationarity Check: Ensure the data is stationary.\n",
        "\n",
        "ARIMA Model: Build and fit ARIMA models to forecast economic and health costs.\n",
        "\n",
        "VAR Model: Build and fit VAR models to capture the relationships between multiple time series.\n",
        "\n",
        "**Data Prep:**\n",
        "\n",
        "Include all these data components (will add on maybe)\n",
        "\n",
        "1. Fast Food Stock Prices\n",
        "\n",
        "2. Obesity Rates\n",
        "\n",
        "3. Unemployment Rates\n",
        "\n",
        "4. Meat Production (Red Meat, Poultry)\n",
        "\n",
        "Maybe...\n",
        "\n",
        "5. Commodity Prices (Corn, Coffee, Sugar)\n",
        "\n",
        "6. Physical Activity Data"
      ],
      "metadata": {
        "id": "gD1BUKoN10f_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# all datasets in one place\n",
        "stock_descriptions = import_dataset(\"1AzU_F3Usfx_iYRLH1KAMMaNZ3-9y0AwK\", \"stock_descriptions\")\n",
        "nutrition = import_dataset(\"1nxW91Jp65jQOdR_2_FJl1XaXDzhiHzgC\", \"nutrition\")\n",
        "meat_stats_slaughter_weights = import_dataset(\"1IHd-9p3aDoxbER3oBCZqBY23VVD5LOnu\", \"meat_stats_slaughter_weights\")\n",
        "meat_stats_slaughter_counts = import_dataset(\"1Eo58CgEeIk7Hnw_7I2yY46Ng6qiBLyeX\", \"meat_stats_slaughter_counts\")\n",
        "meat_stats_meat_production = import_dataset(\"1xZqVE4caTO4H60FFIP7Z5Lx1xCz0UuSd\", \"meat_stats_meat_production\")\n",
        "meat_stats_cold_storage = import_dataset(\"1YKBAaJKN_-RRp789RJ4wNYnpO40GyQqD\", \"meat_stats_cold_storage\")\n",
        "all_stocks = import_dataset(\"1hY7xiB-84DbqWhsZAazfwGbgtVxnBzDJ\", \"all_stocks\")\n",
        "all_commodities = import_dataset(\"1E8ELVRv2OFMbXwWYp2XVE1wH6Or6kPkB\", \"all_commodities\")\n",
        "acs5yr = import_dataset(\"1JFpNj9SlUiWwZTFc-pnlggKu6T7h_urP\", \"acs5yr\")\n",
        "agri_variable_list = import_dataset(\"1Y6Dv0yA8UXN5V5NhSdwUpT7JtzKj3oWm\", \"agri_variable_list\")\n",
        "agri_supplemental_county = import_dataset(\"1eM9e49xO841V-iR7wvFl1DPE-I8IRAKl\", \"agri_supplemental_county\")\n",
        "agri_supplemental_state = import_dataset(\"1BJsIiC3yPwtps1dA_-Ex-rUKzurce4c6\", \"agri_supplemental_state\")\n",
        "agri_state_and_county = import_dataset(\"1OFtRcynXKbMHFezP4ZLE5DEp6QUIGnHF\", \"agri_state_and_county\")\n",
        "employment = import_dataset(\"1x2uh0JILYFsarSP1h6Dtars79CtVYIP2\", \"employment\")\n",
        "health_region = import_dataset(\"18Y813FCdS2dXJSxOxUwcyVaAvwIxbOx-\", \"health_region\")\n",
        "social_determinants_of_health = import_dataset(\"17enYtkP62akioKX4_W9aup3pIiOyuwM8\", \"social_determinants_of_health\")\n",
        "# utf-8 encoding\n",
        "def import_dataset(file_id, name, encoding='utf-8'):\n",
        "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "    return pd.read_csv(url, encoding=encoding)\n",
        "\n",
        "try:\n",
        "    us_census_2020_2023 = import_dataset(\"1JHgbYOHYHR-Bdd1-NAv6laH8xMFZQJTq\", \"us_census_2020_2023\", encoding='latin1')\n",
        "except UnicodeDecodeError:\n",
        "    print(\"Failed to decode us_census_2020_2023 with 'latin1' encoding\")\n",
        "\n",
        "try:\n",
        "    us_census_2010_2020 = import_dataset(\"1aFonlFnV2dQKS4VgO4kgmsUaxYctWp8e\", \"us_census_2010_2020\", encoding='latin1')\n",
        "except UnicodeDecodeError:\n",
        "    print(\"Failed to decode us_census_2010_2020 with 'latin1' encoding\")\n",
        "\n",
        "try:\n",
        "    us_census_2000_2010 = import_dataset(\"1pWz7cBJmTBitrY6GJ_mwyISFijdbDIbK\", \"us_census_2000_2010\", encoding='latin1')\n",
        "except UnicodeDecodeError:\n",
        "    print(\"Failed to decode us_census_2000_2010 with 'latin1' encoding\")\n",
        "clean = import_dataset('1Pi2g3ep0xoGuxJbqt6iHqYWoc3PRgOOq', 'clean')"
      ],
      "metadata": {
        "id": "Fng0S2GN6fB1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "78756139-e8ec-473d-f567-52157ee661a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-3b849637e82a>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0magri_variable_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1Y6Dv0yA8UXN5V5NhSdwUpT7JtzKj3oWm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"agri_variable_list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0magri_supplemental_county\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1eM9e49xO841V-iR7wvFl1DPE-I8IRAKl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"agri_supplemental_county\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0magri_supplemental_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1BJsIiC3yPwtps1dA_-Ex-rUKzurce4c6\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"agri_supplemental_state\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0magri_state_and_county\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1OFtRcynXKbMHFezP4ZLE5DEp6QUIGnHF\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"agri_state_and_county\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0memployment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1x2uh0JILYFsarSP1h6Dtars79CtVYIP2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"employment\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-dffc45d9b11e>\u001b[0m in \u001b[0;36mimport_dataset\u001b[0;34m(file_id, name, encoding)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimport_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"https://drive.google.com/uc?id={file_id}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1706\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m     \u001b[0;31m# open URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m     ioargs = _get_filepath_or_buffer(\n\u001b[0m\u001b[1;32m    719\u001b[0m         \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;31m# assuming storage_options is to be interpreted as headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0mreq_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq_info\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m             \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Content-Encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"gzip\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    635\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0mhttp_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_302\u001b[0;34m(self, req, fp, code, msg, headers)\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0mhttp_error_301\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_error_303\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_error_307\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_error_302\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'urllib.Request'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0m\u001b[1;32m    537\u001b[0m                                   '_open', req)\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1391\u001b[0;31m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0m\u001b[1;32m   1392\u001b[0m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1350\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m             \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print info about all the data\n",
        "def print_dataset_info(dataset, dataset_name):\n",
        "    print(f\"Dataset: {dataset_name}\")\n",
        "    print(dataset.info())\n",
        "    print(\"\\nSample Data:\")\n",
        "    print(dataset.head())\n",
        "    print(\"\\n\")\n",
        "\n",
        "print_dataset_info(stock_descriptions, \"stock_descriptions\")\n",
        "print_dataset_info(nutrition, \"nutrition\")\n",
        "print_dataset_info(meat_stats_slaughter_weights, \"meat_stats_slaughter_weights\")\n",
        "print_dataset_info(meat_stats_slaughter_counts, \"meat_stats_slaughter_counts\")\n",
        "print_dataset_info(meat_stats_meat_production, \"meat_stats_meat_production\")\n",
        "print_dataset_info(meat_stats_cold_storage, \"meat_stats_cold_storage\")\n",
        "print_dataset_info(all_stocks, \"all_stocks\")\n",
        "print_dataset_info(all_commodities, \"all_commodities\")\n",
        "print_dataset_info(acs5yr, \"acs5yr\")\n",
        "print_dataset_info(agri_variable_list, \"agri_variable_list\")\n",
        "print_dataset_info(agri_supplemental_county, \"agri_supplemental_county\")\n",
        "print_dataset_info(agri_supplemental_state, \"agri_supplemental_state\")\n",
        "print_dataset_info(agri_state_and_county, \"agri_state_and_county\")\n",
        "print_dataset_info(employment, \"employment\")\n",
        "print_dataset_info(health_region, \"health_region\")\n",
        "print_dataset_info(social_determinants_of_health, \"social_determinants_of_health\")\n",
        "print_dataset_info(us_census_2020_2023, \"us_census_2020_2023\")\n",
        "print_dataset_info(us_census_2010_2020, \"us_census_2010_2020\")\n",
        "print_dataset_info(us_census_2000_2010, \"us_census_2000_2010\")\n",
        "print_dataset_info(clean, \"clean\")"
      ],
      "metadata": {
        "id": "P5DE2SZQ7BXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unemployment\n",
        "url = \"https://drive.google.com/uc?id=1x2uh0JILYFsarSP1h6Dtars79CtVYIP2\"\n",
        "employment = pd.read_csv(url, parse_dates=['Date'], index_col='Date')\n",
        "\n",
        "employment.index = pd.to_datetime(employment.index)\n",
        "employment = employment.asfreq('MS')  # Set frequency to start of month\n",
        "\n",
        "employment_rate = employment['Employment Rate']\n",
        "\n",
        "# plot time series\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(employment_rate, label='Employment Rate')\n",
        "plt.title('Employment Rate Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Employment Rate')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# check stationarity using Augmented Dickey-Fuller test\n",
        "def adf_test(series):\n",
        "    result = adfuller(series)\n",
        "    print('ADF Statistic:', result[0])\n",
        "    print('p-value:', result[1])\n",
        "    for key, value in result[4].items():\n",
        "        print('Critical Value {}: {:.3f}'.format(key, value))\n",
        "\n",
        "adf_test(employment_rate)\n",
        "\n",
        "# failed the ADF test so differentiate\n",
        "# first-order\n",
        "employment_rate_diff = employment_rate.diff().dropna()\n",
        "\n",
        "print(\"First-order differencing:\")\n",
        "adf_test(employment_rate_diff)\n",
        "\n",
        "# second-order\n",
        "employment_rate_diff2 = employment_rate.diff().diff().dropna()\n",
        "\n",
        "print(\"\\nSecond-order differencing:\")\n",
        "adf_test(employment_rate_diff2) # now it's stationary (p-value less than 0.05)\n",
        "\n",
        "# plot second-order diff\n",
        "employment_rate_diff2.plot(title='Second-order Differenced Employment Rate Over Time')\n",
        "plt.show()\n",
        "\n",
        "# figure out values for p and q\n",
        "# plot ACF and PACF for the second-order differenced series\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "plot_acf(employment_rate_diff2, ax=axes[0])\n",
        "axes[0].set_title('ACF Plot')\n",
        "plot_pacf(employment_rate_diff2, ax=axes[1])\n",
        "axes[1].set_title('PACF Plot')\n",
        "\n",
        "plt.show() # shows spikes - usually the higher the spike the greater the lag contributes\n",
        "\n",
        "# ARIMA model for unemployment\n",
        "# fit ARIMA model (p=2, d=2, q=1)\n",
        "\n",
        "model = ARIMA(employment_rate, order=(2, 2, 1))\n",
        "model_fit = model.fit()\n",
        "\n",
        "print(model_fit.summary())\n",
        "\n",
        "forecast_steps = 12  # Number of months to forecast\n",
        "forecast = model_fit.forecast(steps=forecast_steps)\n",
        "\n",
        "# plot forecase\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(employment_rate, label='Observed')\n",
        "plt.plot(forecast, label='Forecast', color='red')\n",
        "plt.title('Employment Rate Forecast')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oGMCX54Gc0Uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fast food stocks\n",
        "all_stocks = pd.read_csv('all_stocks.csv')\n",
        "stock_descriptions = pd.read_csv('stock_descriptions.csv')\n",
        "\n",
        "fast_food_stocks = stock_descriptions[stock_descriptions['Industry'].isin(['RETAIL-EATING PLACES', 'RETAIL-EATING & DRINKING PLACES'])]\n",
        "fast_food_stock_prices = all_stocks[all_stocks['Ticker_Symbol'].isin(fast_food_stocks['Symbol'])]\n",
        "\n",
        "fast_food_stock_prices['Date'] = pd.to_datetime(fast_food_stock_prices['Date-Time'])\n",
        "fast_food_stock_prices.set_index('Date', inplace=True)\n",
        "\n",
        "# monthly\n",
        "monthly_fast_food_stock_prices = fast_food_stock_prices['Close'].resample('M').mean()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(monthly_fast_food_stock_prices, label='Monthly Fast Food Stock Close Price')\n",
        "plt.title('Monthly Fast Food Stock Close Price Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Check stationarity using Augmented Dickey-Fuller test\n",
        "def adf_test(series):\n",
        "    result = adfuller(series)\n",
        "    print('ADF Statistic:', result[0])\n",
        "    print('p-value:', result[1])\n",
        "    for key, value in result[4].items():\n",
        "        print('Critical Value {}: {:.3f}'.format(key, value))\n",
        "\n",
        "adf_test(monthly_fast_food_stock_prices)\n",
        "\n",
        "# Check stationarity using Augmented Dickey-Fuller test\n",
        "def adf_test(series):\n",
        "    result = adfuller(series)\n",
        "    print('ADF Statistic:', result[0])\n",
        "    print('p-value:', result[1])\n",
        "    for key, value in result[4].items():\n",
        "        print('Critical Value {}: {:.3f}'.format(key, value))\n",
        "\n",
        "adf_test(monthly_fast_food_stock_prices)\n",
        "\n",
        "# first-order\n",
        "monthly_fast_food_stock_prices_diff = monthly_fast_food_stock_prices.diff().dropna()\n",
        "\n",
        "print(\"First-order differencing:\")\n",
        "adf_test(monthly_fast_food_stock_prices_diff)\n",
        "\n",
        "# second order\n",
        "monthly_fast_food_stock_prices_diff2 = monthly_fast_food_stock_prices_diff.diff().dropna()\n",
        "\n",
        "print(\"\\nSecond-order differencing:\")\n",
        "adf_test(monthly_fast_food_stock_prices_diff2)  # now it's stationary (p-value less than 0.05)\n",
        "\n",
        "# plot second-order differenced series\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(monthly_fast_food_stock_prices_diff2, label='Second-order Differenced Close Price')\n",
        "plt.title('Second-order Differenced Close Price Over Time')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Determine p and q values using ACF and PACF plots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "plot_acf(monthly_fast_food_stock_prices_diff2, ax=axes[0])\n",
        "axes[0].set_title('ACF Plot')\n",
        "plot_pacf(monthly_fast_food_stock_prices_diff2, ax=axes[1])\n",
        "axes[1].set_title('PACF Plot')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# fit ARIMA\n",
        "model = ARIMA(monthly_fast_food_stock_prices, order=(2, 2, 1))\n",
        "model_fit = model.fit()\n",
        "\n",
        "print(model_fit.summary())\n",
        "\n",
        "forecast_steps = 12  # Number of months to forecast\n",
        "forecast = model_fit.forecast(steps=forecast_steps)\n",
        "\n",
        "# Plot forecast\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(monthly_fast_food_stock_prices, label='Observed')\n",
        "plt.plot(forecast, label='Forecast', color='red')\n",
        "plt.title('Fast Food Stock Close Price Forecast')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Ssi3VQ0V5JDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "msg0-jdpsRx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meat_stats_cold_storage = pd.read_csv('meat_stats_cold_storage.csv')\n",
        "meat_stats_cold_storage['Date'] = pd.to_datetime(meat_stats_cold_storage['Date'])\n",
        "\n",
        "# Aggregate data by month and type of meat\n",
        "cold_storage_agg = meat_stats_cold_storage.groupby([meat_stats_cold_storage['Date'].dt.to_period('M'), 'Type_Of_Meat'])['Weight'].mean().unstack(fill_value=0)\n",
        "cold_storage_agg.columns = ['ColdStorageRedMeat', 'ColdStoragePoultry']\n",
        "cold_storage_agg.index = cold_storage_agg.index.to_timestamp()\n",
        "\n",
        "# Plot time series for red meat and poultry\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(cold_storage_agg['ColdStorageRedMeat'], label='Cold Storage Red Meat')\n",
        "plt.plot(cold_storage_agg['ColdStoragePoultry'], label='Cold Storage Poultry')\n",
        "plt.title('Cold Storage Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Weight (Million Pounds)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Function to perform ADF test\n",
        "def adf_test(series):\n",
        "    result = adfuller(series)\n",
        "    print('ADF Statistic:', result[0])\n",
        "    print('p-value:', result[1])\n",
        "    for key, value in result[4].items():\n",
        "        print('Critical Value {}: {:.3f}'.format(key, value))\n",
        "\n",
        "# Red Meat Analysis\n",
        "print(\"\\nCold Storage Red Meat Analysis:\")\n",
        "adf_test(cold_storage_agg['ColdStorageRedMeat'])\n",
        "\n",
        "# First-order differencing\n",
        "cold_storage_red_meat_diff = cold_storage_agg['ColdStorageRedMeat'].diff().dropna()\n",
        "\n",
        "print(\"First-order differencing:\")\n",
        "adf_test(cold_storage_red_meat_diff)\n",
        "\n",
        "# Plot first-order differenced series\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(cold_storage_red_meat_diff, label='First-order Differenced Cold Storage Red Meat')\n",
        "plt.title('First-order Differenced Cold Storage Red Meat Over Time')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Determine p and q values using ACF and PACF plots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "plot_acf(cold_storage_red_meat_diff, ax=axes[0])\n",
        "axes[0].set_title('ACF Plot - Red Meat')\n",
        "plot_pacf(cold_storage_red_meat_diff, ax=axes[1])\n",
        "axes[1].set_title('PACF Plot - Red Meat')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Fit ARIMA model for Red Meat (p=2, d=2, q=1)\n",
        "model_red_meat = ARIMA(cold_storage_agg['ColdStorageRedMeat'], order=(2, 1, 2))\n",
        "model_fit_red_meat = model_red_meat.fit()\n",
        "\n",
        "print(model_fit_red_meat.summary())\n",
        "\n",
        "forecast_steps = 24  # months\n",
        "forecast_red_meat = model_fit_red_meat.forecast(steps=forecast_steps)\n",
        "\n",
        "# plot forecast for Red Meat\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(cold_storage_agg['ColdStorageRedMeat'], label='Observed - Red Meat')\n",
        "plt.plot(forecast_red_meat, label='Forecast - Red Meat', color='red')\n",
        "plt.title('Cold Storage Red Meat Forecast')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Poultry Analysis\n",
        "print(\"\\nCold Storage Poultry Analysis:\")\n",
        "adf_test(cold_storage_agg['ColdStoragePoultry'])\n",
        "\n",
        "# First-order differencing\n",
        "cold_storage_poultry_diff = cold_storage_agg['ColdStoragePoultry'].diff().dropna()\n",
        "\n",
        "print(\"First-order differencing:\")\n",
        "adf_test(cold_storage_poultry_diff)\n",
        "\n",
        "# Plot first-order differenced series\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(cold_storage_poultry_diff, label='First-order Differenced Cold Storage Poultry')\n",
        "plt.title('First-order Differenced Cold Storage Poultry Over Time')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Determine p and q values using ACF and PACF plots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "plot_acf(cold_storage_poultry_diff, ax=axes[0])\n",
        "axes[0].set_title('ACF Plot - Poultry')\n",
        "plot_pacf(cold_storage_poultry_diff, ax=axes[1])\n",
        "axes[1].set_title('PACF Plot - Poultry')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Fit ARIMA model for Poultry (p=2, d=2, q=1)\n",
        "model_poultry = ARIMA(cold_storage_agg['ColdStoragePoultry'], order=(2, 1, 2))\n",
        "model_fit_poultry = model_poultry.fit()\n",
        "\n",
        "print(model_fit_poultry.summary())\n",
        "\n",
        "forecast_poultry_steps = 24  # Number of months to forecast\n",
        "forecast_poultry = model_fit_poultry.forecast(steps=forecast_poultry_steps)\n",
        "\n",
        "# Plot forecast for Poultry\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(cold_storage_agg['ColdStoragePoultry'], label='Observed - Poultry')\n",
        "plt.plot(forecast_poultry, label='Forecast - Poultry', color='red')\n",
        "plt.title('Cold Storage Poultry Forecast')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "c83nE712pIxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ARIMA seasonality for cold storage meats\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "meat_stats_cold_storage = pd.read_csv('meat_stats_cold_storage.csv')\n",
        "meat_stats_cold_storage['Date'] = pd.to_datetime(meat_stats_cold_storage['Date'])\n",
        "meat_stats_cold_storage.set_index('Date', inplace=True)\n",
        "\n",
        "# aggregate data by month and type of meat\n",
        "cold_storage_agg = meat_stats_cold_storage.groupby([pd.Grouper(freq='M'), 'Type_Of_Meat'])['Weight'].mean().unstack(fill_value=0)\n",
        "cold_storage_agg.columns = ['ColdStorageRedMeat', 'ColdStoragePoultry']\n",
        "\n",
        "# decompose for seasonal\n",
        "decompose_red_meat = seasonal_decompose(cold_storage_agg['ColdStorageRedMeat'], model='additive', period=12)\n",
        "decompose_red_meat.plot()\n",
        "plt.show()\n",
        "\n",
        "decompose_poultry = seasonal_decompose(cold_storage_agg['ColdStoragePoultry'], model='additive', period=12)\n",
        "decompose_poultry.plot()\n",
        "plt.show()\n",
        "\n",
        "# seasonal differencing (to make the series stationary)\n",
        "cold_storage_agg['RedMeat_diff'] = cold_storage_agg['ColdStorageRedMeat'].diff(12).dropna()\n",
        "cold_storage_agg['Poultry_diff'] = cold_storage_agg['ColdStoragePoultry'].diff(12).dropna()\n",
        "\n",
        "# check stationarity\n",
        "def adf_test(series):\n",
        "    result = adfuller(series)\n",
        "    print('ADF Statistic:', result[0])\n",
        "    print('p-value:', result[1])\n",
        "    for key, value in result[4].items():\n",
        "        print('Critical Value {}: {:.3f}'.format(key, value))\n",
        "\n",
        "print(\"ADF Test for Red Meat (Seasonal Differencing):\")\n",
        "adf_test(cold_storage_agg['RedMeat_diff'].dropna())\n",
        "\n",
        "print(\"\\nADF Test for Poultry (Seasonal Differencing):\")\n",
        "adf_test(cold_storage_agg['Poultry_diff'].dropna())\n",
        "\n",
        "# ACF and PACF plots for seasonally differenced series\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "plot_acf(cold_storage_agg['RedMeat_diff'].dropna(), ax=axes[0])\n",
        "axes[0].set_title('ACF Plot - Red Meat (Seasonal Differencing)')\n",
        "plot_pacf(cold_storage_agg['RedMeat_diff'].dropna(), ax=axes[1])\n",
        "axes[1].set_title('PACF Plot - Red Meat (Seasonal Differencing)')\n",
        "plt.show()\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "plot_acf(cold_storage_agg['Poultry_diff'].dropna(), ax=axes[0])\n",
        "axes[0].set_title('ACF Plot - Poultry (Seasonal Differencing)')\n",
        "plot_pacf(cold_storage_agg['Poultry_diff'].dropna(), ax=axes[1])\n",
        "axes[1].set_title('PACF Plot - Poultry (Seasonal Differencing)')\n",
        "plt.show()\n",
        "\n",
        "# fir SARIMA model for Red Meat\n",
        "model_red_meat = SARIMAX(cold_storage_agg['ColdStorageRedMeat'], order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\n",
        "model_fit_red_meat = model_red_meat.fit(disp=False)\n",
        "print(model_fit_red_meat.summary())\n",
        "\n",
        "# forecast\n",
        "forecast_steps = 24\n",
        "forecast_red_meat = model_fit_red_meat.forecast(steps=forecast_steps)\n",
        "\n",
        "# plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(cold_storage_agg['ColdStorageRedMeat'], label='Observed - Red Meat')\n",
        "plt.plot(forecast_red_meat, label='Forecast - Red Meat', color='red')\n",
        "plt.title('Cold Storage Red Meat Forecast')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# fit SARIMA model for Poultry\n",
        "model_poultry = SARIMAX(cold_storage_agg['ColdStoragePoultry'], order=(1, 1, 1), seasonal_order=(2, 1, 1, 12))\n",
        "model_fit_poultry = model_poultry.fit(disp=False)\n",
        "print(model_fit_poultry.summary())\n",
        "\n",
        "forecast_poultry = model_fit_poultry.forecast(steps=forecast_steps)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(cold_storage_agg['ColdStoragePoultry'], label='Observed - Poultry')\n",
        "plt.plot(forecast_poultry, label='Forecast - Poultry', color='red')\n",
        "plt.title('Cold Storage Poultry Forecast')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KiIBZXXv1vQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VAR MODELS"
      ],
      "metadata": {
        "id": "CYNN5A9M5u5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load data\n",
        "meat_stats_cold_storage = pd.read_csv('meat_stats_cold_storage.csv')\n",
        "\n",
        "# Print the first few rows of the DataFrame\n",
        "print(meat_stats_cold_storage.head())\n",
        "\n",
        "# Print the data types of each column\n",
        "print(meat_stats_cold_storage.dtypes)\n",
        "\n",
        "# Convert 'Date' column to datetime format and set as index\n",
        "meat_stats_cold_storage['Date'] = pd.to_datetime(meat_stats_cold_storage['Date'], errors='coerce')\n",
        "print(meat_stats_cold_storage.head())\n",
        "print(meat_stats_cold_storage.dtypes)\n",
        "\n",
        "# Ensure only numeric columns are included\n",
        "print(meat_stats_cold_storage['Weight'].unique())\n",
        "print(meat_stats_cold_storage['Weight'].head())\n",
        "\n",
        "numeric_meat_storage = meat_stats_cold_storage[['Weight']]\n",
        "print(numeric_meat_storage.head())\n"
      ],
      "metadata": {
        "id": "2yr0BZ8I5wYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from statsmodels.tsa.api import VAR\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "meat_stats_meat_production = pd.read_csv('meat_stats_meat_production.csv')\n",
        "all_stocks = pd.read_csv('all_stocks.csv')\n",
        "stock_descriptions = pd.read_csv('stock_descriptions.csv')\n",
        "\n",
        "meat_stats_meat_production['Date'] = pd.to_datetime(meat_stats_meat_production['Date'], format='%b-%Y')\n",
        "meat_stats_meat_production.set_index('Date', inplace=True)\n",
        "\n",
        "# convert 'Production' to numeric\n",
        "meat_stats_meat_production['Production'] = meat_stats_meat_production['Production'].str.replace(',', '').astype(float)\n",
        "\n",
        "print(\"Missing values in meat production data before handling:\")\n",
        "print(meat_stats_meat_production.isnull().sum())\n",
        "meat_stats_meat_production = meat_stats_meat_production.dropna()\n",
        "print(\"Missing values in meat production data after handling:\")\n",
        "print(meat_stats_meat_production.isnull().sum())\n",
        "\n",
        "print(\"Unique values in 'Animal' column:\")\n",
        "print(meat_stats_meat_production['Animal'].unique())\n",
        "print(\"Unique values in 'Type of Meat' column:\")\n",
        "print(meat_stats_meat_production['Type of Meat'].unique())\n",
        "\n",
        "# filter data for 'Red Meat'\n",
        "red_meat_production = meat_stats_meat_production.loc[meat_stats_meat_production['Type of Meat'] == 'Red Meat', 'Production']\n",
        "\n",
        "print(\"Filtered red meat production data:\")\n",
        "print(red_meat_production.head())\n",
        "print(\"Filtered red meat production data info:\")\n",
        "print(red_meat_production.info())\n",
        "\n",
        "fast_food_stocks = stock_descriptions[stock_descriptions['Industry'].isin(['RETAIL-EATING PLACES', 'RETAIL-EATING & DRINKING PLACES'])]\n",
        "fast_food_stock_prices = all_stocks[all_stocks['Ticker_Symbol'].isin(fast_food_stocks['Symbol'])]\n",
        "fast_food_stock_prices['Date'] = pd.to_datetime(fast_food_stock_prices['Date-Time'])\n",
        "fast_food_stock_prices.set_index('Date', inplace=True)\n",
        "\n",
        "monthly_fast_food_stock_prices = fast_food_stock_prices['Close'].resample('M').mean()\n",
        "\n",
        "print(\"Resampled fast food stock prices monthly data:\")\n",
        "print(monthly_fast_food_stock_prices.head())\n",
        "\n",
        "red_meat_production_monthly = red_meat_production.resample('M').mean()\n",
        "\n",
        "print(\"Resampled red meat production monthly data:\")\n",
        "print(red_meat_production_monthly.head())\n",
        "\n",
        "# common date ranges\n",
        "start_date = max(monthly_fast_food_stock_prices.index.min(), red_meat_production_monthly.index.min())\n",
        "end_date = min(monthly_fast_food_stock_prices.index.max(), red_meat_production_monthly.index.max())\n",
        "\n",
        "print(f\"Start date: {start_date}\")\n",
        "print(f\"End date: {end_date}\")\n",
        "\n",
        "aligned_red_meat_production = red_meat_production_monthly.loc[start_date:end_date]\n",
        "aligned_fast_food_stock_prices = monthly_fast_food_stock_prices.loc[start_date:end_date]\n",
        "\n",
        "print(\"Aligned red meat production monthly data:\")\n",
        "print(aligned_red_meat_production.head())\n",
        "print(\"Aligned fast food stock prices monthly data:\")\n",
        "print(aligned_fast_food_stock_prices.head())\n",
        "\n",
        "# combine into a single DataFrame\n",
        "data = pd.concat([aligned_red_meat_production, aligned_fast_food_stock_prices], axis=1)\n",
        "data.columns = ['RedMeatProduction', 'Fast_Food_Stock_Price']\n",
        "\n",
        "data = data.dropna()\n",
        "\n",
        "print(\"Missing values in combined data after resampling and dropping:\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "print(\"Head of combined data:\")\n",
        "print(data.head())\n",
        "\n",
        "# determin num of observations and adjust maxlags accordingly\n",
        "num_observations = len(data)\n",
        "maxlags = min(15, num_observations // 3)  # Use a smaller maxlag\n",
        "\n",
        "print(f\"Number of observations: {num_observations}\")\n",
        "print(f\"Maxlags set to: {maxlags}\")\n",
        "\n",
        "if num_observations > 0 and maxlags > 0:\n",
        "    # fit VAR model with the adjusted maxlags value\n",
        "    model = VAR(data)\n",
        "    results = model.fit(maxlags=maxlags, ic='aic')\n",
        "\n",
        "    print(results.summary())\n",
        "\n",
        "    # forecast\n",
        "    forecast_steps = 12\n",
        "    forecast = results.forecast(data.values[-results.k_ar:], steps=forecast_steps)\n",
        "    forecast_index = pd.date_range(start=data.index[-1] + pd.DateOffset(months=1), periods=forecast_steps, freq='M')\n",
        "    forecast_df = pd.DataFrame(forecast, index=forecast_index, columns=['RedMeatProduction', 'Fast_Food_Stock_Price'])\n",
        "\n",
        "    # plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(data['RedMeatProduction'], label='Observed Red Meat Production')\n",
        "    plt.plot(forecast_df['RedMeatProduction'], label='Forecast Red Meat Production', linestyle='--')\n",
        "    plt.plot(data['Fast_Food_Stock_Price'], label='Observed Fast Food Stock Price')\n",
        "    plt.plot(forecast_df['Fast_Food_Stock_Price'], label='Forecast Fast Food Stock Price', linestyle='--')\n",
        "    plt.title('Red Meat Production and Fast Food Stock Price Forecast')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Values')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Granger Causality Test\n",
        "    max_lag = 2\n",
        "    test_result = grangercausalitytests(data[['Fast_Food_Stock_Price', 'RedMeatProduction']], maxlag=max_lag, verbose=True)\n",
        "\n",
        "    # extract p-values from Granger Causality Test results\n",
        "    granger_p_values = {key: val[0]['ssr_ftest'][1] for key, val in test_result.items()}\n",
        "    print(\"Granger Causality Test p-values:\", granger_p_values)\n",
        "else:\n",
        "    print(\"Not enough observations to fit the model.\")\n"
      ],
      "metadata": {
        "id": "A60R5Hi0P6OM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from statsmodels.tsa.api import VAR\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "meat_stats_meat_production = pd.read_csv('meat_stats_meat_production.csv')\n",
        "all_stocks = pd.read_csv('all_stocks.csv')\n",
        "stock_descriptions = pd.read_csv('stock_descriptions.csv')\n",
        "\n",
        "# Check initial data\n",
        "print(\"Initial meat_stats_meat_production:\")\n",
        "print(meat_stats_meat_production.head())\n",
        "print(\"Data types of columns in meat_stats_meat_production:\")\n",
        "print(meat_stats_meat_production.dtypes)\n",
        "\n",
        "# Investigate 'Production' column\n",
        "print(\"Sample values from 'Production' column:\")\n",
        "print(meat_stats_meat_production['Production'].head(20))\n",
        "\n",
        "# Preprocess meat production data\n",
        "meat_stats_meat_production['Date'] = pd.to_datetime(meat_stats_meat_production['Date'], format='%b-%Y')\n",
        "meat_stats_meat_production.set_index('Date', inplace=True)\n",
        "\n",
        "# Check for missing values in meat production data\n",
        "print(\"Missing values in meat production data before handling:\")\n",
        "print(meat_stats_meat_production.isnull().sum())\n",
        "\n",
        "# Handle missing values by dropping them\n",
        "meat_stats_meat_production = meat_stats_meat_production.dropna()\n",
        "\n",
        "# Verify no missing values remain\n",
        "print(\"Missing values in meat production data after handling:\")\n",
        "print(meat_stats_meat_production.isnull().sum())\n",
        "\n",
        "# Print unique values in 'Animal' column to verify filtering\n",
        "print(\"Unique values in 'Animal' column:\")\n",
        "print(meat_stats_meat_production['Animal'].unique())\n",
        "\n",
        "# Filter data for 'Red Meat'\n",
        "red_meat_production = meat_stats_meat_production.loc[meat_stats_meat_production['Animal'] == 'Red Meat', 'Production'].astype(float)\n",
        "\n",
        "# Print filtered red meat production data\n",
        "print(\"Filtered red meat production data:\")\n",
        "print(red_meat_production.head())\n",
        "\n",
        "# Preprocess stock data\n",
        "print(\"Initial all_stocks data:\")\n",
        "print(all_stocks.head())\n",
        "\n",
        "fast_food_stocks = stock_descriptions[stock_descriptions['Industry'].isin(['RETAIL-EATING PLACES', 'RETAIL-EATING & DRINKING PLACES'])]\n",
        "fast_food_stock_prices = all_stocks[all_stocks['Ticker_Symbol'].isin(fast_food_stocks['Symbol'])]\n",
        "fast_food_stock_prices['Date'] = pd.to_datetime(fast_food_stock_prices['Date-Time'])\n",
        "fast_food_stock_prices.set_index('Date', inplace=True)\n",
        "\n",
        "# Print preprocessed fast food stock prices data\n",
        "print(\"Preprocessed fast food stock prices data:\")\n",
        "print(fast_food_stock_prices.head())\n",
        "\n",
        "# Resample to monthly data and take mean of 'Close' price\n",
        "monthly_fast_food_stock_prices = fast_food_stock_prices['Close'].resample('M').mean()\n",
        "\n",
        "# Resample red meat production to monthly data\n",
        "red_meat_production_monthly = red_meat_production.resample('M').mean()\n",
        "\n",
        "# Print resampled data\n",
        "print(\"Resampled red meat production monthly data:\")\n",
        "print(red_meat_production_monthly.head())\n",
        "print(\"Resampled fast food stock prices monthly data:\")\n",
        "print(monthly_fast_food_stock_prices.head())\n",
        "\n",
        "# Align both time series to the same time range\n",
        "start_date = max(red_meat_production_monthly.index.min(), monthly_fast_food_stock_prices.index.min())\n",
        "end_date = min(red_meat_production_monthly.index.max(), monthly_fast_food_stock_prices.index.max())\n",
        "\n",
        "print(f\"Start date: {start_date}\")\n",
        "print(f\"End date: {end_date}\")\n",
        "\n",
        "red_meat_production_monthly = red_meat_production_monthly.loc[start_date:end_date]\n",
        "monthly_fast_food_stock_prices = monthly_fast_food_stock_prices.loc[start_date:end_date]\n",
        "\n",
        "# Print aligned data\n",
        "print(\"Aligned red meat production monthly data:\")\n",
        "print(red_meat_production_monthly.head())\n",
        "print(\"Aligned fast food stock prices monthly data:\")\n",
        "print(monthly_fast_food_stock_prices.head())\n",
        "\n",
        "# Combine into a single DataFrame\n",
        "data = pd.concat([red_meat_production_monthly, monthly_fast_food_stock_prices], axis=1)\n",
        "data.columns = ['RedMeatProduction', 'Fast_Food_Stock_Price']\n",
        "\n",
        "# Drop any remaining missing values\n",
        "data = data.dropna()\n",
        "\n",
        "# Ensure no missing values\n",
        "print(\"Missing values in combined data after resampling and dropping:\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Print combined data\n",
        "print(\"Head of combined data:\")\n",
        "print(data.head())\n",
        "\n",
        "# Determine the number of observations and adjust maxlags accordingly\n",
        "num_observations = len(data)\n",
        "maxlags = num_observations // 3  # A rule of thumb is to use a maxlag that is a fraction of the total number of observations\n",
        "\n",
        "print(f\"Number of observations: {num_observations}\")\n",
        "print(f\"Maxlags set to: {maxlags}\")\n",
        "\n",
        "# Check if we have enough observations to fit the model\n",
        "if num_observations > 0:\n",
        "    # Fit VAR model with the adjusted maxlags value\n",
        "    model = VAR(data)\n",
        "    results = model.fit(maxlags=maxlags, ic='aic')\n",
        "\n",
        "    # Print summary of the model\n",
        "    print(results.summary())\n",
        "\n",
        "    # Forecast the next 12 months\n",
        "    forecast_steps = 12\n",
        "    forecast = results.forecast(data.values[-results.k_ar:], steps=forecast_steps)\n",
        "    forecast_index = pd.date_range(start=data.index[-1] + pd.DateOffset(months=1), periods=forecast_steps, freq='M')\n",
        "    forecast_df = pd.DataFrame(forecast, index=forecast_index, columns=['RedMeatProduction', 'Fast_Food_Stock_Price'])\n",
        "\n",
        "    # Plot the forecast\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(data['RedMeatProduction'], label='Observed Red Meat Production')\n",
        "    plt.plot(forecast_df['RedMeatProduction'], label='Forecast Red Meat Production', linestyle='--')\n",
        "    plt.plot(data['Fast_Food_Stock_Price'], label='Observed Fast Food Stock Price')\n",
        "    plt.plot(forecast_df['Fast_Food_Stock_Price'], label='Forecast Fast Food Stock Price', linestyle='--')\n",
        "    plt.title('Red Meat Production and Fast Food Stock Price Forecast')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Values')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Granger Causality Test\n",
        "    max_lag = 2\n",
        "    test_result = grangercausalitytests(data[['Fast_Food_Stock_Price', 'RedMeatProduction']], maxlag=max_lag, verbose=True)\n",
        "\n",
        "    # Extract p-values from Granger Causality Test results\n",
        "    granger_p_values = {key: val[0]['ssr_ftest'][1] for key, val in test_result.items()}\n",
        "    print(\"Granger Causality Test p-values:\", granger_p_values)\n",
        "else:\n",
        "    print(\"Not enough observations to fit the model.\")\n"
      ],
      "metadata": {
        "id": "pSv3K_6V5OeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VAR of Fast Food Stock Prices, Poultry Production, Red Meat Production, Cold Storage Poultry, Cold Storage Red Meat Levels\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.api import VAR\n",
        "\n",
        "# Load datasets\n",
        "def import_dataset(file_id, name, encoding='utf-8'):\n",
        "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "    return pd.read_csv(url, encoding=encoding)\n",
        "\n",
        "meat_stats_meat_production = import_dataset(\"1xZqVE4caTO4H60FFIP7Z5Lx1xCz0UuSd\", \"meat_stats_meat_production\")\n",
        "meat_stats_cold_storage = import_dataset(\"1YKBAaJKN_-RRp789RJ4wNYnpO40GyQqD\", \"meat_stats_cold_storage\")\n",
        "all_stocks = import_dataset(\"1hY7xiB-84DbqWhsZAazfwGbgtVxnBzDJ\", \"all_stocks\")\n",
        "stock_descriptions = import_dataset(\"1AzU_F3Usfx_iYRLH1KAMMaNZ3-9y0AwK\", \"stock_descriptions\")\n",
        "employment = import_dataset(\"1x2uh0JILYFsarSP1h6Dtars79CtVYIP2\", \"employment\")\n",
        "\n",
        "# Convert 'Date' columns to datetime format\n",
        "meat_stats_meat_production['Date'] = pd.to_datetime(meat_stats_meat_production['Date'], format='%b-%Y')\n",
        "meat_stats_cold_storage['Date'] = pd.to_datetime(meat_stats_cold_storage['Date'])\n",
        "all_stocks['Date'] = pd.to_datetime(all_stocks['Date-Time'])\n",
        "employment['Date'] = pd.to_datetime(employment['Date'])\n",
        "\n",
        "# Set 'Date' as index\n",
        "meat_stats_meat_production.set_index('Date', inplace=True)\n",
        "meat_stats_cold_storage.set_index('Date', inplace=True)\n",
        "all_stocks.set_index('Date', inplace=True)\n",
        "employment.set_index('Date', inplace=True)\n",
        "\n",
        "# Convert 'Production' to numeric\n",
        "meat_stats_meat_production['Production'] = meat_stats_meat_production['Production'].str.replace(',', '').astype(float)\n",
        "\n",
        "# Handle missing values\n",
        "meat_stats_meat_production = meat_stats_meat_production.dropna()\n",
        "meat_stats_cold_storage = meat_stats_cold_storage.dropna()\n",
        "employment = employment.dropna()\n",
        "\n",
        "# Filter data for 'Red Meat' and 'Poultry'\n",
        "red_meat_production = meat_stats_meat_production.loc[meat_stats_meat_production['Type of Meat'] == 'Red Meat', 'Production']\n",
        "poultry_production = meat_stats_meat_production.loc[meat_stats_meat_production['Type of Meat'] == 'Poultry', 'Production']\n",
        "\n",
        "# Filter fast food stocks\n",
        "fast_food_stocks = stock_descriptions[stock_descriptions['Industry'].isin(['RETAIL-EATING PLACES', 'RETAIL-EATING & DRINKING PLACES'])]\n",
        "fast_food_stock_prices = all_stocks[all_stocks['Ticker_Symbol'].isin(fast_food_stocks['Symbol'])]\n",
        "\n",
        "# Resample to monthly data\n",
        "monthly_fast_food_stock_prices = fast_food_stock_prices['Close'].resample('M').mean()\n",
        "red_meat_production_monthly = red_meat_production.resample('M').mean()\n",
        "poultry_production_monthly = poultry_production.resample('M').mean()\n",
        "\n",
        "# Aggregate cold storage data by month and type of meat\n",
        "cold_storage_agg = meat_stats_cold_storage.groupby([pd.Grouper(freq='M'), 'Type_Of_Meat'])['Weight'].mean().unstack(fill_value=0)\n",
        "cold_storage_agg.columns = ['ColdStorageRedMeat', 'ColdStoragePoultry']\n",
        "\n",
        "# Align both time series to the same time range\n",
        "start_date = max(monthly_fast_food_stock_prices.index.min(), red_meat_production_monthly.index.min(), poultry_production_monthly.index.min(), cold_storage_agg.index.min())\n",
        "end_date = min(monthly_fast_food_stock_prices.index.max(), red_meat_production_monthly.index.max(), poultry_production_monthly.index.max(), cold_storage_agg.index.max())\n",
        "\n",
        "monthly_fast_food_stock_prices = monthly_fast_food_stock_prices.loc[start_date:end_date]\n",
        "red_meat_production_monthly = red_meat_production_monthly.loc[start_date:end_date]\n",
        "poultry_production_monthly = poultry_production_monthly.loc[start_date:end_date]\n",
        "cold_storage_agg = cold_storage_agg.loc[start_date:end_date]\n",
        "\n",
        "# Combine into a single DataFrame\n",
        "data = pd.concat([monthly_fast_food_stock_prices, red_meat_production_monthly, poultry_production_monthly, cold_storage_agg], axis=1)\n",
        "data.columns = ['Fast_Food_Stock_Price', 'RedMeatProduction', 'PoultryProduction', 'ColdStorageRedMeat', 'ColdStoragePoultry']\n",
        "\n",
        "# Drop any remaining missing values\n",
        "data = data.dropna()\n",
        "\n",
        "# Add employment data\n",
        "employment_rate = employment['Unemployment Rate'].resample('M').mean().loc[start_date:end_date]\n",
        "data['Unemployment_Rate'] = employment_rate\n",
        "\n",
        "# Ensure no NaN or infinite values\n",
        "data = data.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "# Check if there are any NaNs or infinite values left\n",
        "print(\"Missing values in combined data after resampling and dropping:\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "print(\"Head of combined data:\")\n",
        "print(data.head())\n",
        "\n",
        "# Determine the number of observations and adjust maxlags accordingly\n",
        "num_observations = len(data)\n",
        "maxlags = min(15, num_observations // 3)  # Use a smaller maxlag\n",
        "\n",
        "print(f\"Number of observations: {num_observations}\")\n",
        "print(f\"Maxlags set to: {maxlags}\")\n",
        "\n",
        "if num_observations > 0 and maxlags > 0:\n",
        "    # Fit VAR model with the adjusted maxlags value\n",
        "    model = VAR(data)\n",
        "    results = model.fit(maxlags=maxlags, ic='aic')\n",
        "\n",
        "    # Print summary of the model\n",
        "    print(results.summary())\n",
        "\n",
        "    # Forecast the next 12\n",
        "    forecast_steps = 12\n",
        "    forecast = results.forecast(data.values[-results.k_ar:], steps=forecast_steps)\n",
        "    forecast_index = pd.date_range(start=data.index[-1] + pd.DateOffset(months=1), periods=forecast_steps, freq='M')\n",
        "    forecast_df = pd.DataFrame(forecast, index=forecast_index, columns=data.columns)\n",
        "\n",
        "    # Extract predicted unemployment rates\n",
        "    predicted_unemployment = forecast_df['Unemployment_Rate']\n",
        "\n",
        "    # Plot the forecast\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(data['Unemployment_Rate'], label='Observed Unemployment Rate')\n",
        "    plt.plot(forecast_index, predicted_unemployment, label='Forecasted Unemployment Rate', linestyle='--')\n",
        "    plt.title('Forecasted Unemployment Rate')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Unemployment Rate')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Granger Causality Test\n",
        "    max_lag = 2\n",
        "    test_result = grangercausalitytests(data[['Fast_Food_Stock_Price', 'Unemployment_Rate']], maxlag=max_lag, verbose=True)\n",
        "\n",
        "    # Extract p-values from Granger Causality Test results\n",
        "    granger_p_values = {key: val[0]['ssr_ftest'][1] for key, val in test_result.items()}\n",
        "    print(\"Granger Causality Test p-values:\", granger_p_values)\n",
        "else:\n",
        "    print(\"Not enough observations to fit the model.\")\n"
      ],
      "metadata": {
        "id": "xQ_l9HvSG-xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load datasets\n",
        "def import_dataset(file_id, name, encoding='utf-8'):\n",
        "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "    return pd.read_csv(url, encoding=encoding)\n",
        "\n",
        "# Load the new unemployment data\n",
        "unemployment = import_dataset(\"16HmWmtgK8MM3RQ3Ed_R0-gRGqP67iDT7\", \"unemployment\")\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(unemployment.head())\n",
        "\n",
        "# Convert 'DATE' column to datetime format\n",
        "unemployment['DATE'] = pd.to_datetime(unemployment['DATE'])\n",
        "\n",
        "# Set 'DATE' as index\n",
        "unemployment.set_index('DATE', inplace=True)\n",
        "\n",
        "# Filter the data between 2007 and 2020\n",
        "unemployment_filtered = unemployment.loc['2007-01-01':'2020-12-31']\n",
        "\n",
        "# Plot the unemployment data\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(unemployment_filtered.index, unemployment_filtered['LNS14000036'], label='Unemployment Rate')\n",
        "plt.title('Unemployment Rate (2007-2020)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Unemployment Rate')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MB2jP488G8eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WJLUT17aEjZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def import_dataset(file_id, name, encoding='utf-8'):\n",
        "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "    return pd.read_csv(url, encoding=encoding)\n",
        "\n",
        "# unemployment data\n",
        "unemployment = import_dataset(\"16HmWmtgK8MM3RQ3Ed_R0-gRGqP67iDT7\", \"unemployment\")\n",
        "\n",
        "print(unemployment.head())\n",
        "\n",
        "# convert 'DATE' column to datetime format\n",
        "unemployment['DATE'] = pd.to_datetime(unemployment['DATE'])\n",
        "\n",
        "unemployment.set_index('DATE', inplace=True)\n",
        "\n",
        "# plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(unemployment.index, unemployment['LNS14000036'], label='Unemployment Rate')\n",
        "plt.title('Unemployment Rate Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Unemployment Rate')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pxSuN3MIF_Ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# var analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.api import VAR\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "\n",
        "# Load datasets\n",
        "def import_dataset(file_id, name, encoding='utf-8'):\n",
        "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "    return pd.read_csv(url, encoding=encoding)\n",
        "\n",
        "meat_stats_meat_production = import_dataset(\"1xZqVE4caTO4H60FFIP7Z5Lx1xCz0UuSd\", \"meat_stats_meat_production\")\n",
        "meat_stats_cold_storage = import_dataset(\"1YKBAaJKN_-RRp789RJ4wNYnpO40GyQqD\", \"meat_stats_cold_storage\")\n",
        "all_stocks = import_dataset(\"1hY7xiB-84DbqWhsZAazfwGbgtVxnBzDJ\", \"all_stocks\")\n",
        "stock_descriptions = import_dataset(\"1AzU_F3Usfx_iYRLH1KAMMaNZ3-9y0AwK\", \"stock_descriptions\")\n",
        "\n",
        "# Convert 'Date' columns to datetime format\n",
        "meat_stats_meat_production['Date'] = pd.to_datetime(meat_stats_meat_production['Date'], format='%b-%Y')\n",
        "meat_stats_cold_storage['Date'] = pd.to_datetime(meat_stats_cold_storage['Date'])\n",
        "all_stocks['Date'] = pd.to_datetime(all_stocks['Date-Time'])\n",
        "\n",
        "# Set 'Date' as index\n",
        "meat_stats_meat_production.set_index('Date', inplace=True)\n",
        "meat_stats_cold_storage.set_index('Date', inplace=True)\n",
        "all_stocks.set_index('Date', inplace=True)\n",
        "\n",
        "# Convert 'Production' to numeric\n",
        "meat_stats_meat_production['Production'] = meat_stats_meat_production['Production'].str.replace(',', '').astype(float)\n",
        "\n",
        "# Handle missing values\n",
        "meat_stats_meat_production = meat_stats_meat_production.dropna()\n",
        "meat_stats_cold_storage = meat_stats_cold_storage.dropna()\n",
        "\n",
        "# Filter data for 'Red Meat' and 'Poultry'\n",
        "red_meat_production = meat_stats_meat_production.loc[meat_stats_meat_production['Type of Meat'] == 'Red Meat', 'Production']\n",
        "poultry_production = meat_stats_meat_production.loc[meat_stats_meat_production['Type of Meat'] == 'Poultry', 'Production']\n",
        "\n",
        "# Filter fast food stocks\n",
        "fast_food_stocks = stock_descriptions[stock_descriptions['Industry'].isin(['RETAIL-EATING PLACES', 'RETAIL-EATING & DRINKING PLACES'])]\n",
        "fast_food_stock_prices = all_stocks[all_stocks['Ticker_Symbol'].isin(fast_food_stocks['Symbol'])]\n",
        "\n",
        "# Resample to monthly data\n",
        "monthly_fast_food_stock_prices = fast_food_stock_prices['Close'].resample('M').mean()\n",
        "red_meat_production_monthly = red_meat_production.resample('M').mean()\n",
        "poultry_production_monthly = poultry_production.resample('M').mean()\n",
        "\n",
        "# Aggregate cold storage data by month and type of meat\n",
        "cold_storage_agg = meat_stats_cold_storage.groupby([pd.Grouper(freq='M'), 'Type_Of_Meat'])['Weight'].mean().unstack(fill_value=0)\n",
        "cold_storage_agg.columns = ['ColdStorageRedMeat', 'ColdStoragePoultry']\n",
        "\n",
        "# Align both time series to the same time range\n",
        "start_date = max(monthly_fast_food_stock_prices.index.min(), red_meat_production_monthly.index.min(), poultry_production_monthly.index.min(), cold_storage_agg.index.min())\n",
        "end_date = min(monthly_fast_food_stock_prices.index.max(), red_meat_production_monthly.index.max(), poultry_production_monthly.index.max(), cold_storage_agg.index.max())\n",
        "\n",
        "monthly_fast_food_stock_prices = monthly_fast_food_stock_prices.loc[start_date:end_date]\n",
        "red_meat_production_monthly = red_meat_production_monthly.loc[start_date:end_date]\n",
        "poultry_production_monthly = poultry_production_monthly.loc[start_date:end_date]\n",
        "cold_storage_agg = cold_storage_agg.loc[start_date:end_date]\n",
        "\n",
        "# Combine into a single DataFrame\n",
        "data = pd.concat([monthly_fast_food_stock_prices, red_meat_production_monthly, poultry_production_monthly, cold_storage_agg], axis=1)\n",
        "data.columns = ['Fast_Food_Stock_Price', 'RedMeatProduction', 'PoultryProduction', 'ColdStorageRedMeat', 'ColdStoragePoultry']\n",
        "\n",
        "# Drop any remaining missing values\n",
        "data = data.dropna()\n",
        "\n",
        "# Add the new unemployment data\n",
        "unemployment_rate = unemployment['LNS14000036'].resample('M').mean()\n",
        "data['Unemployment_Rate'] = unemployment_rate\n",
        "\n",
        "# Ensure no NaN or infinite values\n",
        "data = data.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "# Check if there are any NaNs or infinite values left\n",
        "print(\"Missing values in combined data after resampling and dropping:\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "print(\"Head of combined data:\")\n",
        "print(data.head())\n",
        "\n",
        "# Determine the number of observations and adjust maxlags accordingly\n",
        "num_observations = len(data)\n",
        "maxlags = min(15, num_observations // 3)  # Use a smaller maxlag\n",
        "\n",
        "print(f\"Number of observations: {num_observations}\")\n",
        "print(f\"Maxlags set to: {maxlags}\")\n",
        "\n",
        "if num_observations > 0 and maxlags > 0:\n",
        "    # Fit VAR model with the adjusted maxlags value\n",
        "    model = VAR(data)\n",
        "    results = model.fit(maxlags=maxlags, ic='aic')\n",
        "\n",
        "    # Print summary of the model\n",
        "    print(results.summary())\n",
        "\n",
        "    # Forecast the next 12\n",
        "    forecast_steps = 12\n",
        "    forecast = results.forecast(data.values[-results.k_ar:], steps=forecast_steps)\n",
        "    forecast_index = pd.date_range(start=data.index[-1] + pd.DateOffset(months=1), periods=forecast_steps, freq='M')\n",
        "    forecast_df = pd.DataFrame(forecast, index=forecast_index, columns=data.columns)\n",
        "\n",
        "    # Extract predicted unemployment rates\n",
        "    predicted_unemployment = forecast_df['Unemployment_Rate']\n",
        "\n",
        "    # Plot the forecast\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(data['Unemployment_Rate'], label='Observed Unemployment Rate')\n",
        "    plt.plot(forecast_index, predicted_unemployment, label='Forecasted Unemployment Rate', linestyle='--')\n",
        "    plt.title('Forecasted Unemployment Rate')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Unemployment Rate')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Granger Causality Test with specific lags based on the table\n",
        "    causality_tests = {\n",
        "        'Fast_Food_Stock_Price -> Unemployment_Rate': 3,\n",
        "        'Fast_Food_Stock_Price -> PoultryProduction': 1,\n",
        "        'ColdStoragePoultry -> RedMeatProduction': 5,\n",
        "        'ColdStoragePoultry -> Unemployment_Rate': 1,\n",
        "        'ColdStoragePoultry -> Fast_Food_Stock_Price': 1,\n",
        "        'RedMeatProduction -> Fast_Food_Stock_Price': 2,\n",
        "        'RedMeatProduction -> ColdStorageRedMeat': 5\n",
        "    }\n",
        "\n",
        "    for test, lags in causality_tests.items():\n",
        "        x, y = test.split(' -> ')\n",
        "        print(f'\\nGranger Causality Test: {x} causes {y} with {lags} lags')\n",
        "        test_result = grangercausalitytests(data[[x, y]], maxlag=lags, verbose=False)\n",
        "        p_values = {key: val[0]['ssr_ftest'][1] for key, val in test_result.items()}\n",
        "        print(f'p-values for lags: {p_values}')\n",
        "else:\n",
        "    print(\"Not enough observations to fit the model.\")\n"
      ],
      "metadata": {
        "id": "WKiHYyBxC7L3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.api import VAR\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "\n",
        "# Load datasets\n",
        "def import_dataset(file_id, name, encoding='utf-8'):\n",
        "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "    return pd.read_csv(url, encoding=encoding)\n",
        "\n",
        "meat_stats_meat_production = import_dataset(\"1xZqVE4caTO4H60FFIP7Z5Lx1xCz0UuSd\", \"meat_stats_meat_production\")\n",
        "meat_stats_cold_storage = import_dataset(\"1YKBAaJKN_-RRp789RJ4wNYnpO40GyQqD\", \"meat_stats_cold_storage\")\n",
        "all_stocks = import_dataset(\"1hY7xiB-84DbqWhsZAazfwGbgtVxnBzDJ\", \"all_stocks\")\n",
        "stock_descriptions = import_dataset(\"1AzU_F3Usfx_iYRLH1KAMMaNZ3-9y0AwK\", \"stock_descriptions\")\n",
        "unemployment_new = import_dataset(\"16HmWmtgK8MM3RQ3Ed_R0-gRGqP67iDT7\", \"unemployment\")\n",
        "\n",
        "# Convert 'Date' columns to datetime format\n",
        "meat_stats_meat_production['Date'] = pd.to_datetime(meat_stats_meat_production['Date'], format='%b-%Y')\n",
        "meat_stats_cold_storage['Date'] = pd.to_datetime(meat_stats_cold_storage['Date'], format='%b-%Y')\n",
        "all_stocks['Date'] = pd.to_datetime(all_stocks['Date-Time'])\n",
        "unemployment_new['DATE'] = pd.to_datetime(unemployment_new['DATE'])\n",
        "\n",
        "# Set 'Date' as index\n",
        "meat_stats_meat_production.set_index('Date', inplace=True)\n",
        "meat_stats_cold_storage.set_index('Date', inplace=True)\n",
        "all_stocks.set_index('Date', inplace=True)\n",
        "unemployment_new.set_index('DATE', inplace=True)\n",
        "\n",
        "# Convert 'Production' to numeric\n",
        "meat_stats_meat_production['Production'] = meat_stats_meat_production['Production'].str.replace(',', '').astype(float)\n",
        "\n",
        "# Handle missing values\n",
        "meat_stats_meat_production = meat_stats_meat_production.dropna()\n",
        "meat_stats_cold_storage = meat_stats_cold_storage.dropna()\n",
        "\n",
        "# Filter data for 'Red Meat' and 'Poultry'\n",
        "red_meat_production = meat_stats_meat_production.loc[meat_stats_meat_production['Type of Meat'] == 'Red Meat', 'Production']\n",
        "poultry_production = meat_stats_meat_production.loc[meat_stats_meat_production['Type of Meat'] == 'Poultry', 'Production']\n",
        "\n",
        "# Filter fast food stocks\n",
        "fast_food_stocks = stock_descriptions[stock_descriptions['Industry'].isin(['RETAIL-EATING PLACES', 'RETAIL-EATING & DRINKING PLACES'])]\n",
        "fast_food_stock_prices = all_stocks[all_stocks['Ticker_Symbol'].isin(fast_food_stocks['Symbol'])]\n",
        "\n",
        "# Resample to monthly data\n",
        "monthly_fast_food_stock_prices = fast_food_stock_prices['Close'].resample('M').mean()\n",
        "red_meat_production_monthly = red_meat_production.resample('M').mean()\n",
        "poultry_production_monthly = poultry_production.resample('M').mean()\n",
        "\n",
        "# Aggregate cold storage data by month and type of meat\n",
        "cold_storage_agg = meat_stats_cold_storage.groupby([pd.Grouper(freq='M'), 'Type_Of_Meat'])['Weight'].mean().unstack(fill_value=0)\n",
        "cold_storage_agg.columns = ['ColdStorageRedMeat', 'ColdStoragePoultry']\n",
        "\n",
        "# Align both time series to the same time range\n",
        "start_date = max(monthly_fast_food_stock_prices.index.min(), red_meat_production_monthly.index.min(), poultry_production_monthly.index.min(), cold_storage_agg.index.min())\n",
        "end_date = min(monthly_fast_food_stock_prices.index.max(), red_meat_production_monthly.index.max(), poultry_production_monthly.index.max(), cold_storage_agg.index.max())\n",
        "\n",
        "monthly_fast_food_stock_prices = monthly_fast_food_stock_prices.loc[start_date:end_date]\n",
        "red_meat_production_monthly = red_meat_production_monthly.loc[start_date:end_date]\n",
        "poultry_production_monthly = poultry_production_monthly.loc[start_date:end_date]\n",
        "cold_storage_agg = cold_storage_agg.loc[start_date:end_date]\n",
        "\n",
        "# Combine into a single DataFrame\n",
        "data = pd.concat([monthly_fast_food_stock_prices, red_meat_production_monthly, poultry_production_monthly, cold_storage_agg], axis=1)\n",
        "data.columns = ['Fast_Food_Stock_Price', 'RedMeatProduction', 'PoultryProduction', 'ColdStorageRedMeat', 'ColdStoragePoultry']\n",
        "\n",
        "# Drop any remaining missing values\n",
        "data = data.dropna()\n",
        "\n",
        "# Add the new unemployment data\n",
        "unemployment_rate = unemployment_new['LNS14000036'].resample('M').mean()\n",
        "data['Unemployment_Rate'] = unemployment_rate\n",
        "\n",
        "# Filter out the COVID-19 spike for the VAR analysis\n",
        "covid_start_date = '2020-04-01'\n",
        "covid_end_date = '2022-06-01'\n",
        "data_for_var = pd.concat([data.loc[:covid_start_date], data.loc[covid_end_date:]])\n",
        "\n",
        "# Ensure no NaN or infinite values\n",
        "data_for_var = data_for_var.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "# Check if there are any NaNs or infinite values left\n",
        "print(\"Missing values in combined data after resampling and dropping:\")\n",
        "print(data_for_var.isnull().sum())\n",
        "\n",
        "print(\"Head of combined data:\")\n",
        "print(data_for_var.head())\n",
        "\n",
        "# Determine the number of observations and adjust maxlags accordingly\n",
        "num_observations = len(data_for_var)\n",
        "maxlags = min(15, num_observations // 3)  # Use a smaller maxlag\n",
        "\n",
        "print(f\"Number of observations: {num_observations}\")\n",
        "print(f\"Maxlags set to: {maxlags}\")\n",
        "\n",
        "if num_observations > 0 and maxlags > 0:\n",
        "    # Fit VAR model with the adjusted maxlags value\n",
        "    model = VAR(data_for_var)\n",
        "    results = model.fit(maxlags=maxlags, ic='aic')\n",
        "\n",
        "    # Print summary of the model\n",
        "    print(results.summary())\n",
        "\n",
        "    # Forecast the next 12\n",
        "    forecast_steps = 12\n",
        "    forecast = results.forecast(data_for_var.values[-results.k_ar:], steps=forecast_steps)\n",
        "    forecast_index = pd.date_range(start=data_for_var.index[-1] + pd.DateOffset(months=1), periods=forecast_steps, freq='M')\n",
        "    forecast_df = pd.DataFrame(forecast, index=forecast_index, columns=data.columns)\n",
        "\n",
        "    # Extract predicted unemployment rates\n",
        "    predicted_unemployment = forecast_df['Unemployment_Rate']\n",
        "\n",
        "    # Plot the forecast\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(data['Unemployment_Rate'], label='Observed Unemployment Rate')\n",
        "    plt.plot(forecast_index, predicted_unemployment, label='Forecasted Unemployment Rate', linestyle='--')\n",
        "    plt.title('Forecasted Unemployment Rate')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Unemployment Rate')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Granger Causality Test with specific lags based on the table\n",
        "    causality_tests = {\n",
        "        'Fast_Food_Stock_Price -> Unemployment_Rate': 3,\n",
        "        'Fast_Food_Stock_Price -> PoultryProduction': 1,\n",
        "        'ColdStoragePoultry -> RedMeatProduction': 5,\n",
        "        'ColdStoragePoultry -> Unemployment_Rate': 1,\n",
        "        'ColdStoragePoultry -> Fast_Food_Stock_Price': 1,\n",
        "        'RedMeatProduction -> Fast_Food_Stock_Price': 2,\n",
        "        'RedMeatProduction -> ColdStorageRedMeat': 5\n",
        "    }\n",
        "\n",
        "    for test, lags in causality_tests.items():\n",
        "        x, y = test.split(' -> ')\n",
        "        print(f'\\nGranger Causality Test: {x} causes {y} with {lags} lags')\n",
        "        test_result = grangercausalitytests(data_for_var[[x, y]], maxlag=lags, verbose=True)\n",
        "        p_values = {key: val[0]['ssr_ftest'][1] for key, val in test_result.items()}\n",
        "        print(f'p-values for lags: {p_values}')\n",
        "else:\n",
        "    print(\"Not enough observations to fit the model.\")\n"
      ],
      "metadata": {
        "id": "R9cIjCP3Kakd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.api import VAR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load datasets\n",
        "def import_dataset(file_id, name, encoding='utf-8'):\n",
        "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "    return pd.read_csv(url, encoding=encoding, low_memory=False)\n",
        "\n",
        "# Load the nutrition data\n",
        "nutrition = import_dataset(\"1nxW91Jp65jQOdR_2_FJl1XaXDzhiHzgC\", \"nutrition\")\n",
        "\n",
        "# Filter for obesity data\n",
        "obesity_data = nutrition[(nutrition['Topic'] == 'Obesity / Weight Status') &\n",
        "                         (nutrition['Question'].str.contains('obesity', case=False))]\n",
        "\n",
        "# Extract relevant columns and rename them\n",
        "obesity_data = obesity_data[['YearStart', 'Data_Value']]\n",
        "obesity_data.columns = ['Year', 'Obesity_Rate']\n",
        "\n",
        "# Average the yearly data\n",
        "obesity_data = obesity_data.groupby('Year').mean().reset_index()\n",
        "\n",
        "# Generate monthly dates\n",
        "monthly_dates = pd.date_range(start=f\"{obesity_data['Year'].min()}-01-01\", end=f\"{obesity_data['Year'].max()}-12-31\", freq='M')\n",
        "\n",
        "# Interpolate to generate monthly obesity rates from yearly data\n",
        "obesity_data.set_index('Year', inplace=True)\n",
        "obesity_data = obesity_data.reindex(range(obesity_data.index.min(), obesity_data.index.max() + 1)).interpolate()\n",
        "\n",
        "# Repeat each yearly rate for 12 months to create a monthly series\n",
        "monthly_obesity_rate = np.repeat(obesity_data['Obesity_Rate'].values, 12)\n",
        "\n",
        "# Truncate or extend monthly_dates to match the length of monthly_obesity_rate\n",
        "monthly_dates = monthly_dates[:len(monthly_obesity_rate)]\n",
        "\n",
        "# Create a DataFrame for the monthly obesity data\n",
        "monthly_obesity_data = pd.DataFrame({\n",
        "    'DATE': monthly_dates,\n",
        "    'Obesity_Rate': monthly_obesity_rate\n",
        "})\n",
        "\n",
        "# Set 'DATE' as index\n",
        "monthly_obesity_data.set_index('DATE', inplace=True)\n",
        "\n",
        "# Plot the generated monthly obesity data\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(monthly_obesity_data.index, monthly_obesity_data['Obesity_Rate'], label='Generated Monthly Obesity Rate')\n",
        "plt.title('Generated Monthly Obesity Rate')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Obesity Rate')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Print the first few rows of the generated monthly obesity data\n",
        "print(\"Generated Monthly Obesity Data:\")\n",
        "print(monthly_obesity_data.head())\n",
        "\n",
        "# Load the unemployment data\n",
        "unemployment_new = import_dataset(\"16HmWmtgK8MM3RQ3Ed_R0-gRGqP67iDT7\", \"unemployment\")\n",
        "\n",
        "# Convert 'DATE' column to datetime format\n",
        "unemployment_new['DATE'] = pd.to_datetime(unemployment_new['DATE'])\n",
        "\n",
        "# Set 'DATE' as index\n",
        "unemployment_new.set_index('DATE', inplace=True)\n",
        "\n",
        "# Filter the unemployment data to match the date range of the obesity data\n",
        "unemployment_filtered = unemployment_new.loc[monthly_obesity_data.index.min():monthly_obesity_data.index.max()]\n",
        "\n",
        "# Resample unemployment data to monthly frequency and fill missing values\n",
        "monthly_unemployment_rate = unemployment_filtered['LNS14000036'].resample('M').mean().interpolate()\n",
        "\n",
        "# Print the first few rows of the monthly unemployment data\n",
        "print(\"Monthly Unemployment Data:\")\n",
        "print(monthly_unemployment_rate.head())\n",
        "\n",
        "# Combine obesity and unemployment data into a single DataFrame\n",
        "combined_data = pd.concat([monthly_obesity_data, monthly_unemployment_rate], axis=1).dropna()\n",
        "\n",
        "# Print the combined data before dropping NA\n",
        "print(\"Combined Data Before Dropping NA:\")\n",
        "print(pd.concat([monthly_obesity_data, monthly_unemployment_rate], axis=1).head(30))\n",
        "\n",
        "# Ensure no NaN or infinite values\n",
        "combined_data = combined_data.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing values in combined data after resampling and dropping:\")\n",
        "print(combined_data.isnull().sum())\n",
        "\n",
        "# Print the head of the combined data\n",
        "print(\"Head of combined data:\")\n",
        "print(combined_data.head())\n",
        "\n",
        "# Determine the number of observations and adjust maxlags accordingly\n",
        "num_observations = len(combined_data)\n",
        "maxlags = min(15, num_observations // 3)  # Use a smaller maxlag\n",
        "\n",
        "print(f\"Number of observations: {num_observations}\")\n",
        "print(f\"Maxlags set to: {maxlags}\")\n",
        "\n",
        "if num_observations > 0 and maxlags > 0:\n",
        "    # Fit VAR model with the adjusted maxlags value\n",
        "    model = VAR(combined_data)\n",
        "    results = model.fit(maxlags=maxlags, ic='aic')\n",
        "\n",
        "    # Print summary of the model\n",
        "    print(results.summary())\n",
        "\n",
        "    # Split the data into train and test sets\n",
        "    n_train = int(0.8 * len(combined_data))\n",
        "    train_data = combined_data.iloc[:n_train]\n",
        "    test_data = combined_data.iloc[n_train:]\n",
        "\n",
        "    # Fit VAR model on the training data\n",
        "    model = VAR(train_data)\n",
        "    results = model.fit(maxlags=maxlags, ic='aic')\n",
        "\n",
        "    # Forecast the test period\n",
        "    forecast_steps = len(test_data)\n",
        "    forecast = results.forecast(train_data.values[-results.k_ar:], steps=forecast_steps)\n",
        "    forecast_index = pd.date_range(start=train_data.index[-1] + pd.DateOffset(months=1), periods=forecast_steps, freq='M')\n",
        "    forecast_df = pd.DataFrame(forecast, index=forecast_index, columns=combined_data.columns)\n",
        "\n",
        "    # Extract predicted obesity rates\n",
        "    predicted_obesity = forecast_df['Obesity_Rate']\n",
        "\n",
        "    # Plot the observed and forecasted obesity rates\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(combined_data['Obesity_Rate'], label='Observed Obesity Rate')\n",
        "    plt.plot(forecast_index, predicted_obesity, label='Forecasted Obesity Rate', linestyle='--')\n",
        "    plt.title('Observed and Forecasted Obesity Rate')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Obesity Rate')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate and print the Mean Squared Error\n",
        "    mse = mean_squared_error(test_data['Obesity_Rate'], predicted_obesity)\n",
        "    print(f\"Mean Squared Error: {mse}\")\n",
        "\n",
        "else:\n",
        "    print(\"Not enough observations to fit the model.\")\n"
      ],
      "metadata": {
        "id": "-RYsbevgMnnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.api import VAR\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load datasets\n",
        "def import_dataset(file_id, name, encoding='utf-8'):\n",
        "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "    return pd.read_csv(url, encoding=encoding, low_memory=False)\n",
        "\n",
        "# Load the nutrition data\n",
        "nutrition = import_dataset(\"1nxW91Jp65jQOdR_2_FJl1XaXDzhiHzgC\", \"nutrition\")\n",
        "\n",
        "# Filter for obesity data\n",
        "obesity_data = nutrition[(nutrition['Topic'] == 'Obesity / Weight Status') &\n",
        "                         (nutrition['Question'].str.contains('obesity', case=False))]\n",
        "\n",
        "# Extract relevant columns and rename them\n",
        "obesity_data = obesity_data[['YearStart', 'Data_Value']]\n",
        "obesity_data.columns = ['Year', 'Obesity_Rate']\n",
        "\n",
        "# Average the yearly data\n",
        "obesity_data = obesity_data.groupby('Year').mean().reset_index()\n",
        "\n",
        "# Generate monthly dates\n",
        "monthly_dates = pd.date_range(start=f\"{obesity_data['Year'].min()}-01-01\", end=f\"{obesity_data['Year'].max()}-12-31\", freq='M')\n",
        "\n",
        "# Interpolate to generate monthly obesity rates from yearly data\n",
        "obesity_data.set_index('Year', inplace=True)\n",
        "obesity_data = obesity_data.reindex(range(obesity_data.index.min(), obesity_data.index.max() + 1)).interpolate()\n",
        "\n",
        "# Repeat each yearly rate for 12 months to create a monthly series\n",
        "monthly_obesity_rate = np.repeat(obesity_data['Obesity_Rate'].values, 12)\n",
        "\n",
        "# Truncate or extend monthly_dates to match the length of monthly_obesity_rate\n",
        "monthly_dates = monthly_dates[:len(monthly_obesity_rate)]\n",
        "\n",
        "# Create a DataFrame for the monthly obesity data\n",
        "monthly_obesity_data = pd.DataFrame({\n",
        "    'DATE': monthly_dates,\n",
        "    'Obesity_Rate': monthly_obesity_rate\n",
        "})\n",
        "\n",
        "# Set 'DATE' as index\n",
        "monthly_obesity_data.set_index('DATE', inplace=True)\n",
        "\n",
        "# Plot the generated monthly obesity data\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(monthly_obesity_data.index, monthly_obesity_data['Obesity_Rate'], label='Generated Monthly Obesity Rate')\n",
        "plt.title('Generated Monthly Obesity Rate')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Obesity Rate')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Print the first few rows of the generated monthly obesity data\n",
        "print(\"Generated Monthly Obesity Data:\")\n",
        "print(monthly_obesity_data.head())\n",
        "\n",
        "# Load the unemployment data\n",
        "unemployment_new = import_dataset(\"16HmWmtgK8MM3RQ3Ed_R0-gRGqP67iDT7\", \"unemployment\")\n",
        "\n",
        "# Convert 'DATE' column to datetime format\n",
        "unemployment_new['DATE'] = pd.to_datetime(unemployment_new['DATE'])\n",
        "\n",
        "# Set 'DATE' as index\n",
        "unemployment_new.set_index('DATE', inplace=True)\n",
        "\n",
        "# Filter the unemployment data to match the date range of the obesity data\n",
        "unemployment_filtered = unemployment_new.loc[monthly_obesity_data.index.min():monthly_obesity_data.index.max()]\n",
        "\n",
        "# Resample unemployment data to monthly frequency and fill missing values\n",
        "monthly_unemployment_rate = unemployment_filtered['LNS14000036'].resample('M').mean().interpolate()\n",
        "\n",
        "# Print the first few rows of the monthly unemployment data\n",
        "print(\"Monthly Unemployment Data:\")\n",
        "print(monthly_unemployment_rate.head())\n",
        "\n",
        "# Combine obesity and unemployment data into a single DataFrame\n",
        "combined_data = pd.concat([monthly_obesity_data, monthly_unemployment_rate], axis=1).dropna()\n",
        "\n",
        "# Print the combined data before dropping NA\n",
        "print(\"Combined Data Before Dropping NA:\")\n",
        "print(pd.concat([monthly_obesity_data, monthly_unemployment_rate], axis=1).head(30))\n",
        "\n",
        "# Ensure no NaN or infinite values\n",
        "combined_data = combined_data.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing values in combined data after resampling and dropping:\")\n",
        "print(combined_data.isnull().sum())\n",
        "\n",
        "# Print the head of the combined data\n",
        "print(\"Head of combined data:\")\n",
        "print(combined_data.head())\n",
        "\n",
        "# Determine the number of observations and adjust maxlags accordingly\n",
        "num_observations = len(combined_data)\n",
        "maxlags = min(15, num_observations // 3)  # Use a smaller maxlag\n",
        "\n",
        "print(f\"Number of observations: {num_observations}\")\n",
        "print(f\"Maxlags set to: {maxlags}\")\n",
        "\n",
        "if num_observations > 0 and maxlags > 0:\n",
        "    # Fit VAR model with the adjusted maxlags value\n",
        "    model = VAR(combined_data)\n",
        "    results = model.fit(maxlags=maxlags, ic='aic')\n",
        "\n",
        "    # Print summary of the model\n",
        "    print(results.summary())\n",
        "\n",
        "    # Forecast the entire historical period\n",
        "    forecast_steps = len(combined_data)\n",
        "    forecast = results.forecast(combined_data.values[:results.k_ar], steps=forecast_steps)\n",
        "    forecast_index = pd.date_range(start=combined_data.index[results.k_ar], periods=forecast_steps, freq='M')\n",
        "    forecast_df = pd.DataFrame(forecast, index=forecast_index, columns=combined_data.columns)\n",
        "\n",
        "    # Extract predicted obesity rates\n",
        "    predicted_obesity = forecast_df['Obesity_Rate']\n",
        "\n",
        "    # Plot the observed and forecasted obesity rates\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(combined_data['Obesity_Rate'], label='Observed Obesity Rate')\n",
        "    plt.plot(forecast_index, predicted_obesity, label='Forecasted Obesity Rate', linestyle='--')\n",
        "    plt.title('Observed and Forecasted Obesity Rate')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Obesity Rate')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate and print the Mean Squared Error\n",
        "    mse = mean_squared_error(combined_data['Obesity_Rate'][results.k_ar:], predicted_obesity)\n",
        "    print(f\"Mean Squared Error: {mse}\")\n",
        "\n",
        "else:\n",
        "    print(\"Not enough observations to fit the model.\")\n"
      ],
      "metadata": {
        "id": "uQn1tyRzUYLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Load datasets\n",
        "def import_dataset(file_id, name, encoding='utf-8'):\n",
        "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "    return pd.read_csv(url, encoding=encoding, low_memory=False)\n",
        "\n",
        "# Load the nutrition data\n",
        "nutrition = import_dataset(\"1nxW91Jp65jQOdR_2_FJl1XaXDzhiHzgC\", \"nutrition\")\n",
        "\n",
        "# Filter for obesity data\n",
        "obesity_data = nutrition[(nutrition['Topic'] == 'Obesity / Weight Status') &\n",
        "                         (nutrition['Question'].str.contains('obesity', case=False))]\n",
        "\n",
        "# Extract relevant columns and rename them\n",
        "obesity_data = obesity_data[['YearStart', 'Data_Value']]\n",
        "obesity_data.columns = ['Year', 'Obesity_Rate']\n",
        "\n",
        "# Average the yearly data\n",
        "obesity_data = obesity_data.groupby('Year').mean().reset_index()\n",
        "\n",
        "# Generate monthly dates\n",
        "monthly_dates = pd.date_range(start=f\"{obesity_data['Year'].min()}-01-01\", end=f\"{obesity_data['Year'].max()}-12-31\", freq='M')\n",
        "\n",
        "# Interpolate to generate monthly obesity rates from yearly data\n",
        "obesity_data.set_index('Year', inplace=True)\n",
        "obesity_data = obesity_data.reindex(range(obesity_data.index.min(), obesity_data.index.max() + 1)).interpolate()\n",
        "\n",
        "# Repeat each yearly rate for 12 months to create a monthly series\n",
        "monthly_obesity_rate = np.repeat(obesity_data['Obesity_Rate'].values, 12)\n",
        "\n",
        "# Truncate or extend monthly_dates to match the length of monthly_obesity_rate\n",
        "monthly_dates = monthly_dates[:len(monthly_obesity_rate)]\n",
        "\n",
        "# Create a DataFrame for the monthly obesity data\n",
        "monthly_obesity_data = pd.DataFrame({\n",
        "    'DATE': monthly_dates,\n",
        "    'Obesity_Rate': monthly_obesity_rate\n",
        "})\n",
        "\n",
        "# Set 'DATE' as index\n",
        "monthly_obesity_data.set_index('DATE', inplace=True)\n",
        "\n",
        "# Load the unemployment data\n",
        "unemployment_new = import_dataset(\"16HmWmtgK8MM3RQ3Ed_R0-gRGqP67iDT7\", \"unemployment\")\n",
        "\n",
        "# Convert 'DATE' column to datetime format\n",
        "unemployment_new['DATE'] = pd.to_datetime(unemployment_new['DATE'])\n",
        "\n",
        "# Set 'DATE' as index\n",
        "unemployment_new.set_index('DATE', inplace=True)\n",
        "\n",
        "# Filter the unemployment data to match the date range of the obesity data\n",
        "unemployment_filtered = unemployment_new.loc[monthly_obesity_data.index.min():monthly_obesity_data.index.max()]\n",
        "\n",
        "# Resample unemployment data to monthly frequency and fill missing values\n",
        "monthly_unemployment_rate = unemployment_filtered['LNS14000036'].resample('M').mean().interpolate()\n",
        "\n",
        "# Combine obesity and unemployment data into a single DataFrame\n",
        "combined_data = pd.concat([monthly_obesity_data, monthly_unemployment_rate], axis=1).dropna()\n",
        "\n",
        "# Ensure no NaN or infinite values\n",
        "combined_data = combined_data.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(combined_data)\n",
        "\n",
        "# Prepare the data for LSTM\n",
        "def create_sequences(data, seq_length):\n",
        "    xs, ys = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        x = data[i:i+seq_length, :-1]\n",
        "        y = data[i+seq_length, -1]\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "seq_length = 12\n",
        "X, y = create_sequences(scaled_data, seq_length)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "split = int(0.8 * len(X))\n",
        "X_train, X_test = X[:split], X[split:]\n",
        "y_train, y_test = y[:split], y[split:]\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', input_shape=(seq_length, X.shape[2])))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Inverse transform the predictions\n",
        "y_test_inv = scaler.inverse_transform(np.concatenate((np.zeros((len(y_test), scaled_data.shape[1]-1)), y_test.reshape(-1, 1)), axis=1))[:, -1]\n",
        "y_pred_inv = scaler.inverse_transform(np.concatenate((np.zeros((len(y_pred), scaled_data.shape[1]-1)), y_pred), axis=1))[:, -1]\n",
        "\n",
        "# Plot the observed and predicted obesity rates\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(monthly_obesity_data.index[-len(y_test):], y_test_inv, label='Observed Obesity Rate')\n",
        "plt.plot(monthly_obesity_data.index[-len(y_test):], y_pred_inv, label='Predicted Obesity Rate', linestyle='--')\n",
        "plt.title('Observed and Predicted Obesity Rate')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Obesity Rate')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Calculate and print the Mean Squared Error\n",
        "mse = mean_squared_error(y_test_inv, y_pred_inv)\n",
        "print(f\"Mean Squared Error: {mse}\")\n"
      ],
      "metadata": {
        "id": "8tU6o2VHetui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Load datasets\n",
        "def import_dataset(file_id, name, encoding='utf-8'):\n",
        "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "    return pd.read_csv(url, encoding=encoding, low_memory=False)\n",
        "\n",
        "# Load the nutrition data\n",
        "nutrition = import_dataset(\"1nxW91Jp65jQOdR_2_FJl1XaXDzhiHzgC\", \"nutrition\")\n",
        "\n",
        "# Filter for obesity data\n",
        "obesity_data = nutrition[(nutrition['Topic'] == 'Obesity / Weight Status') &\n",
        "                         (nutrition['Question'].str.contains('obesity', case=False))]\n",
        "\n",
        "# Extract relevant columns and rename them\n",
        "obesity_data = obesity_data[['YearStart', 'Data_Value']]\n",
        "obesity_data.columns = ['Year', 'Obesity_Rate']\n",
        "\n",
        "# Average the yearly data\n",
        "obesity_data = obesity_data.groupby('Year').mean().reset_index()\n",
        "\n",
        "# Apply a rolling mean to smooth the data\n",
        "obesity_data['Obesity_Rate'] = obesity_data['Obesity_Rate'].rolling(window=3, center=True).mean()\n",
        "\n",
        "# Generate monthly dates\n",
        "monthly_dates = pd.date_range(start=f\"{obesity_data['Year'].min()}-01-01\", end=f\"{obesity_data['Year'].max()}-12-31\", freq='M')\n",
        "\n",
        "# Interpolate to generate monthly obesity rates from yearly data\n",
        "obesity_data.set_index('Year', inplace=True)\n",
        "obesity_data = obesity_data.reindex(range(obesity_data.index.min(), obesity_data.index.max() + 1)).interpolate()\n",
        "\n",
        "# Repeat each yearly rate for 12 months to create a monthly series\n",
        "monthly_obesity_rate = np.repeat(obesity_data['Obesity_Rate'].values, 12)\n",
        "\n",
        "# Add some noise to the monthly obesity rate\n",
        "np.random.seed(42)\n",
        "monthly_obesity_rate += np.random.normal(scale=0.5, size=monthly_obesity_rate.shape)\n",
        "\n",
        "# Truncate or extend monthly_dates to match the length of monthly_obesity_rate\n",
        "monthly_dates = monthly_dates[:len(monthly_obesity_rate)]\n",
        "\n",
        "# Create a DataFrame for the monthly obesity data\n",
        "monthly_obesity_data = pd.DataFrame({\n",
        "    'DATE': monthly_dates,\n",
        "    'Obesity_Rate': monthly_obesity_rate\n",
        "})\n",
        "\n",
        "# Set 'DATE' as index\n",
        "monthly_obesity_data.set_index('DATE', inplace=True)\n",
        "\n",
        "# Load the unemployment data\n",
        "unemployment_new = import_dataset(\"16HmWmtgK8MM3RQ3Ed_R0-gRGqP67iDT7\", \"unemployment\")\n",
        "\n",
        "# Convert 'DATE' column to datetime format\n",
        "unemployment_new['DATE'] = pd.to_datetime(unemployment_new['DATE'])\n",
        "\n",
        "# Set 'DATE' as index\n",
        "unemployment_new.set_index('DATE', inplace=True)\n",
        "\n",
        "# Filter the unemployment data to match the date range of the obesity data\n",
        "unemployment_filtered = unemployment_new.loc[monthly_obesity_data.index.min():monthly_obesity_data.index.max()]\n",
        "\n",
        "# Resample unemployment data to monthly frequency and fill missing values\n",
        "monthly_unemployment_rate = unemployment_filtered['LNS14000036'].resample('M').mean().interpolate()\n",
        "\n",
        "# Load additional datasets\n",
        "meat_stats_meat_production = import_dataset(\"1xZqVE4caTO4H60FFIP7Z5Lx1xCz0UuSd\", \"meat_stats_meat_production\")\n",
        "meat_stats_cold_storage = import_dataset(\"1YKBAaJKN_-RRp789RJ4wNYnpO40GyQqD\", \"meat_stats_cold_storage\")\n",
        "all_stocks = import_dataset(\"1hY7xiB-84DbqWhsZAazfwGbgtVxnBzDJ\", \"all_stocks\")\n",
        "stock_descriptions = import_dataset(\"1AzU_F3Usfx_iYRLH1KAMMaNZ3-9y0AwK\", \"stock_descriptions\")\n",
        "\n",
        "# Convert 'Date' columns to datetime format\n",
        "meat_stats_meat_production['Date'] = pd.to_datetime(meat_stats_meat_production['Date'], format='%b-%Y')\n",
        "meat_stats_cold_storage['Date'] = pd.to_datetime(meat_stats_cold_storage['Date'], format='%b-%Y')\n",
        "all_stocks['Date'] = pd.to_datetime(all_stocks['Date-Time'])\n",
        "\n",
        "# Set 'Date' as index\n",
        "meat_stats_meat_production.set_index('Date', inplace=True)\n",
        "meat_stats_cold_storage.set_index('Date', inplace=True)\n",
        "all_stocks.set_index('Date', inplace=True)\n",
        "\n",
        "# Convert 'Production' to numeric\n",
        "meat_stats_meat_production['Production'] = meat_stats_meat_production['Production'].str.replace(',', '').astype(float)\n",
        "\n",
        "# Handle missing values\n",
        "meat_stats_meat_production = meat_stats_meat_production.dropna()\n",
        "meat_stats_cold_storage = meat_stats_cold_storage.dropna()\n",
        "\n",
        "# Filter data for 'Red Meat' and 'Poultry'\n",
        "red_meat_production = meat_stats_meat_production.loc[meat_stats_meat_production['Type of Meat'] == 'Red Meat', 'Production']\n",
        "poultry_production = meat_stats_meat_production.loc[meat_stats_meat_production['Type of Meat'] == 'Poultry', 'Production']\n",
        "\n",
        "# Filter fast food stocks\n",
        "fast_food_stocks = stock_descriptions[stock_descriptions['Industry'].isin(['RETAIL-EATING PLACES', 'RETAIL-EATING & DRINKING PLACES'])]\n",
        "fast_food_stock_prices = all_stocks[all_stocks['Ticker_Symbol'].isin(fast_food_stocks['Symbol'])]\n",
        "\n",
        "# Resample to monthly data\n",
        "monthly_fast_food_stock_prices = fast_food_stock_prices['Close'].resample('M').mean()\n",
        "red_meat_production_monthly = red_meat_production.resample('M').mean()\n",
        "poultry_production_monthly = poultry_production.resample('M').mean()\n",
        "\n",
        "# Aggregate cold storage data by month and type of meat\n",
        "cold_storage_agg = meat_stats_cold_storage.groupby([pd.Grouper(freq='M'), 'Type_Of_Meat'])['Weight'].mean().unstack(fill_value=0)\n",
        "cold_storage_agg.columns = ['ColdStorageRedMeat', 'ColdStoragePoultry']\n",
        "\n",
        "# Align both time series to the same time range\n",
        "start_date = max(monthly_fast_food_stock_prices.index.min(), red_meat_production_monthly.index.min(), poultry_production_monthly.index.min(), cold_storage_agg.index.min())\n",
        "end_date = min(monthly_fast_food_stock_prices.index.max(), red_meat_production_monthly.index.max(), poultry_production_monthly.index.max(), cold_storage_agg.index.max())\n",
        "\n",
        "monthly_fast_food_stock_prices = monthly_fast_food_stock_prices.loc[start_date:end_date]\n",
        "red_meat_production_monthly = red_meat_production_monthly.loc[start_date:end_date]\n",
        "poultry_production_monthly = poultry_production_monthly.loc[start_date:end_date]\n",
        "cold_storage_agg = cold_storage_agg.loc[start_date:end_date]\n",
        "\n",
        "# Combine all data\n",
        "combined_data = pd.concat([monthly_obesity_data, monthly_unemployment_rate, monthly_fast_food_stock_prices,\n",
        "                           red_meat_production_monthly, poultry_production_monthly, cold_storage_agg], axis=1).dropna()\n",
        "\n",
        "# Print the combined data before further analysis\n",
        "print(\"Combined Data Before Dropping NA:\")\n",
        "print(combined_data.head(30))\n",
        "\n",
        "# Print missing values in the combined data\n",
        "print(\"Missing values in combined data after resampling and dropping:\")\n",
        "print(combined_data.isnull().sum())\n",
        "\n",
        "# Print the head of the combined data\n",
        "print(\"Head of combined data:\")\n",
        "print(combined_data.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "b0Bo0Ul1gb_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(monthly_obesity_data)\n",
        "# Plot the obesity rate\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(monthly_obesity_data.index, monthly_obesity_data['Obesity_Rate'], label='Obesity Rate')\n",
        "plt.title('Monthly Obesity Rate')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Obesity Rate')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Abn5v5u2zXbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "# Prepare the data for LSTM\n",
        "def create_sequences(data, seq_length):\n",
        "    xs, ys = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        x = data[i:i+seq_length, :]\n",
        "        y = data[i+seq_length, -1]\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "seq_length = 24  # Increased sequence length\n",
        "X, y = create_sequences(scaled_data, seq_length)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "split = int(0.8 * len(X))\n",
        "X_train, X_test = X[:split], X[split:]\n",
        "y_train, y_test = y[:split], y[split:]\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(seq_length, X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(50, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Inverse transform the predictions and test data\n",
        "y_test_full = np.concatenate((X_test[:, -1, :-1], y_test.reshape(-1, 1)), axis=1)\n",
        "y_pred_full = np.concatenate((X_test[:, -1, :-1], y_pred), axis=1)\n",
        "\n",
        "# Inverse transform to get the original scale\n",
        "y_test_inv = scaler.inverse_transform(y_test_full)[:, -1]\n",
        "y_pred_inv = scaler.inverse_transform(y_pred_full)[:, -1]\n",
        "\n",
        "# Rescale the obesity rate to be in percentage format (if needed)\n",
        "y_test_inv = y_test_inv / 10\n",
        "y_pred_inv = y_pred_inv / 10\n",
        "\n",
        "# Plot the observed and predicted obesity rates\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(combined_data.index[-len(y_test):], y_test_inv, label='Observed Obesity Rate')\n",
        "plt.plot(combined_data.index[-len(y_test):], y_pred_inv, label='Predicted Obesity Rate', linestyle='--')\n",
        "plt.title('Observed and Predicted Obesity Rate')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Obesity Rate (%)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Calculate and print the Mean Squared Error\n",
        "mse = mean_squared_error(y_test_inv, y_pred_inv)\n",
        "print(f\"Mean Squared Error: {mse}\")\n"
      ],
      "metadata": {
        "id": "fLuyXpeazIm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the combined graph\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(monthly_obesity_data.index, monthly_obesity_data['Obesity_Rate'], label='Observed Obesity Rate')\n",
        "plt.plot(combined_data.index[-len(y_test):], y_pred_inv, label='Predicted Obesity Rate', linestyle='--')\n",
        "plt.title('Observed and Predicted Obesity Rate')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Obesity Rate (%)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_FdRo2EL0H7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import GRU\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(combined_data)\n",
        "\n",
        "# Prepare the data for LSTM\n",
        "def create_sequences(data, seq_length):\n",
        "    xs, ys = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        x = data[i:i+seq_length, :]\n",
        "        y = data[i+seq_length, -1]\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "seq_length = 24  # Increased sequence length\n",
        "X, y = create_sequences(scaled_data, seq_length)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "split = int(0.8 * len(X))\n",
        "X_train, X_test = X[:split], X[split:]\n",
        "y_train, y_test = y[:split], y[split:]\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(GRU(100, activation='relu', return_sequences=True, input_shape=(seq_length, X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(GRU(50, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=200, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Inverse transform the predictions and test data\n",
        "y_test_full = np.concatenate((X_test[:, -1, :-1], y_test.reshape(-1, 1)), axis=1)\n",
        "y_pred_full = np.concatenate((X_test[:, -1, :-1], y_pred), axis=1)\n",
        "\n",
        "# Inverse transform to get the original scale\n",
        "y_test_inv = scaler.inverse_transform(y_test_full)[:, -1]\n",
        "y_pred_inv = scaler.inverse_transform(y_pred_full)[:, -1]\n",
        "\n",
        "# Plot the observed and predicted obesity rates\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(monthly_obesity_data.index, monthly_obesity_data['Obesity_Rate'], label='Observed Obesity Rate')\n",
        "plt.plot(combined_data.index[-len(y_test):], y_pred_inv, label='Predicted Obesity Rate', linestyle='--')\n",
        "plt.title('Observed and Predicted Obesity Rate')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Obesity Rate (%)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Calculate and print the Mean Squared Error\n",
        "mse = mean_squared_error(y_test_inv, y_pred_inv)\n",
        "print(f\"Mean Squared Error: {mse}\")\n"
      ],
      "metadata": {
        "id": "EUXocH9-1FRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the observed and predicted obesity rates\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(combined_data.index[-len(y_test):], y_test_inv / 10, label='Observed Obesity Rate')\n",
        "plt.plot(combined_data.index[-len(y_test):], y_pred_inv / 10, label='Predicted Obesity Rate', linestyle='--')\n",
        "plt.plot(monthly_obesity_data.index, monthly_obesity_data['Obesity_Rate'], label='Observed Obesity Rate')\n",
        "\n",
        "plt.title('Observed and Predicted Obesity Rate')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Obesity Rate (%)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eAEuihU92EeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "grcdxcDP1xL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Load datasets\n",
        "def import_dataset(file_id, name, encoding='utf-8'):\n",
        "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "    return pd.read_csv(url, encoding=encoding, low_memory=False)\n",
        "\n",
        "# Load the nutrition data\n",
        "nutrition = import_dataset(\"1nxW91Jp65jQOdR_2_FJl1XaXDzhiHzgC\", \"nutrition\")\n",
        "\n",
        "# Filter for obesity data\n",
        "obesity_data = nutrition[(nutrition['Topic'] == 'Obesity / Weight Status') &\n",
        "                         (nutrition['Question'].str.contains('obesity', case=False))]\n",
        "\n",
        "# Extract relevant columns and rename them\n",
        "obesity_data = obesity_data[['YearStart', 'Data_Value']]\n",
        "obesity_data.columns = ['Year', 'Obesity_Rate']\n",
        "\n",
        "# Average the yearly data\n",
        "obesity_data = obesity_data.groupby('Year').mean().reset_index()\n",
        "\n",
        "# Apply a rolling mean to smooth the data\n",
        "obesity_data['Obesity_Rate'] = obesity_data['Obesity_Rate'].rolling(window=3, center=True).mean()\n",
        "\n",
        "# Generate monthly dates\n",
        "monthly_dates = pd.date_range(start=f\"{obesity_data['Year'].min()}-01-01\", end=f\"{obesity_data['Year'].max()}-12-31\", freq='M')\n",
        "\n",
        "# Interpolate to generate monthly obesity rates from yearly data\n",
        "obesity_data.set_index('Year', inplace=True)\n",
        "obesity_data = obesity_data.reindex(range(obesity_data.index.min(), obesity_data.index.max() + 1)).interpolate()\n",
        "\n",
        "# Repeat each yearly rate for 12 months to create a monthly series\n",
        "monthly_obesity_rate = np.repeat(obesity_data['Obesity_Rate'].values, 12)\n",
        "\n",
        "# Add some noise to the monthly obesity rate\n",
        "np.random.seed(42)\n",
        "monthly_obesity_rate += np.random.normal(scale=0.5, size=monthly_obesity_rate.shape)\n",
        "\n",
        "# Truncate or extend monthly_dates to match the length of monthly_obesity_rate\n",
        "monthly_dates = monthly_dates[:len(monthly_obesity_rate)]\n",
        "\n",
        "# Create a DataFrame for the monthly obesity data\n",
        "monthly_obesity_data = pd.DataFrame({\n",
        "    'DATE': monthly_dates,\n",
        "    'Obesity_Rate': monthly_obesity_rate\n",
        "})\n",
        "\n",
        "# Set 'DATE' as index\n",
        "monthly_obesity_data.set_index('DATE', inplace=True)\n",
        "\n",
        "# Load the unemployment data\n",
        "unemployment_new = import_dataset(\"16HmWmtgK8MM3RQ3Ed_R0-gRGqP67iDT7\", \"unemployment\")\n",
        "\n",
        "# Convert 'DATE' column to datetime format\n",
        "unemployment_new['DATE'] = pd.to_datetime(unemployment_new['DATE'])\n",
        "\n",
        "# Set 'DATE' as index\n",
        "unemployment_new.set_index('DATE', inplace=True)\n",
        "\n",
        "# Filter the unemployment data to match the date range of the obesity data\n",
        "unemployment_filtered = unemployment_new.loc[monthly_obesity_data.index.min():monthly_obesity_data.index.max()]\n",
        "\n",
        "# Resample unemployment data to monthly frequency and fill missing values\n",
        "monthly_unemployment_rate = unemployment_filtered['LNS14000036'].resample('M').mean().interpolate()\n",
        "\n",
        "# Align the date range of both datasets\n",
        "aligned_dates = monthly_obesity_data.index.intersection(monthly_unemployment_rate.index)\n",
        "monthly_obesity_data = monthly_obesity_data.loc[aligned_dates]\n",
        "monthly_unemployment_rate = monthly_unemployment_rate.loc[aligned_dates]\n",
        "\n",
        "# Combine obesity and unemployment data into a single DataFrame\n",
        "combined_data = pd.concat([monthly_obesity_data, monthly_unemployment_rate], axis=1).dropna()\n",
        "\n",
        "# Ensure no NaN or infinite values\n",
        "combined_data = combined_data.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "# Print the combined data before further analysis\n",
        "print(\"Combined Data Before Dropping NA:\")\n",
        "print(combined_data.head(30))\n",
        "\n",
        "# Print missing values in the combined data\n",
        "print(\"Missing values in combined data after resampling and dropping:\")\n",
        "print(combined_data.isnull().sum())\n",
        "\n",
        "# Print the head of the combined data\n",
        "print(\"Head of combined data:\")\n",
        "print(combined_data.head())\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(combined_data)\n",
        "\n",
        "# Prepare the data for LSTM\n",
        "def create_sequences(data, seq_length):\n",
        "    xs, ys = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        x = data[i:i+seq_length, :-1]\n",
        "        y = data[i+seq_length, -1]\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "seq_length = 12\n",
        "X, y = create_sequences(scaled_data, seq_length)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "split = int(0.8 * len(X))\n",
        "X_train, X_test = X[:split], X[split:]\n",
        "y_train, y_test = y[:split], y[split:]\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', input_shape=(seq_length, X.shape[2])))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Inverse transform the predictions\n",
        "y_test_inv = scaler.inverse_transform(np.concatenate((np.zeros((len(y_test), scaled_data.shape[1]-1)), y_test.reshape(-1, 1)), axis=1))[:, -1]\n",
        "y_pred_inv = scaler.inverse_transform(np.concatenate((np.zeros((len(y_pred), scaled_data.shape[1]-1)), y_pred), axis=1))[:, -1]\n",
        "\n",
        "# Plot the observed and predicted obesity rates\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(monthly_obesity_data.index[-len(y_test):], y_test_inv, label='Observed Obesity Rate')\n",
        "plt.plot(monthly_obesity_data.index[-len(y_test):], y_pred_inv, label='Predicted Obesity Rate', linestyle='--')\n",
        "plt.title('Observed and Predicted Obesity Rate')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Obesity Rate')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Calculate and print the Mean Squared Error\n",
        "mse = mean_squared_error(y_test_inv, y_pred_inv)\n",
        "print(f\"Mean Squared Error: {mse}\")\n"
      ],
      "metadata": {
        "id": "qADRoREwsWFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Load datasets\n",
        "def import_dataset(file_id, name, encoding='utf-8'):\n",
        "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "    return pd.read_csv(url, encoding=encoding, low_memory=False)\n",
        "\n",
        "# Load the nutrition data\n",
        "nutrition = import_dataset(\"1nxW91Jp65jQOdR_2_FJl1XaXDzhiHzgC\", \"nutrition\")\n",
        "\n",
        "# Filter for obesity data\n",
        "obesity_data = nutrition[(nutrition['Topic'] == 'Obesity / Weight Status') &\n",
        "                         (nutrition['Question'].str.contains('obesity', case=False))]\n",
        "\n",
        "# Extract relevant columns and rename them\n",
        "obesity_data = obesity_data[['YearStart', 'Data_Value']]\n",
        "obesity_data.columns = ['Year', 'Obesity_Rate']\n",
        "\n",
        "# Average the yearly data\n",
        "obesity_data = obesity_data.groupby('Year').mean().reset_index()\n",
        "\n",
        "# Apply a rolling mean to smooth the data\n",
        "obesity_data['Obesity_Rate'] = obesity_data['Obesity_Rate'].rolling(window=3, center=True).mean()\n",
        "\n",
        "# Generate monthly dates\n",
        "monthly_dates = pd.date_range(start=f\"{obesity_data['Year'].min()}-01-01\", end=f\"{obesity_data['Year'].max()}-12-31\", freq='M')\n",
        "\n",
        "# Interpolate to generate monthly obesity rates from yearly data\n",
        "obesity_data.set_index('Year', inplace=True)\n",
        "obesity_data = obesity_data.reindex(range(obesity_data.index.min(), obesity_data.index.max() + 1)).interpolate()\n",
        "\n",
        "# Repeat each yearly rate for 12 months to create a monthly series\n",
        "monthly_obesity_rate = np.repeat(obesity_data['Obesity_Rate'].values, 12)\n",
        "\n",
        "# Add some noise to the monthly obesity rate\n",
        "np.random.seed(42)\n",
        "monthly_obesity_rate += np.random.normal(scale=0.5, size=monthly_obesity_rate.shape)\n",
        "\n",
        "# Truncate or extend monthly_dates to match the length of monthly_obesity_rate\n",
        "monthly_dates = monthly_dates[:len(monthly_obesity_rate)]\n",
        "\n",
        "# Create a DataFrame for the monthly obesity data\n",
        "monthly_obesity_data = pd.DataFrame({\n",
        "    'DATE': monthly_dates,\n",
        "    'Obesity_Rate': monthly_obesity_rate\n",
        "})\n",
        "\n",
        "# Set 'DATE' as index\n",
        "monthly_obesity_data.set_index('DATE', inplace=True)\n",
        "\n",
        "# Load the unemployment data\n",
        "unemployment_new = import_dataset(\"16HmWmtgK8MM3RQ3Ed_R0-gRGqP67iDT7\", \"unemployment\")\n",
        "\n",
        "# Convert 'DATE' column to datetime format\n",
        "unemployment_new['DATE'] = pd.to_datetime(unemployment_new['DATE'])\n",
        "\n",
        "# Set 'DATE' as index\n",
        "unemployment_new.set_index('DATE', inplace=True)\n",
        "\n",
        "# Filter the unemployment data to match the date range of the obesity data\n",
        "unemployment_filtered = unemployment_new.loc[monthly_obesity_data.index.min():monthly_obesity_data.index.max()]\n",
        "\n",
        "# Resample unemployment data to monthly frequency and fill missing values\n",
        "monthly_unemployment_rate = unemployment_filtered['LNS14000036'].resample('M').mean().interpolate()\n",
        "\n",
        "# Load additional datasets\n",
        "meat_stats_meat_production = import_dataset(\"1xZqVE4caTO4H60FFIP7Z5Lx1xCz0UuSd\", \"meat_stats_meat_production\")\n",
        "meat_stats_cold_storage = import_dataset(\"1YKBAaJKN_-RRp789RJ4wNYnpO40GyQqD\", \"meat_stats_cold_storage\")\n",
        "all_stocks = import_dataset(\"1hY7xiB-84DbqWhsZAazfwGbgtVxnBzDJ\", \"all_stocks\")\n",
        "stock_descriptions = import_dataset(\"1AzU_F3Usfx_iYRLH1KAMMaNZ3-9y0AwK\", \"stock_descriptions\")\n",
        "\n",
        "# Convert 'Date' columns to datetime format\n",
        "meat_stats_meat_production['Date'] = pd.to_datetime(meat_stats_meat_production['Date'], format='%b-%Y')\n",
        "meat_stats_cold_storage['Date'] = pd.to_datetime(meat_stats_cold_storage['Date'], format='%b-%Y')\n",
        "all_stocks['Date'] = pd.to_datetime(all_stocks['Date-Time'])\n",
        "\n",
        "# Set 'Date' as index\n",
        "meat_stats_meat_production.set_index('Date', inplace=True)\n",
        "meat_stats_cold_storage.set_index('Date', inplace=True)\n",
        "all_stocks.set_index('Date', inplace=True)\n",
        "\n",
        "# Convert 'Production' to numeric\n",
        "meat_stats_meat_production['Production'] = meat_stats_meat_production['Production'].str.replace(',', '').astype(float)\n",
        "\n",
        "# Handle missing values\n",
        "meat_stats_meat_production = meat_stats_meat_production.dropna()\n",
        "meat_stats_cold_storage = meat_stats_cold_storage.dropna()\n",
        "\n",
        "# Filter data for 'Red Meat' and 'Poultry'\n",
        "red_meat_production = meat_stats_meat_production.loc[meat_stats_meat_production['Type of Meat'] == 'Red Meat', 'Production']\n",
        "poultry_production = meat_stats_meat_production.loc[meat_stats_meat_production['Type of Meat'] == 'Poultry', 'Production']\n",
        "\n",
        "# Filter fast food stocks\n",
        "fast_food_stocks = stock_descriptions[stock_descriptions['Industry'].isin(['RETAIL-EATING PLACES', 'RETAIL-EATING & DRINKING PLACES'])]\n",
        "fast_food_stock_prices = all_stocks[all_stocks['Ticker_Symbol'].isin(fast_food_stocks['Symbol'])]\n",
        "\n",
        "# Resample to monthly data\n",
        "monthly_fast_food_stock_prices = fast_food_stock_prices['Close'].resample('M').mean()\n",
        "red_meat_production_monthly = red_meat_production.resample('M').mean()\n",
        "poultry_production_monthly = poultry_production.resample('M').mean()\n",
        "\n",
        "# Aggregate cold storage data by month and type of meat\n",
        "cold_storage_agg = meat_stats_cold_storage.groupby([pd.Grouper(freq='M'), 'Type_Of_Meat'])['Weight'].mean().unstack(fill_value=0)\n",
        "cold_storage_agg.columns = ['ColdStorageRedMeat', 'ColdStoragePoultry']\n",
        "\n",
        "# Align both time series to the same time range\n",
        "start_date = max(monthly_fast_food_stock_prices.index.min(), red_meat_production_monthly.index.min(), poultry_production_monthly.index.min(), cold_storage_agg.index.min())\n",
        "end_date = min(monthly_fast_food_stock_prices.index.max(), red_meat_production_monthly.index.max(), poultry_production_monthly.index.max(), cold_storage_agg.index.max())\n",
        "\n",
        "monthly_fast_food_stock_prices = monthly_fast_food_stock_prices.loc[start_date:end_date]\n",
        "red_meat_production_monthly = red_meat_production_monthly.loc[start_date:end_date]\n",
        "poultry_production_monthly = poultry_production_monthly.loc[start_date:end_date]\n",
        "cold_storage_agg = cold_storage_agg.loc[start_date:end_date]\n",
        "\n",
        "# Combine all data\n",
        "combined_data = pd.concat([monthly_obesity_data, monthly_unemployment_rate, monthly_fast_food_stock_prices,\n",
        "                           red_meat_production_monthly, poultry_production_monthly, cold_storage_agg], axis=1).dropna()\n",
        "\n",
        "# Print the combined data before further analysis\n",
        "print(\"Combined Data Before Dropping NA:\")\n",
        "print(combined_data.head(30))\n",
        "\n",
        "# Print missing values in the combined data\n",
        "print(\"Missing values in combined data after resampling and dropping:\")\n",
        "print(combined_data.isnull().sum())\n",
        "\n",
        "# Print the head of the combined data\n",
        "print(\"Head of combined data:\")\n",
        "print(combined_data.head())\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(combined_data)\n",
        "\n",
        "# Prepare the data for LSTM\n",
        "def create_sequences(data, seq_length):\n",
        "    xs, ys = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        x = data[i:i+seq_length, :-1]\n",
        "        y = data[i+seq_length, -1]\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "seq_length = 12\n",
        "X, y = create_sequences(scaled_data, seq_length)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "split = int(0.8 * len(X))\n",
        "X_train, X_test = X[:split], X[split:]\n",
        "y_train, y_test = y[:split], y[split:]\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', input_shape=(seq_length, X.shape[2])))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Inverse transform the predictions and test data\n",
        "y_test_inv = scaler.inverse_transform(np.concatenate((np.zeros((len(y_test), scaled_data.shape[1]-1)), y_test.reshape(-1, 1)), axis=1))[:, -1]\n",
        "y_pred_inv = scaler.inverse_transform(np.concatenate((np.zeros((len(y_pred), scaled_data.shape[1]-1)), y_pred), axis=1))[:, -1]\n",
        "\n",
        "# Plot the observed and predicted obesity rates\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(combined_data.index[-len(y_test):], y_test_inv, label='Observed Obesity Rate')\n",
        "plt.plot(combined_data.index[-len(y_test):], y_pred_inv, label='Predicted Obesity Rate', linestyle='--')\n",
        "plt.title('Observed and Predicted Obesity Rate')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Obesity Rate')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Calculate and print the Mean Squared Error\n",
        "mse = mean_squared_error(y_test_inv, y_pred_inv)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1BCx_PGsipDV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}